{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3acd4d8f",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6c35fc",
   "metadata": {},
   "source": [
    "### Scoring Logic Description\n",
    "\n",
    "1. We start by importing the odis dataframe from a CSV that includes all the relevant datapoint to score and display data\n",
    "2. We compute the scores for each criteria specific to the commune (independant from subject)\n",
    "3. We compute the scores for each criteria specific to the subject (dependand from both subject and commune) \n",
    "4. We identify all commune<->neighbour pairs (binômes) for each commune within search radius\n",
    "5. We compute category scores (emploi, logement, education etc...) as an average of the all the scores for a given category\n",
    "6. For each commune we compare the commune and neighbour category scores and weighted the highest one with category weights defined by subject and then keep the best weighted score for each commune\n",
    "8. We display result in on a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16add7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS SHOULD BE THE END OF JUPYTER NOTEBOOK EXPORT\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from scipy import stats\n",
    "import folium as flm #required for gdf.explore()\n",
    "import shapely as shp\n",
    "from shapely.wkt import loads\n",
    "from shapely.geometry import Polygon\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7744f22f",
   "metadata": {},
   "source": [
    "## 1. Fetching key indicators from ODIS source file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d325cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_loading_datasets(odis_file, metiers_file, formations_file, ecoles_file):\n",
    "    odis = gpd.GeoDataFrame(gpd.read_parquet(odis_file))\n",
    "    odis.set_geometry(odis.polygon, inplace=True)\n",
    "    odis = odis[~odis.polygon.isna()]\n",
    "\n",
    "    #Later we need the code FAP <-> FAP Name used to classify jobs\n",
    "    codfap_index = pd.read_csv(metiers_file, delimiter=';')\n",
    "\n",
    "    # Later we need the code formation <-> Formation Name used to classify trainings\n",
    "    # source: https://www.data.gouv.fr/fr/datasets/liste-publique-des-organismes-de-formation-l-6351-7-1-du-code-du-travail/\n",
    "    codformations_index = pd.read_csv(formations_file).set_index('codformation')\n",
    "\n",
    "    # Etablissements scolaires\n",
    "    annuaire_ecoles = pd.read_parquet(ecoles_file)\n",
    "    annuaire_ecoles.geometry = annuaire_ecoles.geometry.apply(shp.from_wkb)\n",
    "\n",
    "    return odis, codfap_index, codformations_index, annuaire_ecoles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5877d7b",
   "metadata": {},
   "source": [
    "## 2. Criterias Scoring for each commune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7ab3709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_communes_criterias(odis):\n",
    "    #met_ratio est le ratio d'offres sur la population de la zone (pour 1000 habitants)\n",
    "    odis['met_ratio']= 1000 * odis.met/odis.pop_be\n",
    "    #met_tension_ratio est le ratio d'offres population de la zone (pour 1000 habitants)\n",
    "    #odis['met_tension_ratio'] = np.where(odis.met_tension == 0, None, 1000 * odis.met_tension/odis.pop_be)\n",
    "    odis['met_tension_ratio'] = 1000 * odis.met_tension/odis.pop_be\n",
    "\n",
    "    #svc_ratio est le ratio de services d'inclusion de la commune (pour 1000 habitants)\n",
    "    #odis['svc_incl_ratio'] = np.where(odis.svc_incl_count == 0, None, 1000 * odis.svc_incl_count/odis.pop_be)\n",
    "    odis['svc_incl_ratio'] = 1000 * odis.svc_incl_count/odis.pop_be\n",
    "\n",
    "    #log_vac_ratio est le ratio de logements vacants de la commune % total logements\n",
    "    #odis['log_vac_ratio'] = np.where(odis.log_vac == 0, None, 1000 * odis.log_vac/odis.log_total)\n",
    "    odis['log_vac_ratio'] = odis.log_vac/odis.log_total\n",
    "\n",
    "    #log_5p+_ratio est le ratio de residences principales de 5 pièces ou plus % total residences principales\n",
    "    #odis['log_5p_ratio'] = np.where(odis['rp_5+pieces'] == 0, None, 1000 * odis['rp_5+pieces']/odis.log_rp)\n",
    "    odis['log_5p_ratio'] = odis['rp_5+pieces']/odis.log_rp\n",
    "\n",
    "    # Risque de fermeture école: ratio de classe à risque de fermeture % nombre d'écoles\n",
    "    odis['risque_fermeture_ratio'] = odis.risque_fermeture/odis.ecoles_ct\n",
    "\n",
    "    #Scaling with PowerTransformer so that \n",
    "    # 1. outliers don't impact too much the end result\n",
    "    # 2. all scores are normaly distributed and centered around 0\n",
    "    #pt = preprocessing.PowerTransformer()\n",
    "    t = preprocessing.QuantileTransformer(output_distribution=\"uniform\")\n",
    "    odis['met_scaled'] = t.fit_transform(odis[['met_ratio']].fillna(0)).round(3)\n",
    "    odis['met_tension_scaled'] = t.fit_transform(odis[['met_tension_ratio']].fillna(0)).round(3)\n",
    "    odis['svc_incl_scaled'] = t.fit_transform(odis[['svc_incl_ratio']].fillna(0)).round(3)\n",
    "    odis['log_vac_scaled'] = t.fit_transform(odis[['log_vac_ratio']].fillna(0)).round(3)\n",
    "    odis['log_5p_scaled'] = t.fit_transform(odis[['log_5p_ratio']].fillna(0)).round(3)\n",
    "    odis['classes_ferm_scaled'] = t.fit_transform(odis[['risque_fermeture_ratio']].fillna(0)).round(3)\n",
    "    odis['pol_scaled'] = odis[['pol_num']].astype('float').round(3)\n",
    "\n",
    "    #Adding a category for each score that will be used to assign weights for the weighted avg\n",
    "    scores_cat = pd.DataFrame([\n",
    "        {'score':'met_scaled','score_name':'Taux Besoin Emploi','cat':'emploi'},\n",
    "        {'score':'met_tension_scaled','score_name':'Taux Besoin Emploi en Tension','cat':'emploi'},\n",
    "        {'score':'svc_incl_scaled','score_name':'Taux Services Inclusion','cat':'soutien'},\n",
    "        {'score':'log_vac_scaled','score_name':'Taux Logements Vacants','cat':'logement'},\n",
    "        {'score':'log_5p_scaled','score_name':'Taux Grandes Résidences Principales','cat':'logement'},\n",
    "        {'score':'classes_ferm_scaled','score_name':'Taux Classe à Risque de Fermeture','cat':'education'},\n",
    "        {'score':'pol_scaled','score_name':'Couleur Politique','cat':'soutien'},\n",
    "        ])\n",
    "    return odis, scores_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58361529",
   "metadata": {},
   "source": [
    "## 3. Criterias specific to subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dec702e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subject preferences weighted score computation\n",
    "# prefs = {\n",
    "#     'emploi':2,\n",
    "#     'logement':1,\n",
    "#     'education':1,\n",
    "#     'soutien':1,\n",
    "#     'mobilité':0,\n",
    "#     'commune_actuelle':'33281',\n",
    "#     'loc_distance_km':10,\n",
    "#     'codes_metiers':{\n",
    "#         'codes_metiers_adulte1':['S1X40','J0X33','A1X41'],\n",
    "#         'codes_metiers_adulte2':['T4X60','T2A60']\n",
    "#     },\n",
    "#     'codes_formations':{\n",
    "#         'codes_formations_adulte1':['320'],\n",
    "#         'codes_formations_adulte2':['333','100']\n",
    "#     },\n",
    "#     'age_enfants':{\n",
    "#         'age_enfant1':4,\n",
    "#         'age_enfant2':10,\n",
    "#         'age_enfant3':None,\n",
    "#         'age_enfant4':None,\n",
    "#         'age_enfant5':None\n",
    "#     }\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ffe9535",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing distance from current commune \n",
    "#Using a crs that allows to compute distance in meters for metropolitan France\n",
    "\n",
    "def distance_calc(df, ref_point):\n",
    "    return int(df.distance(ref_point))\n",
    "\n",
    "def add_distance_to_current_loc(df, current_codgeo):\n",
    "    projected_crs = \"EPSG:2154\"\n",
    "    zone_recherche = gpd.GeoDataFrame(df[df.codgeo == current_codgeo]['polygon'])\n",
    "    zone_recherche.set_geometry('polygon', inplace=True)\n",
    "    zone_recherche.to_crs(projected_crs, inplace=True)\n",
    "\n",
    "    df.to_crs(projected_crs, inplace=True)\n",
    "    df['dist_current_loc'] = df['polygon'].apply(distance_calc, ref_point=zone_recherche.iloc[0].polygon)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d2a316b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_distance_to_current_loc(odis, current_codgeo=LOC_COMMUNE_ACTUELLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c8d1bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding score specific to subject looking for a job identified as en besoin\n",
    "def codes_match(df, codes_list):\n",
    "    #returns a list of codfaps that matches\n",
    "    if df is None:\n",
    "        return []\n",
    "    return list(set(df.tolist()).intersection(set(codes_list)))\n",
    "\n",
    "def fap_names_lookup(df):\n",
    "    return list(codfap_index[codfap_index['Code FAP 341'].isin(df)]['Intitulé FAP 341'])\n",
    "\n",
    "def compute_subject_specific_scores(df, scores_cat, subject_pref): \n",
    "    # Let's create subject-specific scores\n",
    "    t = preprocessing.QuantileTransformer(output_distribution=\"uniform\")\n",
    "    #For each adult we look for jobs categories that match what is needed\n",
    "    i=1\n",
    "    for adult in subject_pref['codes_metiers']:\n",
    "        df['met_match_codes_adult'+str(i)] = df.be_codfap_top.apply(codes_match, codes_list=subject_pref['codes_metiers'][adult])\n",
    "        df['met_match_adult'+str(i)] = df['met_match_codes_adult'+str(i)].apply(len)\n",
    "        df['met_match_adult'+str(i)+'_scaled'] = t.fit_transform(df[['met_match_adult'+str(i)]].fillna(0))\n",
    "        i+=1\n",
    "\n",
    "    j=1\n",
    "    for adult in subject_pref['codes_formations']:\n",
    "        df['form_match_codes_adult'+str(j)] = df.codes_formations.apply(codes_match, codes_list=subject_pref['codes_formations'][adult])\n",
    "        df['form_match_adult'+str(j)] = df['form_match_codes_adult'+str(j)].apply(len)\n",
    "        df['form_match_adult'+str(j)+'_scaled'] = t.fit_transform(df[['form_match_adult'+str(j)]].fillna(0))\n",
    "        j+=1\n",
    "\n",
    "    # We compute the distance from the current location \n",
    "    df['reloc_dist_scaled'] = (1-df['dist_current_loc']/(subject_pref['loc_distance_km']*1000))\n",
    "    df['reloc_epci_scaled'] = np.where(df['epci_code'] == df[df.codgeo == subject_pref['commune_actuelle']]['epci_code'].iloc[0],1,0)\n",
    "    \n",
    "    #Let's not forget to categorize these new scores to the existing score_cat index if it doesnt exist\n",
    "    #if (scores_cat.score != 'met_match_scaled').all():\n",
    "    scores_cat_subject = pd.DataFrame([\n",
    "        {'score':'met_match_adult1_scaled','score_name':'Match compétences et Besoin Emploi Adult 1','cat':'emploi'},\n",
    "        {'score':'met_match_adult2_scaled','score_name':'Match compétences et Besoin Emploi Adult 2','cat':'emploi'},\n",
    "        {'score':'form_match_adult1_scaled','score_name':'Match compétences et Centres de formation','cat':'emploi'},\n",
    "        {'score':'form_match_adult2_scaled','score_name':'Match compétences et Centres de formation','cat':'emploi'},\n",
    "        {'score':'reloc_dist_scaled','score_name':'Distance de la localisation actuelle','cat':'mobilité'},\n",
    "        {'score':'reloc_epci_scaled','score_name':'Même EPCI que la localisation actuelle','cat':'mobilité'}\n",
    "        ])\n",
    "    scores_cat_subject = pd.concat([scores_cat, scores_cat_subject])\n",
    "    #scores_cat = pd.concat([scores_cat, scores_cat_subject])\n",
    "    return df, scores_cat_subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f916b813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create subject-specific scores\n",
    "#scores_cat_subject = compute_subject_specific_scores(odis, scores_cat=scores_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a250e2d3",
   "metadata": {},
   "source": [
    "## 4. Distance filter + Gathering nearby Communes Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd47e8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering dataframe based on subject distance preference (to save on compute time later on)\n",
    "def filter_loc_by_distance(df, distance):\n",
    "    return df[df.dist_current_loc < distance * 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe939091",
   "metadata": {},
   "outputs": [],
   "source": [
    "#odis_search = filter_loc_by_distance(odis, distance=LOC_DISTANCE_KM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9898c8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adding_score_voisins(df_search, df_source):\n",
    "    #df_search is the dataframe pre-filtered by location\n",
    "    #df_source is the dataframe with all the communes\n",
    "    binome_columns = ['codgeo','libgeo','codgeo_voisins','polygon','epci_code','epci_nom']+[col for col in df_source.columns if col.startswith('met_match_codes')]+[col for col in df_source.columns if col.endswith('_scaled')] \n",
    "\n",
    "    # Adds itself to list of voisins = monome case\n",
    "    # Note: this code triggers the SettingWithCopyWarning but I don't know how to fix it...\n",
    "    df_search = df_search.copy()\n",
    "    df_search.codgeo_voisins = df_search.apply(lambda x: np.append(x.codgeo_voisins, x.codgeo), axis=1)\n",
    "\n",
    "    # Explodes the dataframe to have a row for each voisins + itself\n",
    "    df_search['codgeo_voisins_copy'] = df_search['codgeo_voisins']\n",
    "    df_search_exploded = df_search.explode('codgeo_voisins_copy')\n",
    "    \n",
    "    # For each commune (codgeo) in search area (df_search) we add all its voisin's scores\n",
    "    odis_search_exploded = pd.merge(df_search_exploded, df_source[binome_columns].add_suffix('_binome'), left_on='codgeo_voisins_copy', right_on='codgeo_binome', how='left')\n",
    "    \n",
    "    # Adds a column to identify binomes vs monomes + cleanup\n",
    "    odis_search_exploded['binome'] = odis_search_exploded.apply(lambda x: False if x.codgeo == x.codgeo_binome else True, axis=1)\n",
    "    odis_search_exploded.drop(columns={'codgeo_voisins_copy'}, inplace=True)\n",
    "    \n",
    "    return odis_search_exploded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "18d774dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#odis_exploded = adding_score_voisins(odis_search, odis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae24ab0",
   "metadata": {},
   "source": [
    "## 5. Computing category scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25e7e3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cat_scores(df, scores_cat):\n",
    "    df = df.copy()\n",
    "    for cat in set(scores_cat.cat):\n",
    "        cat_scores_indices = scores_cat[scores_cat['cat'] == cat]['score'].tolist()\n",
    "        cat_scores_indices_binome = [score+'_binome' for score in cat_scores_indices]\n",
    "\n",
    "        # Efficiently select all relevant rows at once\n",
    "        cat_scores_df = df[cat_scores_indices]\n",
    "        cat_scores_binome_df = df[cat_scores_indices_binome]\n",
    "        \n",
    "        # Calculate the mean of the selected rows\n",
    "        df[cat + '_cat_score'] = 100 * cat_scores_df.astype(float).mean(axis=1).round(3)\n",
    "        df[cat + '_cat_score_binome'] = 100 * cat_scores_binome_df.astype(float).mean(axis=1).round(3)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fcebd0",
   "metadata": {},
   "source": [
    "## 6. Final Binome Score Weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "831f4061",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COM_LIMITROPHE_PENALTY = 0.1 #décote de 10% pour les communes limitrophes vs commune cible\n",
    "\n",
    "def compute_binome_score(df, binome_penalty, weights):\n",
    "    scores_col = [col for col in df.columns if col.endswith('_cat_score')]\n",
    "    max_scores = pd.DataFrame()\n",
    "    \n",
    "    for col in scores_col:\n",
    "        cat_weight = weights[col.split('_')[0]]\n",
    "        max_scores[col] = cat_weight * np.where(\n",
    "            df[col] >= (1-binome_penalty)*df[col+'_binome'],\n",
    "            df[col],\n",
    "            (1-binome_penalty)*df[col+'_binome']\n",
    "            )\n",
    "    \n",
    "    return max_scores.mean(axis=1).round(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3042ad31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monome_cleanup(df):\n",
    "    if df.loc['binome'] == False:\n",
    "        for col in df.index:\n",
    "            if col.endswith('_binome'):\n",
    "                df.loc[col] = None\n",
    "    return df\n",
    "\n",
    "def best_score_compute(df):\n",
    "    #Keeping the best (top #1) monome or binome result for each commune\n",
    "    best = df.sort_values('weighted_score', ascending=False).groupby('codgeo').head(1)\n",
    "    #Cleanup redundant data in the monome cases\n",
    "    best = best.apply(monome_cleanup, axis=1)\n",
    "    return best\n",
    "\n",
    "#odis_search_best = best_score_compute(odis_exploded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56483e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main function that aggregates most of the above in one sequence\n",
    "def compute_odis_score(df, scores_cat, prefs):\n",
    "    # Note: we consider that the dataframe already has all the commune-specific criteria scores\n",
    "    # Note2: the prefs dict has all the subject preferences\n",
    "    \n",
    "    # We add the distance between each commune and the one set as where the subject currently lives and filter based on distance pref\n",
    "    df = add_distance_to_current_loc(df, current_codgeo=prefs['commune_actuelle'])\n",
    "\n",
    "    # We compute the subject specific scores\n",
    "    df, scores_cat_subject = compute_subject_specific_scores(df, scores_cat=scores_cat, subject_pref=prefs)\n",
    "    \n",
    "    # We filter by distance to reduce the compute cost on a smaller odis_search dataframe\n",
    "    odis_search = filter_loc_by_distance(df, distance=prefs['loc_distance_km'])\n",
    "\n",
    "    # We add the criteria scores for all neighbor communes forming monomes and binomes\n",
    "    odis_exploded = adding_score_voisins(odis_search, df)\n",
    "\n",
    "    # We compute the category scores for both the target and the binome\n",
    "    odis_exploded = compute_cat_scores(odis_exploded, scores_cat=scores_cat_subject)\n",
    "    \n",
    "    # We provide the scores columns as a parameter to compute faster\n",
    "    #scores_col = [col for col in odis_exploded.columns if col.endswith('_cat_score')]\n",
    "    \n",
    "    # We computing the final weighted score for all commune<->voisin combinations\n",
    "    odis_exploded['weighted_score'] = compute_binome_score(\n",
    "        odis_exploded,\n",
    "        binome_penalty=prefs['binome_penalty'],\n",
    "        weights=prefs\n",
    "        )\n",
    "    \n",
    "    # We keep best monome or binome for each commune \n",
    "    odis_search_best = best_score_compute(odis_exploded)\n",
    "\n",
    "    return odis_search_best, scores_cat_subject\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7504e591",
   "metadata": {},
   "source": [
    "## 7. Generating Narrative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1a10f4",
   "metadata": {},
   "source": [
    "Here we want to generate a 'human readable' explanation about why scored high a given location.\n",
    "Things to show:\n",
    "- Target commune name and EPCI\n",
    "- Weighted Score\n",
    "- If Binome, show the binome and EPCI if different from target\n",
    "- Show top 3 criterias target (weighted ?) \n",
    "- Show top 3 criterias for binome (weighted ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4193c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_pitch(df, prefs, scores_cat, codfap_index, codformations_index):\n",
    "    pitch_lines = []\n",
    "    pitch_lines += [df.loc['libgeo'] +' dans l\\'EPCI: '+ df.loc['epci_nom']]\n",
    "    pitch_lines += ['Le score est de: '+str(df.loc['weighted_score'])]\n",
    "    if df.loc['binome']:\n",
    "        pitch_lines += ['Ce score est obtenu en binome avec la commune '+df.loc['libgeo_binome']]\n",
    "        if df.loc['epci_code'] != df.loc['epci_code_binome']:\n",
    "            pitch_lines += ['Cette commune est située dans l\\'EPCI: '+df.loc['epci_nom_binome']]\n",
    "    else:\n",
    "        pitch_lines += ['Ce score est obtenu sans commune binôme']\n",
    "\n",
    "    \n",
    "    #Adding the top contributin criterias\n",
    "    crit_scores_col = [col for col in df.index if '_scaled' in col]#col.endswith('_scaled')]\n",
    "    \n",
    "    df_sorted=df[crit_scores_col].dropna().sort_values(ascending=False)\n",
    "    for i in range(0, 5):\n",
    "        score = df_sorted.index[i][:-7] if df_sorted.index[i].endswith('_binome') else df_sorted.index[i]\n",
    "        score_name = scores_cat[scores_cat.score == score]['score_name'].item()\n",
    "        pitch_lines += ['Le critère #'+str(i+1)+' est: '+score_name+' avec un score de: '+str(df_sorted.iloc[i])]\n",
    "\n",
    "    \n",
    "    #Adding the matching job families if any\n",
    "    metiers_col = [col for col in df.index if col.startswith('met_match_codes')]\n",
    "    for metiers_adultx in metiers_col:\n",
    "        matched_codfap_names = list(codfap_index[codfap_index['Code FAP 341'].isin(df.loc[metiers_adultx])]['Intitulé FAP 341'])\n",
    "        if len(matched_codfap_names) == 1:\n",
    "            pitch_lines += ['La famille de métiers '+ matched_codfap_names[0] +' est rechechée'] \n",
    "        elif len(matched_codfap_names) >= 1:\n",
    "            list_jobs = ''\n",
    "            for job in matched_codfap_names:\n",
    "                list_jobs += job+', '\n",
    "            pitch_lines += ['Les familles de métiers '+ list_jobs[:-2] +' sont rechechées']\n",
    "    \n",
    "    return pitch_lines\n",
    "    #for line in pitch_lines:\n",
    "    #    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9062ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# codfap_index = pd.read_csv('../csv/dares_nomenclature_fap2021.csv', delimiter=';')\n",
    "# codformations_index = pd.read_csv('../csv/index_formations.csv').set_index('codformation')\n",
    "# for index, row in odis_search_best.head(1).iterrows():\n",
    "#     pitch = produce_pitch(row, prefs=prefs, scores_cat=scores_cat, codfap_index=codfap_index, codformations_index=codformations_index)\n",
    "# pitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "de96cda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_pitch_markdown(df, prefs, scores_cat, codfap_index, codformations_index):\n",
    "    pitch_md = []\n",
    "    pitch_md.append('**'+df.loc['libgeo'] +'** dans l\\'EPCI: '+ df.loc['epci_nom']+'\\n')\n",
    "    pitch_md.append('Le score est de: **'+str(df.loc['weighted_score'])+'**\\n')\n",
    "    if df.loc['binome']:\n",
    "        pitch_md.append('Ce score est obtenu en binome avec la commune '+df.loc['libgeo_binome'])\n",
    "        if df.loc['epci_code'] != df.loc['epci_code_binome']:\n",
    "            pitch_md.append(' située dans l\\'EPCI: '+df.loc['epci_nom_binome']+'\\n')\n",
    "    else:\n",
    "        pitch_md += 'Ce score est obtenu sans commune binôme'\n",
    "\n",
    "    \n",
    "    #Adding the top contributin criterias\n",
    "    crit_scores_col = [col for col in df.index if '_scaled' in col]#col.endswith('_scaled')]\n",
    "    \n",
    "    df_sorted=df[crit_scores_col].dropna().sort_values(ascending=False)\n",
    "    for i in range(0, 5):\n",
    "        score = df_sorted.index[i][:-7] if df_sorted.index[i].endswith('_binome') else df_sorted.index[i]\n",
    "        score_name = scores_cat[scores_cat.score == score]['score_name'].item()\n",
    "        pitch_md.append('- Le critère #'+str(i+1)+' est: **'+score_name+'** avec un score de: **'+str(df_sorted.iloc[i])+'**\\n')\n",
    "\n",
    "    \n",
    "    #Adding the matching job families if any\n",
    "    pitch_md.append('**Emploi**\\n') \n",
    "    metiers_col = [col for col in df.index if col.startswith('met_match_codes')]\n",
    "    matched_codfap_names = []\n",
    "    for metiers_adultx in metiers_col:\n",
    "        matched_codfap_names += list(codfap_index[codfap_index['Code FAP 341'].isin(df.loc[metiers_adultx])]['Intitulé FAP 341'])\n",
    "    matched_codfap_names = set(matched_codfap_names)\n",
    "    if len(matched_codfap_names) == 0:\n",
    "        pitch_md.append('Aucun des métiers recherchés ne figurent dans le Top 10 des métiers à pourvoir sur cette zone.\\n')\n",
    "    if len(matched_codfap_names) == 1:\n",
    "        pitch_md.append('- La famille de métiers **'+ matched_codfap_names[0] +'** est rechechée \\n')\n",
    "    elif len(matched_codfap_names) >= 1:\n",
    "        list_jobs = ''\n",
    "        for job in matched_codfap_names:\n",
    "            list_jobs += job+', '\n",
    "        pitch_md.append('- Les familles de métiers **'+ list_jobs[:-2] +'** sont rechechées \\n')\n",
    "        \n",
    "    \n",
    "    pitch_md.append(\"---\")\n",
    "    return '\\n'.join(pitch_md).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ca65c9fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Latresne** dans l'EPCI: CC des Portes de l'Entre-Deux-Mers\n",
       "\n",
       "Le score est de: **66.5**\n",
       "\n",
       "Ce score est obtenu en binome avec la commune Bègles\n",
       " située dans l'EPCI: Bordeaux Métropole\n",
       "\n",
       "- Le critère #1 est: **Même EPCI que la localisation actuelle** avec un score de: **1**\n",
       "\n",
       "- Le critère #2 est: **Match compétences et Besoin Emploi Adult 2** avec un score de: **1.0**\n",
       "\n",
       "- Le critère #3 est: **Match compétences et Besoin Emploi Adult 2** avec un score de: **1.0**\n",
       "\n",
       "- Le critère #4 est: **Couleur Politique** avec un score de: **1.0**\n",
       "\n",
       "- Le critère #5 est: **Match compétences et Centres de formation** avec un score de: **0.982982982982983**\n",
       "\n",
       "**Emploi**\n",
       "\n",
       "- Les familles de métiers **Agents d'entretien de locaux, Aides à domicile et auxiliaires de vie, Magasiniers et préparateurs de commandes peu qualifiés** sont rechechées \n",
       "\n",
       "---"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown as md\n",
    "for index, row in odis_search_best.head(1).iterrows():\n",
    "    pitch = produce_pitch_markdown(row, prefs=prefs, scores_cat=scores_cat, codfap_index=codfap_index, codformations_index=codformations_index)\n",
    "md(pitch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6205d6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4de2b412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS SHOULD BE THE END OF JUPYTER NOTEBOOK EXPORT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663e91e0",
   "metadata": {},
   "source": [
    "## 8. Export to Python file for streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95990aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following commands were written to file `../streamlit/odisscoring.py`:\n",
      "# THIS SHOULD BE THE END OF JUPYTER NOTEBOOK EXPORT\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import geopandas as gpd\n",
      "from scipy import stats\n",
      "import folium as flm #required for gdf.explore()\n",
      "import shapely as shp\n",
      "from shapely.wkt import loads\n",
      "from shapely.geometry import Polygon\n",
      "from sklearn import preprocessing\n",
      "def init_loading_datasets(odis_file, metiers_file, formations_file, ecoles_file):\n",
      "    odis = gpd.GeoDataFrame(gpd.read_parquet(odis_file))\n",
      "    odis.set_geometry(odis.polygon, inplace=True)\n",
      "    odis = odis[~odis.polygon.isna()]\n",
      "\n",
      "    #Later we need the code FAP <-> FAP Name used to classify jobs\n",
      "    codfap_index = pd.read_csv(metiers_file, delimiter=';')\n",
      "\n",
      "    # Later we need the code formation <-> Formation Name used to classify trainings\n",
      "    # source: https://www.data.gouv.fr/fr/datasets/liste-publique-des-organismes-de-formation-l-6351-7-1-du-code-du-travail/\n",
      "    codformations_index = pd.read_csv(formations_file).set_index('codformation')\n",
      "\n",
      "    # Etablissements scolaires\n",
      "    annuaire_ecoles = pd.read_parquet(ecoles_file)\n",
      "    annuaire_ecoles.geometry = annuaire_ecoles.geometry.apply(shp.from_wkb)\n",
      "\n",
      "    return odis, codfap_index, codformations_index, annuaire_ecoles\n",
      "def init_communes_criterias(odis):\n",
      "    #met_ratio est le ratio d'offres sur la population de la zone (pour 1000 habitants)\n",
      "    odis['met_ratio']= 1000 * odis.met/odis.pop_be\n",
      "    #met_tension_ratio est le ratio d'offres population de la zone (pour 1000 habitants)\n",
      "    #odis['met_tension_ratio'] = np.where(odis.met_tension == 0, None, 1000 * odis.met_tension/odis.pop_be)\n",
      "    odis['met_tension_ratio'] = 1000 * odis.met_tension/odis.pop_be\n",
      "\n",
      "    #svc_ratio est le ratio de services d'inclusion de la commune (pour 1000 habitants)\n",
      "    #odis['svc_incl_ratio'] = np.where(odis.svc_incl_count == 0, None, 1000 * odis.svc_incl_count/odis.pop_be)\n",
      "    odis['svc_incl_ratio'] = 1000 * odis.svc_incl_count/odis.pop_be\n",
      "\n",
      "    #log_vac_ratio est le ratio de logements vacants de la commune % total logements\n",
      "    #odis['log_vac_ratio'] = np.where(odis.log_vac == 0, None, 1000 * odis.log_vac/odis.log_total)\n",
      "    odis['log_vac_ratio'] = odis.log_vac/odis.log_total\n",
      "\n",
      "    #log_5p+_ratio est le ratio de residences principales de 5 pièces ou plus % total residences principales\n",
      "    #odis['log_5p_ratio'] = np.where(odis['rp_5+pieces'] == 0, None, 1000 * odis['rp_5+pieces']/odis.log_rp)\n",
      "    odis['log_5p_ratio'] = odis['rp_5+pieces']/odis.log_rp\n",
      "\n",
      "    # Risque de fermeture école: ratio de classe à risque de fermeture % nombre d'écoles\n",
      "    odis['risque_fermeture_ratio'] = odis.risque_fermeture/odis.ecoles_ct\n",
      "\n",
      "    #Scaling with PowerTransformer so that \n",
      "    # 1. outliers don't impact too much the end result\n",
      "    # 2. all scores are normaly distributed and centered around 0\n",
      "    #pt = preprocessing.PowerTransformer()\n",
      "    t = preprocessing.QuantileTransformer(output_distribution=\"uniform\")\n",
      "    odis['met_scaled'] = t.fit_transform(odis[['met_ratio']].fillna(0)).round(3)\n",
      "    odis['met_tension_scaled'] = t.fit_transform(odis[['met_tension_ratio']].fillna(0)).round(3)\n",
      "    odis['svc_incl_scaled'] = t.fit_transform(odis[['svc_incl_ratio']].fillna(0)).round(3)\n",
      "    odis['log_vac_scaled'] = t.fit_transform(odis[['log_vac_ratio']].fillna(0)).round(3)\n",
      "    odis['log_5p_scaled'] = t.fit_transform(odis[['log_5p_ratio']].fillna(0)).round(3)\n",
      "    odis['classes_ferm_scaled'] = t.fit_transform(odis[['risque_fermeture_ratio']].fillna(0)).round(3)\n",
      "    odis['pol_scaled'] = odis[['pol_num']].astype('float').round(3)\n",
      "\n",
      "    #Adding a category for each score that will be used to assign weights for the weighted avg\n",
      "    scores_cat = pd.DataFrame([\n",
      "        {'score':'met_scaled','score_name':'Taux Besoin Emploi','cat':'emploi'},\n",
      "        {'score':'met_tension_scaled','score_name':'Taux Besoin Emploi en Tension','cat':'emploi'},\n",
      "        {'score':'svc_incl_scaled','score_name':'Taux Services Inclusion','cat':'soutien'},\n",
      "        {'score':'log_vac_scaled','score_name':'Taux Logements Vacants','cat':'logement'},\n",
      "        {'score':'log_5p_scaled','score_name':'Taux Grandes Résidences Principales','cat':'logement'},\n",
      "        {'score':'classes_ferm_scaled','score_name':'Taux Classe à Risque de Fermeture','cat':'education'},\n",
      "        {'score':'pol_scaled','score_name':'Couleur Politique','cat':'soutien'},\n",
      "        ])\n",
      "    return odis, scores_cat\n",
      "# Subject preferences weighted score computation\n",
      "# prefs = {\n",
      "#     'emploi':2,\n",
      "#     'logement':1,\n",
      "#     'education':1,\n",
      "#     'soutien':1,\n",
      "#     'mobilité':0,\n",
      "#     'commune_actuelle':'33281',\n",
      "#     'loc_distance_km':10,\n",
      "#     'codes_metiers':{\n",
      "#         'codes_metiers_adulte1':['S1X40','J0X33','A1X41'],\n",
      "#         'codes_metiers_adulte2':['T4X60','T2A60']\n",
      "#     },\n",
      "#     'codes_formations':{\n",
      "#         'codes_formations_adulte1':['320'],\n",
      "#         'codes_formations_adulte2':['333','100']\n",
      "#     },\n",
      "#     'age_enfants':{\n",
      "#         'age_enfant1':4,\n",
      "#         'age_enfant2':10,\n",
      "#         'age_enfant3':None,\n",
      "#         'age_enfant4':None,\n",
      "#         'age_enfant5':None\n",
      "#     }\n",
      "# }\n",
      "#Computing distance from current commune \n",
      "#Using a crs that allows to compute distance in meters for metropolitan France\n",
      "\n",
      "def distance_calc(df, ref_point):\n",
      "    return int(df.distance(ref_point))\n",
      "\n",
      "def add_distance_to_current_loc(df, current_codgeo):\n",
      "    projected_crs = \"EPSG:2154\"\n",
      "    zone_recherche = gpd.GeoDataFrame(df[df.codgeo == current_codgeo]['polygon'])\n",
      "    zone_recherche.set_geometry('polygon', inplace=True)\n",
      "    zone_recherche.to_crs(projected_crs, inplace=True)\n",
      "\n",
      "    df.to_crs(projected_crs, inplace=True)\n",
      "    df['dist_current_loc'] = df['polygon'].apply(distance_calc, ref_point=zone_recherche.iloc[0].polygon)\n",
      "    return df\n",
      "#add_distance_to_current_loc(odis, current_codgeo=LOC_COMMUNE_ACTUELLE)\n",
      "#Adding score specific to subject looking for a job identified as en besoin\n",
      "def codes_match(df, codes_list):\n",
      "    #returns a list of codfaps that matches\n",
      "    if df is None:\n",
      "        return []\n",
      "    return list(set(df.tolist()).intersection(set(codes_list)))\n",
      "\n",
      "def fap_names_lookup(df):\n",
      "    return list(codfap_index[codfap_index['Code FAP 341'].isin(df)]['Intitulé FAP 341'])\n",
      "\n",
      "def compute_subject_specific_scores(df, scores_cat, subject_pref): \n",
      "    # Let's create subject-specific scores\n",
      "    t = preprocessing.QuantileTransformer(output_distribution=\"uniform\")\n",
      "    #For each adult we look for jobs categories that match what is needed\n",
      "    i=1\n",
      "    for adult in subject_pref['codes_metiers']:\n",
      "        df['met_match_codes_adult'+str(i)] = df.be_codfap_top.apply(codes_match, codes_list=subject_pref['codes_metiers'][adult])\n",
      "        df['met_match_adult'+str(i)] = df['met_match_codes_adult'+str(i)].apply(len)\n",
      "        df['met_match_adult'+str(i)+'_scaled'] = t.fit_transform(df[['met_match_adult'+str(i)]].fillna(0))\n",
      "        i+=1\n",
      "\n",
      "    j=1\n",
      "    for adult in subject_pref['codes_formations']:\n",
      "        df['form_match_codes_adult'+str(j)] = df.codes_formations.apply(codes_match, codes_list=subject_pref['codes_formations'][adult])\n",
      "        df['form_match_adult'+str(j)] = df['form_match_codes_adult'+str(j)].apply(len)\n",
      "        df['form_match_adult'+str(j)+'_scaled'] = t.fit_transform(df[['form_match_adult'+str(j)]].fillna(0))\n",
      "        j+=1\n",
      "\n",
      "    # We compute the distance from the current location \n",
      "    df['reloc_dist_scaled'] = (1-df['dist_current_loc']/(subject_pref['loc_distance_km']*1000))\n",
      "    df['reloc_epci_scaled'] = np.where(df['epci_code'] == df[df.codgeo == subject_pref['commune_actuelle']]['epci_code'].iloc[0],1,0)\n",
      "    \n",
      "    #Let's not forget to categorize these new scores to the existing score_cat index if it doesnt exist\n",
      "    #if (scores_cat.score != 'met_match_scaled').all():\n",
      "    scores_cat_subject = pd.DataFrame([\n",
      "        {'score':'met_match_adult1_scaled','score_name':'Match compétences et Besoin Emploi Adult 1','cat':'emploi'},\n",
      "        {'score':'met_match_adult2_scaled','score_name':'Match compétences et Besoin Emploi Adult 2','cat':'emploi'},\n",
      "        {'score':'form_match_adult1_scaled','score_name':'Match compétences et Centres de formation','cat':'emploi'},\n",
      "        {'score':'form_match_adult2_scaled','score_name':'Match compétences et Centres de formation','cat':'emploi'},\n",
      "        {'score':'reloc_dist_scaled','score_name':'Distance de la localisation actuelle','cat':'mobilité'},\n",
      "        {'score':'reloc_epci_scaled','score_name':'Même EPCI que la localisation actuelle','cat':'mobilité'}\n",
      "        ])\n",
      "    scores_cat_subject = pd.concat([scores_cat, scores_cat_subject])\n",
      "    #scores_cat = pd.concat([scores_cat, scores_cat_subject])\n",
      "    return df, scores_cat_subject\n",
      "# Let's create subject-specific scores\n",
      "#scores_cat_subject = compute_subject_specific_scores(odis, scores_cat=scores_cat)\n",
      "# Filtering dataframe based on subject distance preference (to save on compute time later on)\n",
      "def filter_loc_by_distance(df, distance):\n",
      "    return df[df.dist_current_loc < distance * 1000]\n",
      "#odis_search = filter_loc_by_distance(odis, distance=LOC_DISTANCE_KM)\n",
      "def adding_score_voisins(df_search, df_source):\n",
      "    #df_search is the dataframe pre-filtered by location\n",
      "    #df_source is the dataframe with all the communes\n",
      "    binome_columns = ['codgeo','libgeo','codgeo_voisins','polygon','epci_code','epci_nom']+[col for col in df_source.columns if col.startswith('met_match_codes')]+[col for col in df_source.columns if col.endswith('_scaled')] \n",
      "\n",
      "    # Adds itself to list of voisins = monome case\n",
      "    # Note: this code triggers the SettingWithCopyWarning but I don't know how to fix it...\n",
      "    df_search = df_search.copy()\n",
      "    df_search.codgeo_voisins = df_search.apply(lambda x: np.append(x.codgeo_voisins, x.codgeo), axis=1)\n",
      "\n",
      "    # Explodes the dataframe to have a row for each voisins + itself\n",
      "    df_search['codgeo_voisins_copy'] = df_search['codgeo_voisins']\n",
      "    df_search_exploded = df_search.explode('codgeo_voisins_copy')\n",
      "    \n",
      "    # For each commune (codgeo) in search area (df_search) we add all its voisin's scores\n",
      "    odis_search_exploded = pd.merge(df_search_exploded, df_source[binome_columns].add_suffix('_binome'), left_on='codgeo_voisins_copy', right_on='codgeo_binome', how='left')\n",
      "    \n",
      "    # Adds a column to identify binomes vs monomes + cleanup\n",
      "    odis_search_exploded['binome'] = odis_search_exploded.apply(lambda x: False if x.codgeo == x.codgeo_binome else True, axis=1)\n",
      "    odis_search_exploded.drop(columns={'codgeo_voisins_copy'}, inplace=True)\n",
      "    \n",
      "    return odis_search_exploded\n",
      "#odis_exploded = adding_score_voisins(odis_search, odis)\n",
      "def compute_cat_scores(df, scores_cat):\n",
      "    df = df.copy()\n",
      "    for cat in set(scores_cat.cat):\n",
      "        cat_scores_indices = scores_cat[scores_cat['cat'] == cat]['score'].tolist()\n",
      "        cat_scores_indices_binome = [score+'_binome' for score in cat_scores_indices]\n",
      "\n",
      "        # Efficiently select all relevant rows at once\n",
      "        cat_scores_df = df[cat_scores_indices]\n",
      "        cat_scores_binome_df = df[cat_scores_indices_binome]\n",
      "        \n",
      "        # Calculate the mean of the selected rows\n",
      "        df[cat + '_cat_score'] = 100 * cat_scores_df.astype(float).mean(axis=1).round(3)\n",
      "        df[cat + '_cat_score_binome'] = 100 * cat_scores_binome_df.astype(float).mean(axis=1).round(3)\n",
      "\n",
      "    return df\n",
      "#COM_LIMITROPHE_PENALTY = 0.1 #décote de 10% pour les communes limitrophes vs commune cible\n",
      "\n",
      "def compute_binome_score(df, binome_penalty, weights):\n",
      "    scores_col = [col for col in df.columns if col.endswith('_cat_score')]\n",
      "    max_scores = pd.DataFrame()\n",
      "    \n",
      "    for col in scores_col:\n",
      "        cat_weight = weights[col.split('_')[0]]\n",
      "        max_scores[col] = cat_weight * np.where(\n",
      "            df[col] >= (1-binome_penalty)*df[col+'_binome'],\n",
      "            df[col],\n",
      "            (1-binome_penalty)*df[col+'_binome']\n",
      "            )\n",
      "    \n",
      "    return max_scores.mean(axis=1).round(1)\n",
      "def monome_cleanup(df):\n",
      "    if df.loc['binome'] == False:\n",
      "        for col in df.index:\n",
      "            if col.endswith('_binome'):\n",
      "                df.loc[col] = None\n",
      "    return df\n",
      "\n",
      "def best_score_compute(df):\n",
      "    #Keeping the best (top #1) monome or binome result for each commune\n",
      "    best = df.sort_values('weighted_score', ascending=False).groupby('codgeo').head(1)\n",
      "    #Cleanup redundant data in the monome cases\n",
      "    best = best.apply(monome_cleanup, axis=1)\n",
      "    return best\n",
      "\n",
      "#odis_search_best = best_score_compute(odis_exploded)\n",
      "#Main function that aggregates most of the above in one sequence\n",
      "def compute_odis_score(df, scores_cat, prefs):\n",
      "    # Note: we consider that the dataframe already has all the commune-specific criteria scores\n",
      "    # Note2: the prefs dict has all the subject preferences\n",
      "    \n",
      "    # We add the distance between each commune and the one set as where the subject currently lives and filter based on distance pref\n",
      "    df = add_distance_to_current_loc(df, current_codgeo=prefs['commune_actuelle'])\n",
      "\n",
      "    # We compute the subject specific scores\n",
      "    df, scores_cat_subject = compute_subject_specific_scores(df, scores_cat=scores_cat, subject_pref=prefs)\n",
      "    \n",
      "    # We filter by distance to reduce the compute cost on a smaller odis_search dataframe\n",
      "    odis_search = filter_loc_by_distance(df, distance=prefs['loc_distance_km'])\n",
      "\n",
      "    # We add the criteria scores for all neighbor communes forming monomes and binomes\n",
      "    odis_exploded = adding_score_voisins(odis_search, df)\n",
      "\n",
      "    # We compute the category scores for both the target and the binome\n",
      "    odis_exploded = compute_cat_scores(odis_exploded, scores_cat=scores_cat_subject)\n",
      "    \n",
      "    # We provide the scores columns as a parameter to compute faster\n",
      "    #scores_col = [col for col in odis_exploded.columns if col.endswith('_cat_score')]\n",
      "    \n",
      "    # We computing the final weighted score for all commune<->voisin combinations\n",
      "    odis_exploded['weighted_score'] = compute_binome_score(\n",
      "        odis_exploded,\n",
      "        binome_penalty=prefs['binome_penalty'],\n",
      "        weights=prefs\n",
      "        )\n",
      "    \n",
      "    # We keep best monome or binome for each commune \n",
      "    odis_search_best = best_score_compute(odis_exploded)\n",
      "\n",
      "    return odis_search_best, scores_cat_subject\n",
      "def produce_pitch(df, prefs, scores_cat, codfap_index, codformations_index):\n",
      "    pitch_lines = []\n",
      "    pitch_lines += [df.loc['libgeo'] +'dans l\\'EPCI: '+ df.loc['epci_nom']]\n",
      "    pitch_lines += ['Le score est de: '+str(df.loc['weighted_score'])]\n",
      "    if df.loc['binome']:\n",
      "        pitch_lines += ['Ce score est obtenu en binome avec la commune '+df.loc['libgeo_binome']]\n",
      "        if df.loc['epci_code'] != df.loc['epci_code_binome']:\n",
      "            pitch_lines += ['Cette commune est située dans l\\'EPCI: '+df.loc['epci_nom_binome']]\n",
      "    else:\n",
      "        pitch_lines += ['Ce score est obtenu sans commune binôme']\n",
      "\n",
      "    \n",
      "    #Adding the top contributin criterias\n",
      "    crit_scores_col = [col for col in df.index if '_scaled' in col]#col.endswith('_scaled')]\n",
      "    \n",
      "    df_sorted=df[crit_scores_col].dropna().sort_values(ascending=False)\n",
      "    for i in range(0, 5):\n",
      "        score = df_sorted.index[i][:-7] if df_sorted.index[i].endswith('_binome') else df_sorted.index[i]\n",
      "        score_name = scores_cat[scores_cat.score == score]['score_name'].item()\n",
      "        pitch_lines += ['Le critère #'+str(i+1)+' est: '+score_name+' avec un score de: '+str(df_sorted.iloc[i])]\n",
      "\n",
      "    \n",
      "    #Adding the matching job families if any\n",
      "    metiers_col = [col for col in df.index if col.startswith('met_match_codes')]\n",
      "    for metiers_adultx in metiers_col:\n",
      "        matched_codfap_names = list(codfap_index[codfap_index['Code FAP 341'].isin(df.loc[metiers_adultx])]['Intitulé FAP 341'])\n",
      "        if len(matched_codfap_names) == 1:\n",
      "            pitch_lines += ['La famille de métiers '+ matched_codfap_names[0] +' est rechechée'] \n",
      "        elif len(matched_codfap_names) >= 1:\n",
      "            list_jobs = ''\n",
      "            for job in matched_codfap_names:\n",
      "                list_jobs += job+', '\n",
      "            pitch_lines += ['Les familles de métiers '+ list_jobs[:-2] +' sont rechechées']\n",
      "    \n",
      "    return pitch_lines\n",
      "    #for line in pitch_lines:\n",
      "    #    print(line)\n",
      "# codfap_index = pd.read_csv('../csv/dares_nomenclature_fap2021.csv', delimiter=';')\n",
      "# codformations_index = pd.read_csv('../csv/index_formations.csv').set_index('codformation')\n",
      "# for index, row in odis_search_best.head(1).iterrows():\n",
      "#     pitch = produce_pitch(row, prefs=prefs, scores_cat=scores_cat, codfap_index=codfap_index, codformations_index=codformations_index)\n",
      "# pitch\n",
      "def produce_html_tooltip(df):\n",
      "    tooltip = '<div>'\n",
      "    tooltip += '<div><strong>'+df.loc['libgeo'] +'</strong> dans l\\'EPCI: '+ df.loc['epci_nom']+'</div>'\n",
      "    tooltip += '<div>'+'Le score est de: <strong>'+str(df.loc['weighted_score'])+'</strong></div>'\n",
      "    tooltip += '<div style=\"background-color:white;height:20px;width:200px;\">'\n",
      "    tooltip += '<div style=\"background-color:green;height:20px;width:'+str(df.loc['weighted_score']*2)+'px;\"></div></div>'\n",
      "    \n",
      "    if df.loc['binome']:\n",
      "        tooltip += '<div>'+'Ce score est obtenu en binome avec la commune '+df.loc['libgeo_binome']+'</div>'\n",
      "        if df.loc['epci_code'] != df.loc['epci_code_binome']:\n",
      "            tooltip += '<div>'+'Cette commune est située dans l\\'EPCI: '+df.loc['epci_nom_binome']+'</div>'\n",
      "    else:\n",
      "        tooltip += '<div>'+'Ce score est obtenu sans commune binôme'+'</div>'\n",
      "\n",
      "    \n",
      "    #Adding the top contributin criterias\n",
      "    crit_scores_col = [col for col in df.index if '_scaled' in col]#col.endswith('_scaled')]\n",
      "    cat_scores_col = [col for col in df.index if col.endswith('_cat_score')]\n",
      "    df_sorted=df[cat_scores_col].dropna().sort_values(ascending=False)\n",
      "    for i in range(0, 5):\n",
      "        tooltip += '<div>'+'Le critère #'+str(i+1)+' est: '+df_sorted.index[i]+' avec un score de: '+str(df_sorted.iloc[i])+'</div>'\n",
      "\n",
      "    #Adding the matching job families if any\n",
      "    matched_codfap_names = list(codfap_index[codfap_index['Code FAP 341'].isin(df.loc['met_match_codes'])]['Intitulé FAP 341'])\n",
      "    if len(matched_codfap_names) == 1:\n",
      "        tooltip += '<div>'+'La famille de métiers '+ str(matched_codfap_names) +' est rechechée'+'</div>' \n",
      "    elif len(matched_codfap_names) >= 1:\n",
      "        tooltip += '<div>'+'Les familles de métiers '+ str(matched_codfap_names) +' sont rechechées'+'</div>'\n",
      "    tooltip += '</div>'\n",
      "    \n",
      "    return tooltip\n",
      "# THIS SHOULD BE THE END OF JUPYTER NOTEBOOK EXPORT\n"
     ]
    }
   ],
   "source": [
    "%save -f -r ../streamlit/odisscoring.py 1-20\n",
    "# This saves the cells 0 to 22 (and their execution history unfortunately) to a python file that I can use in Streamlit\n",
    "# Make sure to restart before running this cell\n",
    "# Don't forget to restart streamlit after this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ae6d547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart and run all the cells above this one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9bf13d",
   "metadata": {},
   "source": [
    "## Notebook explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65293654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init\n",
    "#init_libraries()\n",
    "\n",
    "ODIS_FILE = '../csv/odis_april_2025_jacques.parquet'\n",
    "METIERS_FILE = '../csv/dares_nomenclature_fap2021.csv'\n",
    "FORMATIONS_FILE = '../csv/index_formations.csv'\n",
    "ECOLES_FILE = '../csv/annuaire_ecoles_france_mini.parquet'\n",
    "\n",
    "odis, codfap_index, codformations_index, annuaire_ecoles = init_loading_datasets(\n",
    "    odis_file=ODIS_FILE,\n",
    "    metiers_file=METIERS_FILE,\n",
    "    formations_file=FORMATIONS_FILE,\n",
    "    ecoles_file=ECOLES_FILE\n",
    "    )\n",
    "\n",
    "odis, scores_cat = init_communes_criterias(odis)\n",
    "\n",
    "# Subject preferences weighted score computation\n",
    "prefs = {\n",
    "    'emploi':2,\n",
    "    'logement':1,\n",
    "    'education':1,\n",
    "    'soutien':1,\n",
    "    'mobilité':0,\n",
    "    'commune_actuelle':'33281',\n",
    "    'loc_distance_km':10,\n",
    "    'codes_metiers':{\n",
    "        'codes_metiers_adulte1':['S1X40','J0X33','A1X41'],\n",
    "        'codes_metiers_adulte2':['T4X60','T2A60']\n",
    "    },\n",
    "    'codes_formations':{\n",
    "        'codes_formations_adulte1':[423],\n",
    "        'codes_formations_adulte2':[315,100]\n",
    "    },\n",
    "    'age_enfants':{\n",
    "        'age_enfant1':4,\n",
    "        'age_enfant2':10,\n",
    "        'age_enfant3':None,\n",
    "        'age_enfant4':None,\n",
    "        'age_enfant5':None\n",
    "    },\n",
    "    'binome_penalty':0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b63584a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing Weighted Scores given a source dataframe with geo info and a scoring categorisation\n",
    "odis_search_best, scores_cat = compute_odis_score(odis, scores_cat=scores_cat, prefs=prefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8c72c542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing results on an interactive map\n",
    "cols_to_show = (\n",
    "        ['codgeo','libgeo','weighted_score','binome','libgeo_binome','dist_current_loc','polygon']\n",
    "        +[col for col in odis_search_best.columns if '_codes_' in col]\n",
    "        +[col for col in odis_search_best.columns if '_cat_score' in col]\n",
    "        )\n",
    "#odis_search_best[cols_to_show].explore('weighted_score', popup=True)\n",
    "#odis_search_best.plot('weighted_score')\n",
    "#odis_search_best[cols_to_show].explore('weighted_score', tooltip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60186409",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydict = {\n",
    "    'Scores': ['toto'],\n",
    "    'Scores+top1': ['toto'],\n",
    "    'Scores+top2': ['toto'],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a223adf",
   "metadata": {},
   "source": [
    "## 9. Export to SuperSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e08dbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def concatenate_strings(row):\n",
    "#   return '{\"type\": \"Feature\",\"geometry\":' + shp.to_geojson(row['polygon']) + '}'\n",
    "\n",
    "\n",
    "# odis_search_best_export = gpd.GeoDataFrame(odis_search_best.copy())\n",
    "# odis_search_best_export.set_geometry(odis_search_best_export.polygon, crs='EPSG:2154', inplace=True)\n",
    "# odis_search_best_export.to_crs(epsg=4326, inplace=True)\n",
    "# odis_search_best_export[\"polygon_as_json\"] = odis_search_best_export.apply(concatenate_strings, axis=1)\n",
    "# odis_search_best_export.drop(['polygon','polygon_binome'], axis=1, inplace=True)\n",
    "\n",
    "# cols = ['met_match_codes','met_match_codes_binome','be_codfap_top','be_libfap_top','codgeo_voisins_binome','pitch']\n",
    "# for col in cols:\n",
    "#     odis_search_best_export[col] = odis_search_best_export[col].apply(lambda x: x.tolist() if type(x) == np.ndarray else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "93370656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sqlalchemy import create_engine, text\n",
    "\n",
    "# db_host = \"localhost\"  # Replace with the actual host (e.g., 'superset_db' if in the same Docker network, or 'localhost' if exposed)\n",
    "# db_port = \"5433\"  # Replace with the actual port (usually 5432)\n",
    "# db_user = \"superset\"  # Replace with the database user (often 'superset')\n",
    "# db_password = \"superset\"  # Replace with the database password\n",
    "# db_name = \"examples\"  # Replace with the database name (often 'superset')\n",
    "\n",
    "# engine = create_engine(f'postgresql+psycopg2://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71aaddc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# table_name = \"odis_stream2_result\"  # Choose a name for the table in PostgreSQL\n",
    "# odis_search_best_export.to_sql(table_name, engine, if_exists='replace', schema='public', index=False)\n",
    "# sql = text(\"GRANT SELECT ON odis_stream2_result TO examples\")\n",
    "\n",
    "# with engine.begin() as connection:\n",
    "#     connection.execute(sql)\n",
    "\n",
    "# print(f\"DataFrame successfully written to table '{table_name}' in the Superset database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4674265d",
   "metadata": {},
   "source": [
    "Note to myself:\n",
    "Après avoir importé les données dans Postgres il faut donner les droits au user 'examples' sur la table\n",
    "> docker exec -it superset_db psql -h superset_db -p 5432 -U superset -d examples\n",
    "\n",
    "> GRANT SELECT ON odis_stream2_result TO examples;\n",
    "\n",
    "> GRANT USAGE ON SCHEMA public TO examples;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff23fc33",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "123893e3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "odis-Nf-mTAVv-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
