{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3acd4d8f",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6c35fc",
   "metadata": {},
   "source": [
    "### Scoring Logic Description\n",
    "\n",
    "1. We start by importing the odis dataframe from a CSV that includes all the relevant datapoint to score and display data\n",
    "2. We compute the scores for each criteria specific to the commune (independant from subject)\n",
    "3. We compute the scores for each criteria specific to the subject (dependand from both subject and commune) \n",
    "4. We identify all commune<->neighbour pairs (binômes) for each commune within search radius\n",
    "5. We compute category scores (emploi, logement, education etc...) as an average of the all the scores for a given category\n",
    "6. For each commune we compare the commune and neighbour category scores and weighted the highest one with category weights defined by subject and then keep the best weighted score for each commune\n",
    "8. We display result in on a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16add7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS SHOULD BE THE BEGINNING OF JUPYTER NOTEBOOK EXPORT\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from scipy import stats\n",
    "import folium as flm #required for gdf.explore()\n",
    "import shapely as shp\n",
    "from shapely.wkt import loads\n",
    "from shapely.geometry import Polygon\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7744f22f",
   "metadata": {},
   "source": [
    "### 1. Fetching key indicators from ODIS source file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d325cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_loading_datasets(odis_file, scores_cat_file, metiers_file, formations_file, ecoles_file, maternites_file, sante_file, inclusion_file):\n",
    "    odis = gpd.GeoDataFrame(gpd.read_parquet(odis_file))\n",
    "    odis.set_geometry(odis.polygon, inplace=True)\n",
    "    odis.polygon.set_precision(10**-5)\n",
    "    odis = odis[~odis.polygon.isna()]\n",
    "    odis.set_index('codgeo', inplace=True)\n",
    "\n",
    "    # Index of all scores and their explanations\n",
    "    scores_cat = pd.read_csv(scores_cat_file)\n",
    "\n",
    "    #Later we need the code FAP <-> FAP Name used to classify jobs\n",
    "    codfap_index = pd.read_csv(metiers_file, delimiter=';')\n",
    "\n",
    "    # Later we need the code formation <-> Formation Name used to classify trainings\n",
    "    # source: https://www.data.gouv.fr/fr/datasets/liste-publique-des-organismes-de-formation-l-6351-7-1-du-code-du-travail/\n",
    "    codformations_index = pd.read_csv(formations_file).set_index('codformation')\n",
    "\n",
    "    # Etablissements scolaires\n",
    "    annuaire_ecoles = pd.read_parquet(ecoles_file)\n",
    "    annuaire_ecoles.geometry = annuaire_ecoles.geometry.apply(shp.from_wkb)\n",
    "\n",
    "    #Annuaire Maternités\n",
    "    # Source: https://www.data.gouv.fr/fr/datasets/liste-des-maternites-de-france-depuis-2000/\n",
    "    annuaire_maternites = pd.read_csv(maternites_file, delimiter=';')\n",
    "    annuaire_maternites.drop_duplicates(subset=['FI_ET'], keep='last', inplace=True)\n",
    "    annuaire_maternites.head()\n",
    "\n",
    "    # Annuaire etablissements santé\n",
    "    # Source: https://www.data.gouv.fr/fr/datasets/reexposition-des-donnees-finess/\n",
    "    annuaire_sante = pd.read_parquet(sante_file)\n",
    "    annuaire_sante = annuaire_sante[annuaire_sante.LibelleSph == 'Etablissement public de santé']\n",
    "    annuaire_sante['geometry'] = gpd.points_from_xy(annuaire_sante.coordxet, annuaire_sante.coordyet, crs='epsg:2154')\n",
    "    annuaire_sante = pd.merge(annuaire_sante, annuaire_maternites[['FI_ET']], left_on='nofinesset', right_on='FI_ET', how='left', indicator=\"maternite\")\n",
    "    annuaire_sante.drop(columns=['FI_ET'], inplace=True)\n",
    "    annuaire_sante.maternite = np.where(annuaire_sante.maternite == 'both', True, False)\n",
    "    annuaire_sante['codgeo'] = annuaire_sante.Departement + annuaire_sante.Commune\n",
    "\n",
    "    # Annuaire des services d'inclusion\n",
    "    annuaire_inclusion = gpd.read_parquet(inclusion_file)\n",
    "    incl_index=annuaire_inclusion[['codgeo', 'categorie', 'service']].drop_duplicates()\n",
    "    incl_index['key'] = incl_index.categorie+'_'+incl_index.service\n",
    "    incl_index=incl_index.groupby('codgeo').agg({'key':lambda x: set(x)})\n",
    "\n",
    "    return odis, scores_cat, codfap_index, codformations_index, annuaire_ecoles, annuaire_sante, annuaire_inclusion, incl_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a250e2d3",
   "metadata": {},
   "source": [
    "### 2. Distance filter + Gathering nearby Communes Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd47e8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering dataframe based on subject distance preference (to save on compute time later on)\n",
    "def filter_loc_by_distance(df, distance):\n",
    "    return df[df.dist_current_loc < distance * 1000]\n",
    "\n",
    "# Put None as a score in the monome case\n",
    "def monome_cleanup(df):\n",
    "    mask = ~df['binome']\n",
    "    for col in df.columns:\n",
    "        if col.endswith('_binome'):\n",
    "            df.loc[mask, col] = None\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9898c8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adding_score_voisins(df_search, scores_cat):\n",
    "    #df_search is the dataframe pre-filtered by location\n",
    "\n",
    "    binome_columns = ['codgeo','libgeo','polygon','epci_code','epci_nom'] + scores_cat[scores_cat.incl_binome]['score'].to_list()+scores_cat[scores_cat.incl_binome]['metric'].to_list()\n",
    "    # We take the subset of possible score columns that actually exist in our dataframe\n",
    "    binome_columns = list(set(binome_columns) & set(df_search.columns))\n",
    "    df_binomes = df_search[binome_columns].copy()\n",
    "\n",
    "    # Adds itself to list of voisins = monome case\n",
    "    # Note: this code triggers the SettingWithCopyWarning but I don't know how to fix it...\n",
    "    df_search['codgeo_voisins_and_self'] = df_search.apply(lambda x: np.append(x.codgeo_voisins, x.name), axis=1)\n",
    "\n",
    "    # Explodes the dataframe to have a row for each voisins + itself\n",
    "    df_search_exploded = df_search.explode('codgeo_voisins_and_self')\n",
    "    df_search_exploded.rename(columns={'codgeo_voisins_and_self':'codgeo_binome'}, inplace=True)\n",
    "    \n",
    "    # For each commune (codgeo) in search area (df_search) we all its binome's scores (incl. self as a binome)\n",
    "    odis_search_exploded = pd.merge(\n",
    "        df_search_exploded, \n",
    "        df_binomes.add_suffix('_binome'), \n",
    "        left_on='codgeo_binome', \n",
    "        right_index=True, \n",
    "        how='inner', \n",
    "        validate=\"many_to_one\")\n",
    "    \n",
    "    # Adds a column to identify binomes vs monomes + cleanup\n",
    "    odis_search_exploded['binome'] = np.where(odis_search_exploded.index == odis_search_exploded.codgeo_binome, False, True)\n",
    "\n",
    " \n",
    "    #We remove all values for the monome case to avoid accounting for them in the category score calculation\n",
    "    # odis_search_exploded = monome_cleanup(odis_search_exploded)\n",
    "\n",
    "    return odis_search_exploded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58361529",
   "metadata": {},
   "source": [
    "### 3. Criterias Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ffe9535",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing distance from current commune \n",
    "#Using a crs that allows to compute distance in meters for metropolitan France\n",
    "\n",
    "def distance_calc(df, ref_point):\n",
    "    return int(df.distance(ref_point))\n",
    "\n",
    "def add_distance_to_current_loc(df, current_codgeo):\n",
    "    projected_crs = \"EPSG:2154\"\n",
    "    # We first need to change CRS to a projected CRS\n",
    "    df_projected = gpd.GeoDataFrame(df)\n",
    "    df_projected = df_projected.to_crs(projected_crs)\n",
    "\n",
    "    zone_recherche = df_projected[df_projected.index == current_codgeo].copy()\n",
    "    zone_recherche['centroid'] = zone_recherche.centroid\n",
    "    zone_recherche = gpd.GeoDataFrame(zone_recherche, geometry='centroid')\n",
    "    zone_recherche.to_crs(projected_crs, inplace=True)\n",
    "    \n",
    "    df_projected = df_projected.sjoin_nearest(zone_recherche, distance_col=\"dist_current_loc\")[['dist_current_loc']]\n",
    "    df = pd.merge(df, df_projected, left_index=True, right_index=True, how='left')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d2a316b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding score specific to subject looking for a job identified as en besoin\n",
    "def codes_match(df, codes_list):\n",
    "    #returns a list of codfaps that matches\n",
    "    if df is None:\n",
    "        return []\n",
    "    return list(set(df.tolist()).intersection(set(codes_list)))\n",
    "\n",
    "def fap_names_lookup(df):\n",
    "    return list(codfap_index[codfap_index['Code FAP 341'].isin(df)]['Intitulé FAP 341'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c8d1bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_criteria_scores(df, prefs, incl_index): \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Using QuantileTransfer to normalize all scores between 0 and 1 for the region\n",
    "    t = preprocessing.QuantileTransformer(output_distribution=\"uniform\")\n",
    "\n",
    "    # EMPLOI\n",
    "\n",
    "    #met_ration est le ratio d'offres non-pourvues pour 1000 habitants\n",
    "    df['met_ratio']= 1000 * df['met']/df['pop_be']\n",
    "    df['met_scaled'] = t.fit_transform(df[['met_ratio']].fillna(0))\n",
    "    \n",
    "    #met_tension_ratio est le ratio d'offres pour des métiers déclarés en tensions sur la zone (pour 1000 habitants)\n",
    "    # df['met_tension_ratio'] = 1000 * df['met_tension']/df['pop_be']\n",
    "    # df['met_tension_scaled'] = t.fit_transform(df[['met_tension_ratio']].fillna(0))\n",
    "\n",
    "    # jobs categories that match\n",
    "    for adult in range(0,prefs['nb_adultes']):\n",
    "        if len(prefs['codes_metiers'][adult]) > 0:\n",
    "            df['met_match_codes_adult'+str(adult+1)] = df.be_codfap_top.apply(codes_match, codes_list=prefs['codes_metiers'][adult])\n",
    "            df['met_match_adult'+str(adult+1)] = df['met_match_codes_adult'+str(adult+1)].apply(len)\n",
    "            df['met_match_adult'+str(adult+1)+'_scaled'] = t.fit_transform(df[['met_match_adult'+str(adult+1)]].fillna(0))\n",
    "    \n",
    "    # training centers that match\n",
    "    for adult in range(0,prefs['nb_adultes']):\n",
    "        if len(prefs['codes_formations'][adult]) > 0:\n",
    "            df['form_match_codes_adult'+str(adult+1)] = df.codes_formations.apply(codes_match, codes_list=prefs['codes_formations'][adult])\n",
    "            df['form_match_adult'+str(adult+1)] = df['form_match_codes_adult'+str(adult+1)].apply(len)\n",
    "            df['form_match_adult'+str(adult+1)+'_scaled'] = t.fit_transform(df[['form_match_adult'+str(adult+1)]].fillna(0))\n",
    "\n",
    "    \n",
    "    # HEBERGEMENT / LOGEMENT\n",
    "    if prefs['hebergement'] == \"Chez l'habitant\":\n",
    "        #log_5p+_ratio est le ratio de residences principales de 5 pièces ou plus % total residences principales\n",
    "        df['log_5p_ratio'] = df['rp_5+pieces']/df['log_rp']\n",
    "        df['log_5p_scaled'] = t.fit_transform(df[['log_5p_ratio']].fillna(0))\n",
    "    \n",
    "    if prefs['logement'] == \"Logement Social\":\n",
    "        # log_soc_inoccupes = nombre de logements sociaux vacants + vides\n",
    "        df['log_soc_inoc_ratio'] = df['log_soc_inoccupes']/df['log_soc_total'] \n",
    "        df['log_soc_inoc_scaled'] = t.fit_transform(df[['log_soc_inoc_ratio']].fillna(0))\n",
    "    elif prefs['logement'] == \"Location\":\n",
    "        #log_vac_ratio est le ratio de logements vacants de la commune % total logements\n",
    "        df['log_vac_ratio'] = df['log_vac']/df['log_total']\n",
    "        df['log_vac_scaled'] = t.fit_transform(df[['log_vac_ratio']].fillna(0))\n",
    "\n",
    "    \n",
    "    # EDUCATION\n",
    "    if len(prefs['classe_enfants']) > 0: \n",
    "        # Risque de fermeture école: ratio de classe à risque de fermeture % nombre d'écoles\n",
    "        df['risque_fermeture_ratio'] = df['risque_fermeture']/df['ecoles_ct']\n",
    "        df['classes_ferm_scaled'] = t.fit_transform(df[['risque_fermeture_ratio']].fillna(0))\n",
    "\n",
    "    # MOBILITE\n",
    "\n",
    "    # 1. distance from the current location \n",
    "    df['reloc_dist_scaled'] = (1-df['dist_current_loc']/(prefs['loc_distance_km']*1000))\n",
    "    df['reloc_epci_scaled'] = np.where(df['epci_code'] == df.loc[prefs['commune_actuelle']]['epci_code'],1,0)\n",
    "\n",
    "    \n",
    "    # SOUTIEN LOCAL\n",
    "    # other needs facilities\n",
    "    if bool(prefs['besoins_autres']):\n",
    "        # We keep things pretty simple here: for every need type, if there is at least corresponding facility in the same geo or nearby geo we score 1\n",
    "        df['besoins_match'] = 0\n",
    "        for row in df.itertuples():\n",
    "            match_counter = 0\n",
    "            # codgeos = row.codgeo_voisins.tolist()+[row.Index] if row.codgeo_voisins is not None else [row.Index]\n",
    "            codgeos = [row.Index] # For now we simplify and only look at the specific geos and not neighboring communes\n",
    "            for codgeo in codgeos:\n",
    "                if codgeo in incl_index.index:\n",
    "                    for key, values in prefs['besoins_autres'].items():\n",
    "                        for v in values:\n",
    "                            # if v is None:\n",
    "                            #     if key in incl_index.loc[codgeo].item():\n",
    "                            #         match_counter += 1\n",
    "                            # else: \n",
    "                            if key+'_'+v in incl_index.loc[codgeo].item():\n",
    "                                match_counter += 1\n",
    "            df.loc[row.Index, 'besoins_match'] = match_counter\n",
    "        df['besoins_match_scaled'] = t.fit_transform(df[['besoins_match']].fillna(0))\n",
    "    else:\n",
    "        #svc_ratio est le ratio de services d'inclusion de la commune (pour 1000 habitants)\n",
    "        df['svc_incl_ratio'] = 1000 * df['svc_incl_count']/df['pop_be']\n",
    "        df['svc_incl_scaled'] = t.fit_transform(df[['svc_incl_ratio']].fillna(0))\n",
    "\n",
    "    #pol est le score selon la couleur politique (extreme droite = 0, gauche = 1)\n",
    "    df['pol_scaled'] = df[['pol_num']].astype('float')\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae24ab0",
   "metadata": {},
   "source": [
    "### 4. Category Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25e7e3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cat_scores(df, scores_cat, penalty):\n",
    "    # Efficiently compute score in a scalar way (select all relevant rows at once)\n",
    "    df = df.copy()\n",
    "\n",
    "    # For each score category (emploi, education etc.) we compute the score weighted with penalty (e.g. 10%) for neighbors \n",
    "    for cat in set(scores_cat.cat):\n",
    "        # We select the relevant columns for this specific category\n",
    "        cat_scores_indices = [col for col in df.columns if col in scores_cat[scores_cat.cat == cat]['score'].to_list()] \n",
    "\n",
    "        if not cat_scores_indices:\n",
    "            # if there is no computed score relevant for this category we skip it\n",
    "            continue\n",
    "        \n",
    "        cat_scores_indices_binome = [col+'_binome' for col in cat_scores_indices if col+'_binome' in df.columns]\n",
    "        \n",
    "        # First we build the criteria score dataframe with penalty for binome commune\n",
    "        cat_scores_df = pd.concat([df[cat_scores_indices],(1-penalty)*df[cat_scores_indices_binome]], axis=1) \n",
    "\n",
    "        # Second we compare neighbor score to assessed commune and keep the max\n",
    "        for col in cat_scores_indices:\n",
    "            col_binome = col+'_binome'\n",
    "            if col_binome in cat_scores_df.columns:\n",
    "                cat_scores_df[col+'_max'] = cat_scores_df[[col, col_binome]].max(axis=1)\n",
    "            else:\n",
    "                cat_scores_df[col+'_max'] = cat_scores_df[col] # this is the case where the criteria is not relevant for binome comparison\n",
    "\n",
    "        # Third and finally we compute the mean to get the final cat score (NaN values are ignored which is what we want)\n",
    "        df[cat + '_cat_score'] = cat_scores_df.filter(like='_max', axis=1).mean(axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fcebd0",
   "metadata": {},
   "source": [
    "### 5. Final Binome Score Weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "831f4061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_binome_score_old(df, binome_penalty, prefs):\n",
    "    scores_col = [col for col in df.columns if col.endswith('_cat_score')]\n",
    "    max_scores = pd.DataFrame()\n",
    "    \n",
    "    for col in scores_col:\n",
    "        cat_weight = prefs[col.split('_')[0]]\n",
    "        max_scores[col] = cat_weight * np.where(\n",
    "            df[col] >= (1-binome_penalty)*df[col+'_binome'],\n",
    "            df[col],\n",
    "            (1-binome_penalty)*df[col+'_binome']\n",
    "            )\n",
    "    \n",
    "    return max_scores.mean(axis=1).round(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c77eb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_binome_score(df, scores_cat, prefs):\n",
    "    scores_cat_col = [col for col in df.columns if col.endswith('_cat_score')]\n",
    "    weighted_scores = 0\n",
    "    total_weight = 0\n",
    "    for cat_score in scores_cat_col:\n",
    "        if df[cat_score] is not None:\n",
    "            cat_weight =  prefs['poids_'+cat_score.split('_')[0]]\n",
    "            weighted_scores += cat_weight * df[cat_score]\n",
    "            total_weight += cat_weight\n",
    "    \n",
    "    return weighted_scores / total_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3042ad31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_score_compute(df):\n",
    "    #Keeping the best (top #1) monome or binome result for each commune\n",
    "    best = df.sort_values('weighted_score', ascending=False).groupby('codgeo').head(1)\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56483e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main function that aggregates most of the above in one sequence\n",
    "def compute_odis_score(df, scores_cat, prefs, incl_index):\n",
    "    \n",
    "    # We restrict to communes with pop larger than 1000 (30% of all communes)\n",
    "    df = df[df.population > prefs['pop_min']]\n",
    "\n",
    "    # We add the disctance from the current location defined in the app\n",
    "    df = add_distance_to_current_loc(df, current_codgeo=prefs['commune_actuelle'])\n",
    "\n",
    "    # We filter by distance to reduce the compute cost on a smaller odis_search dataframe\n",
    "    odis_search = filter_loc_by_distance(df, distance=prefs['loc_distance_km'])\n",
    "\n",
    "    # We compute the subject specific scores\n",
    "    odis_scored = compute_criteria_scores(odis_search, prefs=prefs, incl_index=incl_index)\n",
    "\n",
    "    # We add the criteria scores for all neighbor communes forming monomes and binomes\n",
    "    odis_exploded = adding_score_voisins(odis_scored, scores_cat)\n",
    "\n",
    "    # We compute the category scores for both the target and the binome\n",
    "    odis_exploded = compute_cat_scores(odis_exploded, scores_cat=scores_cat, penalty=prefs['binome_penalty'])\n",
    "\n",
    "    # We computing the final weighted score for all commune<->voisin combinations\n",
    "    odis_exploded['weighted_score'] = compute_binome_score(odis_exploded, scores_cat=scores_cat, prefs=prefs)\n",
    "\n",
    "    # We keep best monome or binome for each commune \n",
    "    odis_search_best = best_score_compute(odis_exploded)\n",
    "\n",
    "    return odis_search_best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7504e591",
   "metadata": {},
   "source": [
    "### 6. Generating Narrative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1a10f4",
   "metadata": {},
   "source": [
    "Here we want to generate a 'human readable' explanation about why scored high a given location.\n",
    "Things to show:\n",
    "- Target commune name and EPCI\n",
    "- Weighted Score\n",
    "- If Binome, show the binome and EPCI if different from target\n",
    "- Show top 3 criterias target (weighted ?) \n",
    "- Show top 3 criterias for binome (weighted ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4de2b412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS SHOULD BE THE END OF JUPYTER NOTEBOOK EXPORT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663e91e0",
   "metadata": {},
   "source": [
    "## Export to Python file for streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95990aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following commands were written to file `../streamlit/odis_stream2_scoring.py`:\n",
      "# THIS SHOULD BE THE BEGINNING OF JUPYTER NOTEBOOK EXPORT\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import geopandas as gpd\n",
      "from scipy import stats\n",
      "import folium as flm #required for gdf.explore()\n",
      "import shapely as shp\n",
      "from shapely.wkt import loads\n",
      "from shapely.geometry import Polygon\n",
      "from sklearn import preprocessing\n",
      "def init_loading_datasets(odis_file, scores_cat_file, metiers_file, formations_file, ecoles_file, maternites_file, sante_file, inclusion_file):\n",
      "    odis = gpd.GeoDataFrame(gpd.read_parquet(odis_file))\n",
      "    odis.set_geometry(odis.polygon, inplace=True)\n",
      "    odis.polygon.set_precision(10**-5)\n",
      "    odis = odis[~odis.polygon.isna()]\n",
      "    odis.set_index('codgeo', inplace=True)\n",
      "\n",
      "    # Index of all scores and their explanations\n",
      "    scores_cat = pd.read_csv(scores_cat_file)\n",
      "\n",
      "    #Later we need the code FAP <-> FAP Name used to classify jobs\n",
      "    codfap_index = pd.read_csv(metiers_file, delimiter=';')\n",
      "\n",
      "    # Later we need the code formation <-> Formation Name used to classify trainings\n",
      "    # source: https://www.data.gouv.fr/fr/datasets/liste-publique-des-organismes-de-formation-l-6351-7-1-du-code-du-travail/\n",
      "    codformations_index = pd.read_csv(formations_file).set_index('codformation')\n",
      "\n",
      "    # Etablissements scolaires\n",
      "    annuaire_ecoles = pd.read_parquet(ecoles_file)\n",
      "    annuaire_ecoles.geometry = annuaire_ecoles.geometry.apply(shp.from_wkb)\n",
      "\n",
      "    #Annuaire Maternités\n",
      "    # Source: https://www.data.gouv.fr/fr/datasets/liste-des-maternites-de-france-depuis-2000/\n",
      "    annuaire_maternites = pd.read_csv(maternites_file, delimiter=';')\n",
      "    annuaire_maternites.drop_duplicates(subset=['FI_ET'], keep='last', inplace=True)\n",
      "    annuaire_maternites.head()\n",
      "\n",
      "    # Annuaire etablissements santé\n",
      "    # Source: https://www.data.gouv.fr/fr/datasets/reexposition-des-donnees-finess/\n",
      "    annuaire_sante = pd.read_parquet(sante_file)\n",
      "    annuaire_sante = annuaire_sante[annuaire_sante.LibelleSph == 'Etablissement public de santé']\n",
      "    annuaire_sante['geometry'] = gpd.points_from_xy(annuaire_sante.coordxet, annuaire_sante.coordyet, crs='epsg:2154')\n",
      "    annuaire_sante = pd.merge(annuaire_sante, annuaire_maternites[['FI_ET']], left_on='nofinesset', right_on='FI_ET', how='left', indicator=\"maternite\")\n",
      "    annuaire_sante.drop(columns=['FI_ET'], inplace=True)\n",
      "    annuaire_sante.maternite = np.where(annuaire_sante.maternite == 'both', True, False)\n",
      "    annuaire_sante['codgeo'] = annuaire_sante.Departement + annuaire_sante.Commune\n",
      "\n",
      "    # Annuaire des services d'inclusion\n",
      "    annuaire_inclusion = gpd.read_parquet(inclusion_file)\n",
      "    incl_index=annuaire_inclusion[['codgeo', 'categorie', 'service']].drop_duplicates()\n",
      "    incl_index['key'] = incl_index.categorie+'_'+incl_index.service\n",
      "    incl_index=incl_index.groupby('codgeo').agg({'key':lambda x: set(x)})\n",
      "\n",
      "    return odis, scores_cat, codfap_index, codformations_index, annuaire_ecoles, annuaire_sante, annuaire_inclusion, incl_index\n",
      "# Filtering dataframe based on subject distance preference (to save on compute time later on)\n",
      "def filter_loc_by_distance(df, distance):\n",
      "    return df[df.dist_current_loc < distance * 1000]\n",
      "\n",
      "# Put None as a score in the monome case\n",
      "def monome_cleanup(df):\n",
      "    mask = ~df['binome']\n",
      "    for col in df.columns:\n",
      "        if col.endswith('_binome'):\n",
      "            df.loc[mask, col] = None\n",
      "    return df\n",
      "def adding_score_voisins(df_search, scores_cat):\n",
      "    #df_search is the dataframe pre-filtered by location\n",
      "\n",
      "    binome_columns = ['codgeo','libgeo','polygon','epci_code','epci_nom'] + scores_cat[scores_cat.incl_binome]['score'].to_list()+scores_cat[scores_cat.incl_binome]['metric'].to_list()\n",
      "    # We take the subset of possible score columns that actually exist in our dataframe\n",
      "    binome_columns = list(set(binome_columns) & set(df_search.columns))\n",
      "    df_binomes = df_search[binome_columns].copy()\n",
      "\n",
      "    # Adds itself to list of voisins = monome case\n",
      "    # Note: this code triggers the SettingWithCopyWarning but I don't know how to fix it...\n",
      "    df_search['codgeo_voisins_and_self'] = df_search.apply(lambda x: np.append(x.codgeo_voisins, x.name), axis=1)\n",
      "\n",
      "    # Explodes the dataframe to have a row for each voisins + itself\n",
      "    df_search_exploded = df_search.explode('codgeo_voisins_and_self')\n",
      "    df_search_exploded.rename(columns={'codgeo_voisins_and_self':'codgeo_binome'}, inplace=True)\n",
      "    \n",
      "    # For each commune (codgeo) in search area (df_search) we all its binome's scores (incl. self as a binome)\n",
      "    odis_search_exploded = pd.merge(\n",
      "        df_search_exploded, \n",
      "        df_binomes.add_suffix('_binome'), \n",
      "        left_on='codgeo_binome', \n",
      "        right_index=True, \n",
      "        how='inner', \n",
      "        validate=\"many_to_one\")\n",
      "    \n",
      "    # Adds a column to identify binomes vs monomes + cleanup\n",
      "    odis_search_exploded['binome'] = np.where(odis_search_exploded.index == odis_search_exploded.codgeo_binome, False, True)\n",
      "\n",
      " \n",
      "    #We remove all values for the monome case to avoid accounting for them in the category score calculation\n",
      "    # odis_search_exploded = monome_cleanup(odis_search_exploded)\n",
      "\n",
      "    return odis_search_exploded\n",
      "#Computing distance from current commune \n",
      "#Using a crs that allows to compute distance in meters for metropolitan France\n",
      "\n",
      "def distance_calc(df, ref_point):\n",
      "    return int(df.distance(ref_point))\n",
      "\n",
      "def add_distance_to_current_loc(df, current_codgeo):\n",
      "    projected_crs = \"EPSG:2154\"\n",
      "    # We first need to change CRS to a projected CRS\n",
      "    df_projected = gpd.GeoDataFrame(df)\n",
      "    df_projected = df_projected.to_crs(projected_crs)\n",
      "\n",
      "    zone_recherche = df_projected[df_projected.index == current_codgeo].copy()\n",
      "    zone_recherche['centroid'] = zone_recherche.centroid\n",
      "    zone_recherche = gpd.GeoDataFrame(zone_recherche, geometry='centroid')\n",
      "    zone_recherche.to_crs(projected_crs, inplace=True)\n",
      "    \n",
      "    df_projected = df_projected.sjoin_nearest(zone_recherche, distance_col=\"dist_current_loc\")[['dist_current_loc']]\n",
      "    df = pd.merge(df, df_projected, left_index=True, right_index=True, how='left')\n",
      "    \n",
      "    return df\n",
      "#Adding score specific to subject looking for a job identified as en besoin\n",
      "def codes_match(df, codes_list):\n",
      "    #returns a list of codfaps that matches\n",
      "    if df is None:\n",
      "        return []\n",
      "    return list(set(df.tolist()).intersection(set(codes_list)))\n",
      "\n",
      "def fap_names_lookup(df):\n",
      "    return list(codfap_index[codfap_index['Code FAP 341'].isin(df)]['Intitulé FAP 341'])\n",
      "def compute_criteria_scores(df, prefs, incl_index): \n",
      "    df = df.copy()\n",
      "    \n",
      "    # Using QuantileTransfer to normalize all scores between 0 and 1 for the region\n",
      "    t = preprocessing.QuantileTransformer(output_distribution=\"uniform\")\n",
      "\n",
      "    # EMPLOI\n",
      "\n",
      "    #met_ration est le ratio d'offres non-pourvues pour 1000 habitants\n",
      "    df['met_ratio']= 1000 * df['met']/df['pop_be']\n",
      "    df['met_scaled'] = t.fit_transform(df[['met_ratio']].fillna(0))\n",
      "    \n",
      "    #met_tension_ratio est le ratio d'offres pour des métiers déclarés en tensions sur la zone (pour 1000 habitants)\n",
      "    # df['met_tension_ratio'] = 1000 * df['met_tension']/df['pop_be']\n",
      "    # df['met_tension_scaled'] = t.fit_transform(df[['met_tension_ratio']].fillna(0))\n",
      "\n",
      "    # jobs categories that match\n",
      "    for adult in range(0,prefs['nb_adultes']):\n",
      "        if len(prefs['codes_metiers'][adult]) > 0:\n",
      "            df['met_match_codes_adult'+str(adult+1)] = df.be_codfap_top.apply(codes_match, codes_list=prefs['codes_metiers'][adult])\n",
      "            df['met_match_adult'+str(adult+1)] = df['met_match_codes_adult'+str(adult+1)].apply(len)\n",
      "            df['met_match_adult'+str(adult+1)+'_scaled'] = t.fit_transform(df[['met_match_adult'+str(adult+1)]].fillna(0))\n",
      "    \n",
      "    # training centers that match\n",
      "    for adult in range(0,prefs['nb_adultes']):\n",
      "        if len(prefs['codes_formations'][adult]) > 0:\n",
      "            df['form_match_codes_adult'+str(adult+1)] = df.codes_formations.apply(codes_match, codes_list=prefs['codes_formations'][adult])\n",
      "            df['form_match_adult'+str(adult+1)] = df['form_match_codes_adult'+str(adult+1)].apply(len)\n",
      "            df['form_match_adult'+str(adult+1)+'_scaled'] = t.fit_transform(df[['form_match_adult'+str(adult+1)]].fillna(0))\n",
      "\n",
      "    \n",
      "    # HEBERGEMENT / LOGEMENT\n",
      "    if prefs['hebergement'] == \"Chez l'habitant\":\n",
      "        #log_5p+_ratio est le ratio de residences principales de 5 pièces ou plus % total residences principales\n",
      "        df['log_5p_ratio'] = df['rp_5+pieces']/df['log_rp']\n",
      "        df['log_5p_scaled'] = t.fit_transform(df[['log_5p_ratio']].fillna(0))\n",
      "    \n",
      "    if prefs['logement'] == \"Logement Social\":\n",
      "        # log_soc_inoccupes = nombre de logements sociaux vacants + vides\n",
      "        df['log_soc_inoc_ratio'] = df['log_soc_inoccupes']/df['log_soc_total'] \n",
      "        df['log_soc_inoc_scaled'] = t.fit_transform(df[['log_soc_inoc_ratio']].fillna(0))\n",
      "    elif prefs['logement'] == \"Location\":\n",
      "        #log_vac_ratio est le ratio de logements vacants de la commune % total logements\n",
      "        df['log_vac_ratio'] = df['log_vac']/df['log_total']\n",
      "        df['log_vac_scaled'] = t.fit_transform(df[['log_vac_ratio']].fillna(0))\n",
      "\n",
      "    \n",
      "    # EDUCATION\n",
      "    if len(prefs['classe_enfants']) > 0: \n",
      "        # Risque de fermeture école: ratio de classe à risque de fermeture % nombre d'écoles\n",
      "        df['risque_fermeture_ratio'] = df['risque_fermeture']/df['ecoles_ct']\n",
      "        df['classes_ferm_scaled'] = t.fit_transform(df[['risque_fermeture_ratio']].fillna(0))\n",
      "\n",
      "    # MOBILITE\n",
      "\n",
      "    # 1. distance from the current location \n",
      "    df['reloc_dist_scaled'] = (1-df['dist_current_loc']/(prefs['loc_distance_km']*1000))\n",
      "    df['reloc_epci_scaled'] = np.where(df['epci_code'] == df.loc[prefs['commune_actuelle']]['epci_code'],1,0)\n",
      "\n",
      "    \n",
      "    # SOUTIEN LOCAL\n",
      "    # other needs facilities\n",
      "    if bool(prefs['besoins_autres']):\n",
      "        # We keep things pretty simple here: for every need type, if there is at least corresponding facility in the same geo or nearby geo we score 1\n",
      "        df['besoins_match'] = 0\n",
      "        for row in df.itertuples():\n",
      "            match_counter = 0\n",
      "            # codgeos = row.codgeo_voisins.tolist()+[row.Index] if row.codgeo_voisins is not None else [row.Index]\n",
      "            codgeos = [row.Index] # For now we simplify and only look at the specific geos and not neighboring communes\n",
      "            for codgeo in codgeos:\n",
      "                if codgeo in incl_index.index:\n",
      "                    for key, values in prefs['besoins_autres'].items():\n",
      "                        for v in values:\n",
      "                            # if v is None:\n",
      "                            #     if key in incl_index.loc[codgeo].item():\n",
      "                            #         match_counter += 1\n",
      "                            # else: \n",
      "                            if key+'_'+v in incl_index.loc[codgeo].item():\n",
      "                                match_counter += 1\n",
      "            df.loc[row.Index, 'besoins_match'] = match_counter\n",
      "        df['besoins_match_scaled'] = t.fit_transform(df[['besoins_match']].fillna(0))\n",
      "    else:\n",
      "        #svc_ratio est le ratio de services d'inclusion de la commune (pour 1000 habitants)\n",
      "        df['svc_incl_ratio'] = 1000 * df['svc_incl_count']/df['pop_be']\n",
      "        df['svc_incl_scaled'] = t.fit_transform(df[['svc_incl_ratio']].fillna(0))\n",
      "\n",
      "    #pol est le score selon la couleur politique (extreme droite = 0, gauche = 1)\n",
      "    df['pol_scaled'] = df[['pol_num']].astype('float')\n",
      "        \n",
      "    return df\n",
      "def compute_cat_scores(df, scores_cat, penalty):\n",
      "    # Efficiently compute score in a scalar way (select all relevant rows at once)\n",
      "    df = df.copy()\n",
      "\n",
      "    # For each score category (emploi, education etc.) we compute the score weighted with penalty (e.g. 10%) for neighbors \n",
      "    for cat in set(scores_cat.cat):\n",
      "        # We select the relevant columns for this specific category\n",
      "        cat_scores_indices = [col for col in df.columns if col in scores_cat[scores_cat.cat == cat]['score'].to_list()] \n",
      "\n",
      "        if not cat_scores_indices:\n",
      "            # if there is no computed score relevant for this category we skip it\n",
      "            continue\n",
      "        \n",
      "        cat_scores_indices_binome = [col+'_binome' for col in cat_scores_indices if col+'_binome' in df.columns]\n",
      "        \n",
      "        # First we build the criteria score dataframe with penalty for binome commune\n",
      "        cat_scores_df = pd.concat([df[cat_scores_indices],(1-penalty)*df[cat_scores_indices_binome]], axis=1) \n",
      "\n",
      "        # Second we compare neighbor score to assessed commune and keep the max\n",
      "        for col in cat_scores_indices:\n",
      "            col_binome = col+'_binome'\n",
      "            if col_binome in cat_scores_df.columns:\n",
      "                cat_scores_df[col+'_max'] = cat_scores_df[[col, col_binome]].max(axis=1)\n",
      "            else:\n",
      "                cat_scores_df[col+'_max'] = cat_scores_df[col] # this is the case where the criteria is not relevant for binome comparison\n",
      "\n",
      "        # Third and finally we compute the mean to get the final cat score (NaN values are ignored which is what we want)\n",
      "        df[cat + '_cat_score'] = cat_scores_df.filter(like='_max', axis=1).mean(axis=1)\n",
      "    \n",
      "    return df\n",
      "def compute_binome_score_old(df, binome_penalty, prefs):\n",
      "    scores_col = [col for col in df.columns if col.endswith('_cat_score')]\n",
      "    max_scores = pd.DataFrame()\n",
      "    \n",
      "    for col in scores_col:\n",
      "        cat_weight = prefs[col.split('_')[0]]\n",
      "        max_scores[col] = cat_weight * np.where(\n",
      "            df[col] >= (1-binome_penalty)*df[col+'_binome'],\n",
      "            df[col],\n",
      "            (1-binome_penalty)*df[col+'_binome']\n",
      "            )\n",
      "    \n",
      "    return max_scores.mean(axis=1).round(1)\n",
      "def compute_binome_score(df, scores_cat, prefs):\n",
      "    scores_cat_col = [col for col in df.columns if col.endswith('_cat_score')]\n",
      "    weighted_scores = 0\n",
      "    total_weight = 0\n",
      "    for cat_score in scores_cat_col:\n",
      "        if df[cat_score] is not None:\n",
      "            cat_weight =  prefs['poids_'+cat_score.split('_')[0]]\n",
      "            weighted_scores += cat_weight * df[cat_score]\n",
      "            total_weight += cat_weight\n",
      "    \n",
      "    return weighted_scores / total_weight\n",
      "def best_score_compute(df):\n",
      "    #Keeping the best (top #1) monome or binome result for each commune\n",
      "    best = df.sort_values('weighted_score', ascending=False).groupby('codgeo').head(1)\n",
      "    return best\n",
      "#Main function that aggregates most of the above in one sequence\n",
      "def compute_odis_score(df, scores_cat, prefs, incl_index):\n",
      "    \n",
      "    # We restrict to communes with pop larger than 1000 (30% of all communes)\n",
      "    df = df[df.population > prefs['pop_min']]\n",
      "\n",
      "    # We add the disctance from the current location defined in the app\n",
      "    df = add_distance_to_current_loc(df, current_codgeo=prefs['commune_actuelle'])\n",
      "\n",
      "    # We filter by distance to reduce the compute cost on a smaller odis_search dataframe\n",
      "    odis_search = filter_loc_by_distance(df, distance=prefs['loc_distance_km'])\n",
      "\n",
      "    # We compute the subject specific scores\n",
      "    odis_scored = compute_criteria_scores(odis_search, prefs=prefs, incl_index=incl_index)\n",
      "\n",
      "    # We add the criteria scores for all neighbor communes forming monomes and binomes\n",
      "    odis_exploded = adding_score_voisins(odis_scored, scores_cat)\n",
      "\n",
      "    # We compute the category scores for both the target and the binome\n",
      "    odis_exploded = compute_cat_scores(odis_exploded, scores_cat=scores_cat, penalty=prefs['binome_penalty'])\n",
      "\n",
      "    # We computing the final weighted score for all commune<->voisin combinations\n",
      "    odis_exploded['weighted_score'] = compute_binome_score(odis_exploded, scores_cat=scores_cat, prefs=prefs)\n",
      "\n",
      "    # We keep best monome or binome for each commune \n",
      "    odis_search_best = best_score_compute(odis_exploded)\n",
      "\n",
      "    return odis_search_best\n",
      "# THIS SHOULD BE THE END OF JUPYTER NOTEBOOK EXPORT\n"
     ]
    }
   ],
   "source": [
    "%save -f -r ../streamlit/odis_stream2_scoring.py 1-13\n",
    "# This saves the cells 0 to 22 (and their execution history unfortunately) to a python file that I can use in Streamlit\n",
    "# Make sure to restart before running this cell\n",
    "# Don't forget to restart streamlit after this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae6d547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart and run all the cells above this one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9bf13d",
   "metadata": {},
   "source": [
    "## Notebook explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65293654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "\n",
    "ODIS_FILE = '../csv/odis_june_2025_jacques.parquet'\n",
    "SCORES_CAT_FILE = '../csv/odis_scores_cat.csv'\n",
    "METIERS_FILE = '../csv/dares_nomenclature_fap2021.csv'\n",
    "FORMATIONS_FILE = '../csv/index_formations.csv'\n",
    "ECOLES_FILE = '../csv/annuaire_ecoles_france_mini.parquet'\n",
    "MATERNITE_FILE = '../csv/annuaire_maternites_DREES.csv'\n",
    "SANTE_FILE = '../csv/annuaire_sante_finess.parquet'\n",
    "INCLUSION_FILE = '../csv/odis_services_incl_exploded.parquet'\n",
    "\n",
    "odis, scores_cat, codfap_index, codformations_index, annuaire_ecoles, annuaire_sante, annuaire_inclusion, incl_index = init_loading_datasets(\n",
    "    odis_file=ODIS_FILE,\n",
    "    scores_cat_file=SCORES_CAT_FILE,\n",
    "    metiers_file=METIERS_FILE,\n",
    "    formations_file=FORMATIONS_FILE,\n",
    "    ecoles_file=ECOLES_FILE,\n",
    "    maternites_file=MATERNITE_FILE,\n",
    "    sante_file=SANTE_FILE,\n",
    "    inclusion_file=INCLUSION_FILE\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28482f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subject preferences weighted score computation\n",
    "prefs = {\n",
    "    'poids_emploi':2,\n",
    "    'poids_logement':1,\n",
    "    'poids_education':1,\n",
    "    'poids_soutien':1,\n",
    "    'poids_mobilité':0,\n",
    "    'commune_actuelle':'33063', # 75056=Paris, 33063=Bordeaux, 18033=Bourges\n",
    "    'hebergement':\"Chez l'habitant\",\n",
    "    'logement':'Logement Social',\n",
    "    'loc_distance_km':25,\n",
    "    'nb_adultes':2,\n",
    "    'nb_enfants':0, #1,\n",
    "    'codes_metiers':[['S1X40','J0X33','A1X41'], ['T4X60','T2A60']],\n",
    "    'codes_formations':[[423], [315,100]],\n",
    "    'classe_enfants': [],#['Maternelle', 'Collège'],\n",
    "    'besoin_sante': None,\n",
    "    'besoins_autres': {\n",
    "        'apprendre-francais':['-'],\n",
    "        'famille':['garde-denfants']\n",
    "    },\n",
    "    'binome_penalty':0.1,\n",
    "    'pop_min': 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b63584a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.31|Add Distance End\n",
      "0.03|Filter Loc by Distance\n",
      "0.15|Compute Subject Score End\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jacques/.cache/pypoetry/virtualenvs/odis-Nf-mTAVv-py3.10/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:2829: UserWarning: n_quantiles (1000) is greater than the total number of samples (155). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "/home/jacques/.cache/pypoetry/virtualenvs/odis-Nf-mTAVv-py3.10/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:2829: UserWarning: n_quantiles (1000) is greater than the total number of samples (155). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "/home/jacques/.cache/pypoetry/virtualenvs/odis-Nf-mTAVv-py3.10/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:2829: UserWarning: n_quantiles (1000) is greater than the total number of samples (155). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "/home/jacques/.cache/pypoetry/virtualenvs/odis-Nf-mTAVv-py3.10/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:2829: UserWarning: n_quantiles (1000) is greater than the total number of samples (155). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "/home/jacques/.cache/pypoetry/virtualenvs/odis-Nf-mTAVv-py3.10/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:2829: UserWarning: n_quantiles (1000) is greater than the total number of samples (155). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "/home/jacques/.cache/pypoetry/virtualenvs/odis-Nf-mTAVv-py3.10/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:2829: UserWarning: n_quantiles (1000) is greater than the total number of samples (155). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "/home/jacques/.cache/pypoetry/virtualenvs/odis-Nf-mTAVv-py3.10/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:2829: UserWarning: n_quantiles (1000) is greater than the total number of samples (155). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n",
      "/home/jacques/.cache/pypoetry/virtualenvs/odis-Nf-mTAVv-py3.10/lib/python3.10/site-packages/sklearn/preprocessing/_data.py:2829: UserWarning: n_quantiles (1000) is greater than the total number of samples (155). n_quantiles is set to n_samples.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08|Adding Score Voisin\n",
      "0.41|Compute Cat Score End\n",
      "0.01|Compute Weighted Score End\n",
      "0.01|Compute Best Score End\n"
     ]
    }
   ],
   "source": [
    "# Step by Step Execution\n",
    "from time import time\n",
    "\n",
    "def performance_tracker(t, text, timer_mode):\n",
    "    if timer_mode:\n",
    "        print(str(round(time()-t,2))+'|'+text)\n",
    "        return time()\n",
    "t = time()\n",
    "timer_mode = True\n",
    "\n",
    "df = odis\n",
    "score_cat = scores_cat\n",
    "prefs = prefs\n",
    "#\n",
    "df = add_distance_to_current_loc(df, current_codgeo=prefs['commune_actuelle'])\n",
    "t = performance_tracker(t, 'Add Distance End', timer_mode)\n",
    "\n",
    "# We filter by distance to reduce the compute cost on a smaller odis_search dataframe\n",
    "odis_search = filter_loc_by_distance(df, distance=prefs['loc_distance_km'])\n",
    "t = performance_tracker(t, 'Filter Loc by Distance', timer_mode)\n",
    "\n",
    "# We compute the subject specific scores\n",
    "odis_scored = compute_criteria_scores(odis_search, prefs=prefs, incl_index=incl_index)\n",
    "t = performance_tracker(t, 'Compute Subject Score End', timer_mode)\n",
    "\n",
    "# We add the criteria scores for all neighbor communes forming monomes and binomes\n",
    "odis_exploded = adding_score_voisins(odis_scored, scores_cat)\n",
    "t = performance_tracker(t, 'Adding Score Voisin', timer_mode)\n",
    "\n",
    "# We compute the category scores for both the target and the binome\n",
    "odis_exploded = compute_cat_scores(odis_exploded, scores_cat=scores_cat, penalty=prefs['binome_penalty'])\n",
    "t = performance_tracker(t, 'Compute Cat Score End', timer_mode)\n",
    "\n",
    "# We computing the final weighted score for all commune<->voisin combinations\n",
    "odis_exploded['weighted_score'] = compute_binome_score(odis_exploded, scores_cat=scores_cat, prefs=prefs)\n",
    "t = performance_tracker(t, 'Compute Weighted Score End', timer_mode)\n",
    "\n",
    "# We keep best monome or binome for each commune \n",
    "odis_search_best = best_score_compute(odis_exploded)\n",
    "t = performance_tracker(t, 'Compute Best Score End', timer_mode)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8843e3",
   "metadata": {},
   "source": [
    "## The End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "odis-Nf-mTAVv-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
