{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3acd4d8f",
   "metadata": {},
   "source": [
    "## Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6c35fc",
   "metadata": {},
   "source": [
    "### Scoring Logic Description\n",
    "\n",
    "1. We start by importing the odis dataframe from a CSV that includes all the relevant datapoint to score and display data\n",
    "2. We compute the scores for each criteria specific to the commune (independant from subject)\n",
    "3. We compute the scores for each criteria specific to the subject (dependand from both subject and commune) \n",
    "4. We identify all commune<->neighbour pairs (binômes) for each commune within search radius\n",
    "5. We compute category scores (emploi, logement, education etc...) as an average of the all the scores for a given category\n",
    "6. For each commune we compare the commune and neighbour category scores and weighted the highest one with category weights defined by subject and then keep the best weighted score for each commune\n",
    "8. We display result in on a map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16add7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS SHOULD BE THE END OF JUPYTER NOTEBOOK EXPORT\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from scipy import stats\n",
    "import folium as flm #required for gdf.explore()\n",
    "import shapely as shp\n",
    "from shapely.wkt import loads\n",
    "from shapely.geometry import Polygon\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7744f22f",
   "metadata": {},
   "source": [
    "### 1. Fetching key indicators from ODIS source file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d325cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_loading_datasets(odis_file, scores_cat_file, metiers_file, formations_file, ecoles_file, maternites_file, sante_file):\n",
    "    odis = gpd.GeoDataFrame(gpd.read_parquet(odis_file))\n",
    "    odis.set_geometry(odis.polygon, inplace=True)\n",
    "    odis = odis[~odis.polygon.isna()]\n",
    "    odis.set_index('codgeo', inplace=True)\n",
    "\n",
    "    # Index of all scores and their explanations\n",
    "    scores_cat = pd.read_csv(scores_cat_file)\n",
    "\n",
    "    #Later we need the code FAP <-> FAP Name used to classify jobs\n",
    "    codfap_index = pd.read_csv(metiers_file, delimiter=';')\n",
    "\n",
    "    # Later we need the code formation <-> Formation Name used to classify trainings\n",
    "    # source: https://www.data.gouv.fr/fr/datasets/liste-publique-des-organismes-de-formation-l-6351-7-1-du-code-du-travail/\n",
    "    codformations_index = pd.read_csv(formations_file).set_index('codformation')\n",
    "\n",
    "    # Etablissements scolaires\n",
    "    annuaire_ecoles = pd.read_parquet(ecoles_file)\n",
    "    annuaire_ecoles.geometry = annuaire_ecoles.geometry.apply(shp.from_wkb)\n",
    "\n",
    "    #Annuaire Maternités\n",
    "    # Source: https://www.data.gouv.fr/fr/datasets/liste-des-maternites-de-france-depuis-2000/\n",
    "    annuaire_maternites = pd.read_csv(maternites_file, delimiter=';')\n",
    "    annuaire_maternites.drop_duplicates(subset=['FI_ET'], keep='last', inplace=True)\n",
    "    annuaire_maternites.head()\n",
    "\n",
    "    # Annuaire etablissements santé\n",
    "    # Source: https://www.data.gouv.fr/fr/datasets/reexposition-des-donnees-finess/\n",
    "    annuaire_sante = pd.read_parquet(sante_file)\n",
    "    annuaire_sante = annuaire_sante[annuaire_sante.LibelleSph == 'Etablissement public de santé']\n",
    "    annuaire_sante['geometry'] = gpd.points_from_xy(annuaire_sante.coordxet, annuaire_sante.coordyet, crs='epsg:2154')\n",
    "    annuaire_sante = pd.merge(annuaire_sante, annuaire_maternites[['FI_ET']], left_on='nofinesset', right_on='FI_ET', how='left', indicator=\"maternite\")\n",
    "    annuaire_sante.drop(columns=['FI_ET'], inplace=True)\n",
    "    annuaire_sante.maternite = np.where(annuaire_sante.maternite == 'both', True, False)\n",
    "    annuaire_sante['codgeo'] = annuaire_sante.Departement + annuaire_sante.Commune\n",
    "\n",
    "    return odis, scores_cat, codfap_index, codformations_index, annuaire_ecoles, annuaire_sante"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a250e2d3",
   "metadata": {},
   "source": [
    "### 2. Distance filter + Gathering nearby Communes Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd47e8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering dataframe based on subject distance preference (to save on compute time later on)\n",
    "def filter_loc_by_distance(df, distance):\n",
    "    return df[df.dist_current_loc < distance * 1000]\n",
    "\n",
    "# Put None as a score in the monome case\n",
    "def monome_cleanup(df):\n",
    "    mask = ~df['binome']\n",
    "    for col in df.columns:\n",
    "        if col.endswith('_binome'):\n",
    "            df.loc[mask, col] = None\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9898c8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adding_score_voisins(df_search, scores_cat):\n",
    "    #df_search is the dataframe pre-filtered by location\n",
    "    #df_source is the dataframe with all the communes\n",
    "    t = time()\n",
    "    timer_mode=True\n",
    "    binome_columns = ['codgeo','libgeo','polygon','epci_code','epci_nom'] + scores_cat[scores_cat.incl_binome]['score'].to_list()+scores_cat[scores_cat.incl_binome]['metric'].to_list()\n",
    "    binome_columns = list(set(binome_columns) & set(df_search.columns))\n",
    "    df_binomes = df_search[binome_columns].copy()\n",
    "    \n",
    "    t = performance_tracker(t, 'adding_score_voisin Prep Done', timer_mode)\n",
    "\n",
    "    # Adds itself to list of voisins = monome case\n",
    "    # Note: this code triggers the SettingWithCopyWarning but I don't know how to fix it...\n",
    "    df_search.codgeo_voisins = df_search.apply(lambda x: np.append(x.codgeo_voisins, x.name), axis=1)\n",
    "    t = performance_tracker(t, 'adding_score_voisin Additself Done', timer_mode)\n",
    "    # Explodes the dataframe to have a row for each voisins + itself\n",
    "    df_search['codgeo_voisins_copy'] = df_search['codgeo_voisins']\n",
    "    df_search_exploded = df_search.explode('codgeo_voisins_copy')\n",
    "    df_search_exploded.rename(columns={'codgeo_voisins_copy':'codgeo_binome'}, inplace=True)\n",
    "    t = performance_tracker(t, 'adding_score_voisin Explode Done', timer_mode)\n",
    "    \n",
    "    # For each commune (codgeo) in search area (df_search) we add all its voisin's scores\n",
    "    odis_search_exploded = pd.merge(\n",
    "        df_search_exploded, \n",
    "        df_binomes.add_suffix('_binome'), \n",
    "        left_on='codgeo_binome', \n",
    "        right_index=True, \n",
    "        how='inner', \n",
    "        validate=\"many_to_one\")\n",
    "    t = performance_tracker(t, 'adding_score_voisin Merge Done', timer_mode)\n",
    "    \n",
    "    # Adds a column to identify binomes vs monomes + cleanup\n",
    "    odis_search_exploded['binome'] = np.where(odis_search_exploded.index == odis_search_exploded.codgeo_binome, True, False)\n",
    "    t = performance_tracker(t, 'adding_score_voisin Binome Replace Done', timer_mode)\n",
    " \n",
    " \n",
    "\n",
    "    #We remove all values for the monome case to avoid accounting for them in the category score calculation\n",
    "    odis_search_exploded = monome_cleanup(odis_search_exploded)\n",
    "    t = performance_tracker(t, 'adding_score_voisin Monome Cleanup Done', timer_mode)\n",
    "    return odis_search_exploded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58361529",
   "metadata": {},
   "source": [
    "### 3. Criterias Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ffe9535",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing distance from current commune \n",
    "#Using a crs that allows to compute distance in meters for metropolitan France\n",
    "\n",
    "def distance_calc(df, ref_point):\n",
    "    return int(df.distance(ref_point))\n",
    "\n",
    "def add_distance_to_current_loc(df, current_codgeo):\n",
    "    projected_crs = \"EPSG:2154\"\n",
    "    # We first need to change CRS to a projected CRS\n",
    "    df_projected = gpd.GeoDataFrame(df)\n",
    "    df_projected = df_projected.to_crs(projected_crs)\n",
    "\n",
    "    zone_recherche = df_projected[df_projected.index == current_codgeo].copy()\n",
    "    zone_recherche['centroid'] = zone_recherche.centroid\n",
    "    zone_recherche = gpd.GeoDataFrame(zone_recherche, geometry='centroid')\n",
    "    zone_recherche.to_crs(projected_crs, inplace=True)\n",
    "    \n",
    "    df_projected = df_projected.sjoin_nearest(zone_recherche, distance_col=\"dist_current_loc\")[['dist_current_loc']]\n",
    "    df = pd.merge(df, df_projected, left_index=True, right_index=True, how='left')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d2a316b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding score specific to subject looking for a job identified as en besoin\n",
    "def codes_match(df, codes_list):\n",
    "    #returns a list of codfaps that matches\n",
    "    if df is None:\n",
    "        return []\n",
    "    return list(set(df.tolist()).intersection(set(codes_list)))\n",
    "\n",
    "def fap_names_lookup(df):\n",
    "    return list(codfap_index[codfap_index['Code FAP 341'].isin(df)]['Intitulé FAP 341'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c8d1bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_criteria_scores(df, prefs): \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Using QuantileTransfer to normalize all scores between 0 and 1 for the region\n",
    "    t = preprocessing.QuantileTransformer(output_distribution=\"uniform\")\n",
    "\n",
    "    #met_ration est le ratio d'offres non-pourvues pour 1000 habitants\n",
    "    df['met_ratio']= 1000 * df.met/df.pop_be\n",
    "    df['met_scaled'] = t.fit_transform(df[['met_ratio']].fillna(0))\n",
    "    #met_tension_ratio est le ratio d'offres population de la zone (pour 1000 habitants)\n",
    "    df['met_tension_ratio'] = 1000 * df.met_tension/df.pop_be\n",
    "    df['met_tension_scaled'] = t.fit_transform(df[['met_tension_ratio']].fillna(0))\n",
    "    #svc_ratio est le ratio de services d'inclusion de la commune (pour 1000 habitants)\n",
    "    df['svc_incl_ratio'] = 1000 * df.svc_incl_count/df.pop_be\n",
    "    df['svc_incl_scaled'] = t.fit_transform(df[['svc_incl_ratio']].fillna(0))\n",
    "    #log_vac_ratio est le ratio de logements vacants de la commune % total logements\n",
    "    df['log_vac_ratio'] = df.log_vac/df.log_total\n",
    "    df['log_vac_scaled'] = t.fit_transform(df[['log_vac_ratio']].fillna(0))\n",
    "    #pol est le score selon la couleur politique (extreme droite = 0, gauche = 1)\n",
    "    df['pol_scaled'] = df[['pol_num']].astype('float')\n",
    "    \n",
    "    if prefs['hebergement'] == \"Chez l'habitant\":\n",
    "        #log_5p+_ratio est le ratio de residences principales de 5 pièces ou plus % total residences principales\n",
    "        df['log_5p_ratio'] = df['rp_5+pieces']/df.log_rp\n",
    "        df['log_5p_scaled'] = t.fit_transform(df[['log_5p_ratio']].fillna(0))\n",
    "\n",
    "    if len(prefs['classe_enfants']) > 0: \n",
    "        # Risque de fermeture école: ratio de classe à risque de fermeture % nombre d'écoles\n",
    "        df['risque_fermeture_ratio'] = df.risque_fermeture/df.ecoles_ct\n",
    "        df['classes_ferm_scaled'] = t.fit_transform(df[['risque_fermeture_ratio']].fillna(0))\n",
    "\n",
    "\n",
    "    # Subject Specific criterias\n",
    "\n",
    "    # We compute the distance from the current location \n",
    "    df['reloc_dist_scaled'] = (1-df['dist_current_loc']/(prefs['loc_distance_km']*1000))\n",
    "    df['reloc_epci_scaled'] = np.where(df['epci_code'] == df.loc[prefs['commune_actuelle']]['epci_code'],1,0)\n",
    "\n",
    "    #For each adult we look for jobs categories that match what is needed\n",
    "    i=1\n",
    "    for adult in range(0,prefs['nb_adultes']):\n",
    "        if len(prefs['codes_metiers'][adult]) > 0:\n",
    "            df['met_match_codes_adult'+str(i)] = df.be_codfap_top.apply(codes_match, codes_list=prefs['codes_metiers'][adult])\n",
    "            df['met_match_adult'+str(i)] = df['met_match_codes_adult'+str(i)].apply(len)\n",
    "            df['met_match_adult'+str(i)+'_scaled'] = t.fit_transform(df[['met_match_adult'+str(i)]].fillna(0))\n",
    "            i+=1\n",
    "\n",
    "    j=1\n",
    "    for adult in range(0,prefs['nb_adultes']):\n",
    "        if len(prefs['codes_formations'][adult]) > 0:\n",
    "            df['form_match_codes_adult'+str(j)] = df.codes_formations.apply(codes_match, codes_list=prefs['codes_formations'][adult])\n",
    "            df['form_match_adult'+str(j)] = df['form_match_codes_adult'+str(j)].apply(len)\n",
    "            df['form_match_adult'+str(j)+'_scaled'] = t.fit_transform(df[['form_match_adult'+str(j)]].fillna(0))\n",
    "        j+=1\n",
    "\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae24ab0",
   "metadata": {},
   "source": [
    "### 4. Category Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25e7e3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cat_scores(df, scores_cat, penalty):\n",
    "    df = df.copy()\n",
    "    df_binome = pd.DataFrame()\n",
    "    columns_in_use = set(df.columns) & set(scores_cat.score)\n",
    "    columns_in_use_binome = set(df.columns) & set([score+'_binome' for score in scores_cat.score])\n",
    "    for cat in set(scores_cat.cat):\n",
    "        cat_scores_indices = [score for score in scores_cat[scores_cat['cat'] == cat]['score'] if score in columns_in_use]\n",
    "        cat_scores_indices_binome = [score+'_binome' for score in scores_cat[scores_cat['cat'] == cat]['score'] if score+'_binome' in columns_in_use_binome]\n",
    "\n",
    "        # Efficiently select all relevant rows at once\n",
    "        cat_scores_df = df[cat_scores_indices]\n",
    "        for col in cat_scores_indices_binome:\n",
    "            mask = df[col].notna()\n",
    "            df_binome[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            df_binome.loc[mask, col] = df.loc[mask, col] * (1-penalty) \n",
    "            cat_scores_df = pd.concat([cat_scores_df, df_binome[col]], axis=1)\n",
    "        df[cat + '_cat_score'] = cat_scores_df.astype(float).mean(axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fcebd0",
   "metadata": {},
   "source": [
    "### 5. Final Binome Score Weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "831f4061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_binome_score_old(df, binome_penalty, prefs):\n",
    "    scores_col = [col for col in df.columns if col.endswith('_cat_score')]\n",
    "    max_scores = pd.DataFrame()\n",
    "    \n",
    "    for col in scores_col:\n",
    "        cat_weight = prefs[col.split('_')[0]]\n",
    "        max_scores[col] = cat_weight * np.where(\n",
    "            df[col] >= (1-binome_penalty)*df[col+'_binome'],\n",
    "            df[col],\n",
    "            (1-binome_penalty)*df[col+'_binome']\n",
    "            )\n",
    "    \n",
    "    return max_scores.mean(axis=1).round(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c77eb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_binome_score(df, scores_cat, prefs):\n",
    "    scores_cat_col = [col for col in df.columns if col.endswith('_cat_score')]\n",
    "    weighted_scores = pd.DataFrame()\n",
    "    for col in scores_cat_col:\n",
    "        cat_weight =  prefs['poids_'+col.split('_')[0]]\n",
    "        weighted_scores[col] = cat_weight * df[col]\n",
    "    \n",
    "    return weighted_scores.astype(float).mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3042ad31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_score_compute(df):\n",
    "    #Keeping the best (top #1) monome or binome result for each commune\n",
    "    best = df.sort_values('weighted_score', ascending=False).groupby('codgeo').head(1)\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56483e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main function that aggregates most of the above in one sequence\n",
    "def compute_odis_score(df, scores_cat, prefs):\n",
    "    df = add_distance_to_current_loc(df, current_codgeo=prefs['commune_actuelle'])\n",
    "\n",
    "    # We filter by distance to reduce the compute cost on a smaller odis_search dataframe\n",
    "    odis_search = filter_loc_by_distance(df, distance=prefs['loc_distance_km'])\n",
    "\n",
    "    # We compute the subject specific scores\n",
    "    odis_scored = compute_criteria_scores(odis_search, prefs=prefs)\n",
    "\n",
    "    # We add the criteria scores for all neighbor communes forming monomes and binomes\n",
    "    odis_exploded = adding_score_voisins(odis_scored, scores_cat)\n",
    "\n",
    "    # We compute the category scores for both the target and the binome\n",
    "    odis_exploded = compute_cat_scores(odis_exploded, scores_cat=scores_cat, penalty=prefs['binome_penalty'])\n",
    "\n",
    "    # We computing the final weighted score for all commune<->voisin combinations\n",
    "    odis_exploded['weighted_score'] = compute_binome_score(odis_exploded, scores_cat=scores_cat, prefs=prefs)\n",
    "\n",
    "    # We keep best monome or binome for each commune \n",
    "    odis_search_best = best_score_compute(odis_exploded)\n",
    "\n",
    "    return odis_search_best\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7504e591",
   "metadata": {},
   "source": [
    "### 6. Generating Narrative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1a10f4",
   "metadata": {},
   "source": [
    "Here we want to generate a 'human readable' explanation about why scored high a given location.\n",
    "Things to show:\n",
    "- Target commune name and EPCI\n",
    "- Weighted Score\n",
    "- If Binome, show the binome and EPCI if different from target\n",
    "- Show top 3 criterias target (weighted ?) \n",
    "- Show top 3 criterias for binome (weighted ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4de2b412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS SHOULD BE THE END OF JUPYTER NOTEBOOK EXPORT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663e91e0",
   "metadata": {},
   "source": [
    "## Export to Python file for streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95990aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following commands were written to file `../streamlit/odis_stream2_scoring.py`:\n",
      "# THIS SHOULD BE THE END OF JUPYTER NOTEBOOK EXPORT\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import geopandas as gpd\n",
      "from scipy import stats\n",
      "import folium as flm #required for gdf.explore()\n",
      "import shapely as shp\n",
      "from shapely.wkt import loads\n",
      "from shapely.geometry import Polygon\n",
      "from sklearn import preprocessing\n",
      "def init_loading_datasets(odis_file, scores_cat_file, metiers_file, formations_file, ecoles_file, maternites_file, sante_file):\n",
      "    odis = gpd.GeoDataFrame(gpd.read_parquet(odis_file))\n",
      "    odis.set_geometry(odis.polygon, inplace=True)\n",
      "    odis = odis[~odis.polygon.isna()]\n",
      "    odis.set_index('codgeo', inplace=True)\n",
      "\n",
      "    # Index of all scores and their explanations\n",
      "    scores_cat = pd.read_csv(scores_cat_file)\n",
      "\n",
      "    #Later we need the code FAP <-> FAP Name used to classify jobs\n",
      "    codfap_index = pd.read_csv(metiers_file, delimiter=';')\n",
      "\n",
      "    # Later we need the code formation <-> Formation Name used to classify trainings\n",
      "    # source: https://www.data.gouv.fr/fr/datasets/liste-publique-des-organismes-de-formation-l-6351-7-1-du-code-du-travail/\n",
      "    codformations_index = pd.read_csv(formations_file).set_index('codformation')\n",
      "\n",
      "    # Etablissements scolaires\n",
      "    annuaire_ecoles = pd.read_parquet(ecoles_file)\n",
      "    annuaire_ecoles.geometry = annuaire_ecoles.geometry.apply(shp.from_wkb)\n",
      "\n",
      "    #Annuaire Maternités\n",
      "    # Source: https://www.data.gouv.fr/fr/datasets/liste-des-maternites-de-france-depuis-2000/\n",
      "    annuaire_maternites = pd.read_csv(maternites_file, delimiter=';')\n",
      "    annuaire_maternites.drop_duplicates(subset=['FI_ET'], keep='last', inplace=True)\n",
      "    annuaire_maternites.head()\n",
      "\n",
      "    # Annuaire etablissements santé\n",
      "    # Source: https://www.data.gouv.fr/fr/datasets/reexposition-des-donnees-finess/\n",
      "    annuaire_sante = pd.read_parquet(sante_file)\n",
      "    annuaire_sante = annuaire_sante[annuaire_sante.LibelleSph == 'Etablissement public de santé']\n",
      "    annuaire_sante['geometry'] = gpd.points_from_xy(annuaire_sante.coordxet, annuaire_sante.coordyet, crs='epsg:2154')\n",
      "    annuaire_sante = pd.merge(annuaire_sante, annuaire_maternites[['FI_ET']], left_on='nofinesset', right_on='FI_ET', how='left', indicator=\"maternite\")\n",
      "    annuaire_sante.drop(columns=['FI_ET'], inplace=True)\n",
      "    annuaire_sante.maternite = np.where(annuaire_sante.maternite == 'both', True, False)\n",
      "    annuaire_sante['codgeo'] = annuaire_sante.Departement + annuaire_sante.Commune\n",
      "\n",
      "    return odis, scores_cat, codfap_index, codformations_index, annuaire_ecoles, annuaire_sante\n",
      "# Filtering dataframe based on subject distance preference (to save on compute time later on)\n",
      "def filter_loc_by_distance(df, distance):\n",
      "    return df[df.dist_current_loc < distance * 1000]\n",
      "\n",
      "# Put None as a score in the monome case\n",
      "def monome_cleanup(df):\n",
      "    mask = ~df['binome']\n",
      "    for col in df.columns:\n",
      "        if col.endswith('_binome'):\n",
      "            df.loc[mask, col] = None\n",
      "    return df\n",
      "def adding_score_voisins(df_search, scores_cat):\n",
      "    #df_search is the dataframe pre-filtered by location\n",
      "    #df_source is the dataframe with all the communes\n",
      "    binome_columns = ['codgeo','libgeo','polygon','epci_code','epci_nom'] + scores_cat[scores_cat.incl_binome]['score'].to_list()+scores_cat[scores_cat.incl_binome]['metric'].to_list()\n",
      "    binome_columns = list(set(binome_columns) & set(df_search.columns))\n",
      "    df_binomes = df_search[binome_columns].copy()\n",
      "\n",
      "    # Adds itself to list of voisins = monome case\n",
      "    # Note: this code triggers the SettingWithCopyWarning but I don't know how to fix it...\n",
      "    df_search.codgeo_voisins = df_search.apply(lambda x: np.append(x.codgeo_voisins, x.index), axis=1)\n",
      "\n",
      "    # Explodes the dataframe to have a row for each voisins + itself\n",
      "    df_search['codgeo_voisins_copy'] = df_search['codgeo_voisins']\n",
      "    df_search_exploded = df_search.explode('codgeo_voisins_copy')\n",
      "    \n",
      "    # For each commune (codgeo) in search area (df_search) we add all its voisin's scores\n",
      "    odis_search_exploded = pd.merge(df_search_exploded, df_binomes.add_suffix('_binome'), left_on='codgeo_voisins_copy', right_index=True, how='left', indicator=\"binome\")\n",
      "    odis_search_exploded.binome = np.where(odis_search_exploded.binome == 'both', True, False)\n",
      "    \n",
      "    # Adds a column to identify binomes vs monomes + cleanup\n",
      "    # odis_search_exploded['binome'] = odis_search_exploded.apply(lambda x: False if x.index == x.codgeo_binome else True, axis=1)\n",
      "    odis_search_exploded.drop(columns={'codgeo_voisins_copy'}, inplace=True)\n",
      "\n",
      "    #We remove all values for the monome case to avoid accounting for them in the category score calculation\n",
      "    odis_search_exploded = monome_cleanup(odis_search_exploded)\n",
      "\n",
      "    return odis_search_exploded\n",
      "#Computing distance from current commune \n",
      "#Using a crs that allows to compute distance in meters for metropolitan France\n",
      "\n",
      "def distance_calc(df, ref_point):\n",
      "    return int(df.distance(ref_point))\n",
      "\n",
      "def add_distance_to_current_loc(df, current_codgeo):\n",
      "    projected_crs = \"EPSG:2154\"\n",
      "    # We first need to change CRS to a projected CRS\n",
      "    df_projected = gpd.GeoDataFrame(df)\n",
      "    df_projected = df_projected.to_crs(projected_crs)\n",
      "\n",
      "    zone_recherche = df_projected[df_projected.index == current_codgeo].copy()\n",
      "    zone_recherche['centroid'] = zone_recherche.centroid\n",
      "    zone_recherche = gpd.GeoDataFrame(zone_recherche, geometry='centroid')\n",
      "    zone_recherche.to_crs(projected_crs, inplace=True)\n",
      "    \n",
      "    df_projected = df_projected.sjoin_nearest(zone_recherche, distance_col=\"dist_current_loc\")[['dist_current_loc']]\n",
      "    df = pd.merge(df, df_projected, left_index=True, right_index=True, how='left')\n",
      "    \n",
      "    return df\n",
      "#Adding score specific to subject looking for a job identified as en besoin\n",
      "def codes_match(df, codes_list):\n",
      "    #returns a list of codfaps that matches\n",
      "    if df is None:\n",
      "        return []\n",
      "    return list(set(df.tolist()).intersection(set(codes_list)))\n",
      "\n",
      "def fap_names_lookup(df):\n",
      "    return list(codfap_index[codfap_index['Code FAP 341'].isin(df)]['Intitulé FAP 341'])\n",
      "def compute_criteria_scores(df, prefs): \n",
      "    df = df.copy()\n",
      "    \n",
      "    # Using QuantileTransfer to normalize all scores between 0 and 1 for the region\n",
      "    t = preprocessing.QuantileTransformer(output_distribution=\"uniform\")\n",
      "\n",
      "    #met_ration est le ratio d'offres non-pourvues pour 1000 habitants\n",
      "    df['met_ratio']= 1000 * df.met/df.pop_be\n",
      "    df['met_scaled'] = t.fit_transform(df[['met_ratio']].fillna(0))\n",
      "    #met_tension_ratio est le ratio d'offres population de la zone (pour 1000 habitants)\n",
      "    df['met_tension_ratio'] = 1000 * df.met_tension/df.pop_be\n",
      "    df['met_tension_scaled'] = t.fit_transform(df[['met_tension_ratio']].fillna(0))\n",
      "    #svc_ratio est le ratio de services d'inclusion de la commune (pour 1000 habitants)\n",
      "    df['svc_incl_ratio'] = 1000 * df.svc_incl_count/df.pop_be\n",
      "    df['svc_incl_scaled'] = t.fit_transform(df[['svc_incl_ratio']].fillna(0))\n",
      "    #log_vac_ratio est le ratio de logements vacants de la commune % total logements\n",
      "    df['log_vac_ratio'] = df.log_vac/df.log_total\n",
      "    df['log_vac_scaled'] = t.fit_transform(df[['log_vac_ratio']].fillna(0))\n",
      "    #pol est le score selon la couleur politique (extreme droite = 0, gauche = 1)\n",
      "    df['pol_scaled'] = df[['pol_num']].astype('float')\n",
      "    \n",
      "    if prefs['hebergement'] == \"Chez l'habitant\":\n",
      "        #log_5p+_ratio est le ratio de residences principales de 5 pièces ou plus % total residences principales\n",
      "        df['log_5p_ratio'] = df['rp_5+pieces']/df.log_rp\n",
      "        df['log_5p_scaled'] = t.fit_transform(df[['log_5p_ratio']].fillna(0))\n",
      "\n",
      "    if len(prefs['classe_enfants']) > 0: \n",
      "        # Risque de fermeture école: ratio de classe à risque de fermeture % nombre d'écoles\n",
      "        df['risque_fermeture_ratio'] = df.risque_fermeture/df.ecoles_ct\n",
      "        df['classes_ferm_scaled'] = t.fit_transform(df[['risque_fermeture_ratio']].fillna(0))\n",
      "\n",
      "\n",
      "    # Subject Specific criterias\n",
      "\n",
      "    # We compute the distance from the current location \n",
      "    df['reloc_dist_scaled'] = (1-df['dist_current_loc']/(prefs['loc_distance_km']*1000))\n",
      "    df['reloc_epci_scaled'] = np.where(df['epci_code'] == df.loc[prefs['commune_actuelle']]['epci_code'],1,0)\n",
      "\n",
      "    #For each adult we look for jobs categories that match what is needed\n",
      "    i=1\n",
      "    for adult in range(0,prefs['nb_adultes']):\n",
      "        if len(prefs['codes_metiers'][adult]) > 0:\n",
      "            df['met_match_codes_adult'+str(i)] = df.be_codfap_top.apply(codes_match, codes_list=prefs['codes_metiers'][adult])\n",
      "            df['met_match_adult'+str(i)] = df['met_match_codes_adult'+str(i)].apply(len)\n",
      "            df['met_match_adult'+str(i)+'_scaled'] = t.fit_transform(df[['met_match_adult'+str(i)]].fillna(0))\n",
      "            i+=1\n",
      "\n",
      "    j=1\n",
      "    for adult in range(0,prefs['nb_adultes']):\n",
      "        if len(prefs['codes_formations'][adult]) > 0:\n",
      "            df['form_match_codes_adult'+str(j)] = df.codes_formations.apply(codes_match, codes_list=prefs['codes_formations'][adult])\n",
      "            df['form_match_adult'+str(j)] = df['form_match_codes_adult'+str(j)].apply(len)\n",
      "            df['form_match_adult'+str(j)+'_scaled'] = t.fit_transform(df[['form_match_adult'+str(j)]].fillna(0))\n",
      "        j+=1\n",
      "\n",
      "    \n",
      "    return df\n",
      "def compute_cat_scores(df, scores_cat, penalty):\n",
      "    df = df.copy()\n",
      "    df_binome = pd.DataFrame()\n",
      "    columns_in_use = set(df.columns) & set(scores_cat.score)\n",
      "    columns_in_use_binome = set(df.columns) & set([score+'_binome' for score in scores_cat.score])\n",
      "    for cat in set(scores_cat.cat):\n",
      "        cat_scores_indices = [score for score in scores_cat[scores_cat['cat'] == cat]['score'] if score in columns_in_use]\n",
      "        cat_scores_indices_binome = [score+'_binome' for score in scores_cat[scores_cat['cat'] == cat]['score'] if score+'_binome' in columns_in_use_binome]\n",
      "\n",
      "        # Efficiently select all relevant rows at once\n",
      "        cat_scores_df = df[cat_scores_indices]\n",
      "        for col in cat_scores_indices_binome:\n",
      "            mask = df[col].notna()\n",
      "            df_binome[col] = pd.to_numeric(df[col], errors='coerce')\n",
      "            df_binome.loc[mask, col] = df.loc[mask, col] * (1-penalty) \n",
      "            cat_scores_df = pd.concat([cat_scores_df, df_binome[col]], axis=1)\n",
      "        df[cat + '_cat_score'] = cat_scores_df.astype(float).mean(axis=1)\n",
      "\n",
      "    return df\n",
      "def compute_binome_score_old(df, binome_penalty, prefs):\n",
      "    scores_col = [col for col in df.columns if col.endswith('_cat_score')]\n",
      "    max_scores = pd.DataFrame()\n",
      "    \n",
      "    for col in scores_col:\n",
      "        cat_weight = prefs[col.split('_')[0]]\n",
      "        max_scores[col] = cat_weight * np.where(\n",
      "            df[col] >= (1-binome_penalty)*df[col+'_binome'],\n",
      "            df[col],\n",
      "            (1-binome_penalty)*df[col+'_binome']\n",
      "            )\n",
      "    \n",
      "    return max_scores.mean(axis=1).round(1)\n",
      "def compute_binome_score(df, scores_cat, prefs):\n",
      "    scores_cat_col = [col for col in df.columns if col.endswith('_cat_score')]\n",
      "    weighted_scores = pd.DataFrame()\n",
      "    for col in scores_cat_col:\n",
      "        cat_weight =  prefs['poids_'+col.split('_')[0]]\n",
      "        weighted_scores[col] = cat_weight * df[col]\n",
      "    \n",
      "    return weighted_scores.astype(float).mean(axis=1)\n",
      "def best_score_compute(df):\n",
      "    #Keeping the best (top #1) monome or binome result for each commune\n",
      "    best = df.sort_values('weighted_score', ascending=False).groupby('codgeo').head(1)\n",
      "    return best\n",
      "#Main function that aggregates most of the above in one sequence\n",
      "def compute_odis_score(df, scores_cat, prefs):\n",
      "    df = add_distance_to_current_loc(df, current_codgeo=prefs['commune_actuelle'])\n",
      "\n",
      "    # We filter by distance to reduce the compute cost on a smaller odis_search dataframe\n",
      "    odis_search = filter_loc_by_distance(df, distance=prefs['loc_distance_km'])\n",
      "\n",
      "    # We compute the subject specific scores\n",
      "    odis_scored = compute_criteria_scores(odis_search, prefs=prefs)\n",
      "\n",
      "    # We add the criteria scores for all neighbor communes forming monomes and binomes\n",
      "    odis_exploded = adding_score_voisins(odis_scored, scores_cat)\n",
      "\n",
      "    # We compute the category scores for both the target and the binome\n",
      "    odis_exploded = compute_cat_scores(odis_exploded, scores_cat=scores_cat, penalty=prefs['binome_penalty'])\n",
      "\n",
      "    # We computing the final weighted score for all commune<->voisin combinations\n",
      "    odis_exploded['weighted_score'] = compute_binome_score(odis_exploded, scores_cat=scores_cat, prefs=prefs)\n",
      "\n",
      "    # We keep best monome or binome for each commune \n",
      "    odis_search_best = best_score_compute(odis_exploded)\n",
      "\n",
      "    return odis_search_best\n",
      "# THIS SHOULD BE THE END OF JUPYTER NOTEBOOK EXPORT\n"
     ]
    }
   ],
   "source": [
    "%save -f -r ../streamlit/odis_stream2_scoring.py 1-13\n",
    "# This saves the cells 0 to 22 (and their execution history unfortunately) to a python file that I can use in Streamlit\n",
    "# Make sure to restart before running this cell\n",
    "# Don't forget to restart streamlit after this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7ae6d547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart and run all the cells above this one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9bf13d",
   "metadata": {},
   "source": [
    "## Notebook explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "65293654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "\n",
    "ODIS_FILE = '../csv/odis_june_2025_jacques.parquet'\n",
    "SCORES_CAT_FILE = '../csv/odis_scores_cat.csv'\n",
    "METIERS_FILE = '../csv/dares_nomenclature_fap2021.csv'\n",
    "FORMATIONS_FILE = '../csv/index_formations.csv'\n",
    "ECOLES_FILE = '../csv/annuaire_ecoles_france_mini.parquet'\n",
    "MATERNITE_FILE = '../csv/annuaire_maternites_DREES.csv'\n",
    "SANTE_FILE = '../csv/annuaire_sante_finess.parquet'\n",
    "\n",
    "odis, scores_cat, codfap_index, codformations_index, annuaire_ecoles, annuaire_sante = init_loading_datasets(\n",
    "    odis_file=ODIS_FILE,\n",
    "    scores_cat_file=SCORES_CAT_FILE,\n",
    "    metiers_file=METIERS_FILE,\n",
    "    formations_file=FORMATIONS_FILE,\n",
    "    ecoles_file=ECOLES_FILE,\n",
    "    maternites_file=MATERNITE_FILE,\n",
    "    sante_file=SANTE_FILE\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28482f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subject preferences weighted score computation\n",
    "prefs = {\n",
    "    'poids_emploi':2,\n",
    "    'poids_logement':1,\n",
    "    'poids_education':1,\n",
    "    'poids_soutien':1,\n",
    "    'poids_mobilité':0,\n",
    "    'commune_actuelle':'33001',\n",
    "    'hebergement':\"Chez l'habitant\",\n",
    "    'logement':'Location',\n",
    "    'loc_distance_km':200,\n",
    "    'nb_adultes':1,\n",
    "    'nb_enfants':1,\n",
    "    'codes_metiers':[['S1X40','J0X33','A1X41'], ['T4X60','T2A60']],\n",
    "    'codes_formations':[['423'], ['315','100']],\n",
    "    'classe_enfants':['Maternelle', 'Collège'],\n",
    "    'besoin_sante': None,\n",
    "    'binome_penalty':0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6b63584a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49|Add Distance End\n",
      "0.01|Filter Loc by Distance\n",
      "0.09|Compute Subject Score End\n",
      "0.01|adding_score_voisin Prep Done\n",
      "0.22|adding_score_voisin Additself Done\n",
      "0.05|adding_score_voisin Explode Done\n",
      "0.08|adding_score_voisin Merge Done\n",
      "0.01|adding_score_voisin Binome Replace Done\n",
      "0.03|adding_score_voisin Monome Cleanup Done\n",
      "0.41|Adding Score Voisin\n",
      "1.36|Compute Cat Score End\n",
      "0.02|Compute Weighted Score End\n",
      "0.05|Compute Best Score End\n"
     ]
    }
   ],
   "source": [
    "# Step by Step Execution\n",
    "from time import time\n",
    "\n",
    "def performance_tracker(t, text, timer_mode):\n",
    "    if timer_mode:\n",
    "        print(str(round(time()-t,2))+'|'+text)\n",
    "        return time()\n",
    "t = time()\n",
    "timer_mode = True\n",
    "\n",
    "df = odis\n",
    "score_cat = scores_cat\n",
    "prefs = prefs\n",
    "#\n",
    "df = add_distance_to_current_loc(df, current_codgeo=prefs['commune_actuelle'])\n",
    "t = performance_tracker(t, 'Add Distance End', timer_mode)\n",
    "\n",
    "# We filter by distance to reduce the compute cost on a smaller odis_search dataframe\n",
    "odis_search = filter_loc_by_distance(df, distance=prefs['loc_distance_km'])\n",
    "t = performance_tracker(t, 'Filter Loc by Distance', timer_mode)\n",
    "\n",
    "# We compute the subject specific scores\n",
    "odis_scored = compute_criteria_scores(odis_search, prefs=prefs)\n",
    "t = performance_tracker(t, 'Compute Subject Score End', timer_mode)\n",
    "\n",
    "# We add the criteria scores for all neighbor communes forming monomes and binomes\n",
    "odis_exploded = adding_score_voisins(odis_scored, scores_cat)\n",
    "t = performance_tracker(t, 'Adding Score Voisin', timer_mode)\n",
    "\n",
    "# We compute the category scores for both the target and the binome\n",
    "odis_exploded = compute_cat_scores(odis_exploded, scores_cat=scores_cat, penalty=prefs['binome_penalty'])\n",
    "t = performance_tracker(t, 'Compute Cat Score End', timer_mode)\n",
    "\n",
    "# We computing the final weighted score for all commune<->voisin combinations\n",
    "odis_exploded['weighted_score'] = compute_binome_score(odis_exploded, scores_cat=scores_cat, prefs=prefs)\n",
    "t = performance_tracker(t, 'Compute Weighted Score End', timer_mode)\n",
    "\n",
    "# We keep best monome or binome for each commune \n",
    "odis_search_best = best_score_compute(odis_exploded)\n",
    "t = performance_tracker(t, 'Compute Best Score End', timer_mode)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a223adf",
   "metadata": {},
   "source": [
    "## 9. Export to SuperSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e08dbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def concatenate_strings(row):\n",
    "#   return '{\"type\": \"Feature\",\"geometry\":' + shp.to_geojson(row['polygon']) + '}'\n",
    "\n",
    "\n",
    "# odis_search_best_export = gpd.GeoDataFrame(odis_search_best.copy())\n",
    "# odis_search_best_export.set_geometry(odis_search_best_export.polygon, crs='EPSG:2154', inplace=True)\n",
    "# odis_search_best_export.to_crs(epsg=4326, inplace=True)\n",
    "# odis_search_best_export[\"polygon_as_json\"] = odis_search_best_export.apply(concatenate_strings, axis=1)\n",
    "# odis_search_best_export.drop(['polygon','polygon_binome'], axis=1, inplace=True)\n",
    "\n",
    "# cols = ['met_match_codes','met_match_codes_binome','be_codfap_top','be_libfap_top','codgeo_voisins_binome','pitch']\n",
    "# for col in cols:\n",
    "#     odis_search_best_export[col] = odis_search_best_export[col].apply(lambda x: x.tolist() if type(x) == np.ndarray else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93370656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sqlalchemy import create_engine, text\n",
    "\n",
    "# db_host = \"localhost\"  # Replace with the actual host (e.g., 'superset_db' if in the same Docker network, or 'localhost' if exposed)\n",
    "# db_port = \"5433\"  # Replace with the actual port (usually 5432)\n",
    "# db_user = \"superset\"  # Replace with the database user (often 'superset')\n",
    "# db_password = \"superset\"  # Replace with the database password\n",
    "# db_name = \"examples\"  # Replace with the database name (often 'superset')\n",
    "\n",
    "# engine = create_engine(f'postgresql+psycopg2://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aaddc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# table_name = \"odis_stream2_result\"  # Choose a name for the table in PostgreSQL\n",
    "# odis_search_best_export.to_sql(table_name, engine, if_exists='replace', schema='public', index=False)\n",
    "# sql = text(\"GRANT SELECT ON odis_stream2_result TO examples\")\n",
    "\n",
    "# with engine.begin() as connection:\n",
    "#     connection.execute(sql)\n",
    "\n",
    "# print(f\"DataFrame successfully written to table '{table_name}' in the Superset database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4674265d",
   "metadata": {},
   "source": [
    "Note to myself:\n",
    "Après avoir importé les données dans Postgres il faut donner les droits au user 'examples' sur la table\n",
    "> docker exec -it superset_db psql -h superset_db -p 5432 -U superset -d examples\n",
    "\n",
    "> GRANT SELECT ON odis_stream2_result TO examples;\n",
    "\n",
    "> GRANT USAGE ON SCHEMA public TO examples;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff23fc33",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "123893e3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "odis-Nf-mTAVv-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
