{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9568f991",
   "metadata": {},
   "source": [
    "# Preprocessor Notebook : Zipped CSV Files\n",
    "\n",
    "Ce notebook permet de traiter les sources de données récupérés en .zip et contenant des fichiers csv.\n",
    "\n",
    " ### Paramètres\n",
    " Ce Notebook prend des paramètres en entrée, définis sur la seconde cellule (ci-dessous).\n",
    " La cellule a le tag \"parameters\" ce qui permet de lui passer des valeurs via papermill.\n",
    " - filepath : le chemin vers le fichier Excel à traiter\n",
    " - model_name : le nom du modèle source\n",
    "\n",
    " ### Principe\n",
    " Ce notebook extrait les fichiers CSV contenus dans un fichier ZIP et les charge en schéma Bronze.\n",
    " Chaque fichier CSV constitue un dataset et donc une table en schéma Bronze. \n",
    " La règle de nommage des tables est la suivante :\n",
    " - Par défaut : \"nom de la table\" = \"nom_domaine\"_\"nom_modèle\"_\"nom du fichier csv extrait\"\n",
    " - Si le nom dépasse 63 caractères (limite PostgreSQL) : \"nom de la table\" = \"nom_domaine\"_\"nom du fichier csv extrait\"\n",
    " \n",
    " La suite d'actions réalisée est :\n",
    " - Import des fonctions utiles\n",
    " - Dézip de tous les fichiers contenus dans le .zip\n",
    " - Dump d'un artefact pour chaque fichier extrait (quel que soit le format)\n",
    " - Import de tous les artefacts CSV en Pandas \n",
    " - Initialisation d'une connexion PostGreSQL\n",
    " \n",
    " Pour chaque dataframe :\n",
    " - Drop de la table Bronze (if exists)\n",
    " - Chargement du dataframe en Bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aee0f389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "changing to parent dir\n",
      "/Users/alex/dev/13_odis\n"
     ]
    }
   ],
   "source": [
    "# Manage all imports\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "# Dirty trick to be able to import common odis modules, if the notebook is not executed from 13_odis\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "while not current_dir.endswith(\"13_odis\"):\n",
    "    print(\"changing to parent dir\")\n",
    "    os.chdir(parent_dir)\n",
    "    current_dir = parent_dir\n",
    "    parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "print(os.getcwd())\n",
    "sys.path.append(current_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa34476",
   "metadata": {},
   "source": [
    "# Initialisation\n",
    "\n",
    "Chargement des principaux imports et variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f19fac1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional imports\n",
    "from common.config import load_config\n",
    "from common.data_source_model import DataSourceModel\n",
    "from common.utils.file_handler import FileHandler\n",
    "from common.utils.interfaces.data_handler import OperationType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd4c15cb",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# define parameters for papermill\n",
    "\n",
    "# model_name = \"emploi.salaire_median\"\n",
    "# filepath = 'data/imports/emploi/emploi.salaire_median_1.zip'\n",
    "\n",
    "model_name = \"population.nb_menages_2021\"\n",
    "filepath = 'data/imports/population/population.nb_menages_2021_1.zip'\n",
    "\n",
    "# model_name = \"emploi.etablissements_employeurs_secteur_prive\"\n",
    "# filepath = 'data/imports/emploi/emploi.etablissements_employeurs_secteur_prive_1.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fadac0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize common variables\n",
    "dataframes = {}\n",
    "artifacts = []\n",
    "\n",
    "config = load_config(\"datasources.yaml\", response_model=DataSourceModel)\n",
    "model = config.get_model( model_name = model_name )\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# Instantiate File Handler for file loads and dumps\n",
    "handler = FileHandler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d07a8a",
   "metadata": {},
   "source": [
    "# Extraction des CSV\n",
    "\n",
    "Extraction des fichiers CSV du Zip, avec une fonction pour calculer les noms de fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370e4b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subtable_name(dfname:str, model:DataSourceModel) -> str:\n",
    "\n",
    "    subtable_name = ''\n",
    "\n",
    "    long_name = f\"{model.table_name}_{dfname.lower()}\"\n",
    "\n",
    "    print(len(long_name))\n",
    "\n",
    "    if len(long_name) >= 63:\n",
    "        print(f\"Long table name exceeds the 63 characters limit: {long_name}\")\n",
    "        subtable_name = f\"{model.domain_name}_{dfname.lower()}\"\n",
    "        print(f\"Creating table with shorter name: {subtable_name}\")\n",
    "        print(f\"Shorter table name length: {len(subtable_name)}\")\n",
    "    else:\n",
    "        subtable_name = long_name\n",
    "\n",
    "    return subtable_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b5ebbd6-1ec4-4e45-8890-1155ea52684c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv\n",
      "39\n",
      "2025-06-25 23:16:07,360 - DEBUG :: file_handler.py :: dump (130) :: dumping: data/imports/population/population.nb_menages_2021_population_nb_menages_2021_td_men1_2021.csv\n",
      "2025-06-25 23:16:07,429 - DEBUG :: file_handler.py :: file_dump (273) :: population.nb_menages_2021 -> results saved to : 'data/imports/population/population.nb_menages_2021_population_nb_menages_2021_td_men1_2021.csv'\n",
      "{'name': 'population_nb_menages_2021_td_men1_2021', 'storage_info': {'location': 'data/imports/population', 'format': 'csv', 'file_name': 'population.nb_menages_2021_population_nb_menages_2021_td_men1_2021.csv', 'encoding': 'utf-8'}, 'load_to_bronze': True, 'success': True}\n"
     ]
    }
   ],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "# unzip and dump files into the data/imports folder\n",
    "with open(filepath, 'rb') as f:\n",
    "    zip_archive = ZipFile(f)\n",
    "\n",
    "    zip_members = zip_archive.infolist()\n",
    "    for member in zip_members:\n",
    "\n",
    "        if not member.is_dir():\n",
    "\n",
    "            member_filename = member.filename\n",
    "            member_name = member_filename.split(\".\")[0]\n",
    "            member_format = member_filename.split(\".\")[-1]\n",
    "            print(member_format)\n",
    "            \n",
    "            f_member = zip_archive.open( member, 'r' ).read()\n",
    "    \n",
    "            artifact = handler.artifact_dump(\n",
    "                f_member,\n",
    "                get_subtable_name(member_name,model),\n",
    "                model,\n",
    "                format = member_format\n",
    "            )\n",
    "\n",
    "            print(artifact.model_dump(mode=\"yaml\"))\n",
    "\n",
    "            artifacts.append(artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2efce62",
   "metadata": {},
   "source": [
    "# Dump intermédiaire\n",
    "\n",
    "Sauvegarde locale de tous les artefacts extraits du zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0e95b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-25 23:15:54,282 - DEBUG :: file_handler.py :: dump (130) :: dumping: data/imports/population/population.nb_menages_2021_metadata_preprocess.json\n",
      "2025-06-25 23:15:54,285 - DEBUG :: file_handler.py :: file_dump (273) :: population.nb_menages_2021 -> results saved to : 'data/imports/population/population.nb_menages_2021_metadata_preprocess.json'\n",
      "2025-06-25 23:15:54,286 - DEBUG :: file_handler.py :: dump_metadata (455) :: Metadata written in: 'data/imports/population/population.nb_menages_2021_metadata_preprocess.json'\n"
     ]
    }
   ],
   "source": [
    "preprocess_metadata = handler.dump_metadata(\n",
    "    model = model,\n",
    "    operation = OperationType.PREPROCESS,\n",
    "    start_time = start_time,\n",
    "    complete = True,\n",
    "    errors = 0,\n",
    "    artifacts = artifacts,\n",
    "    pages = []\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8450b28d",
   "metadata": {},
   "source": [
    "# Traitement Pandas\n",
    "\n",
    "Import des CSV en Dataframes et chargement en tables Bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e35462b-bcc3-4f98-84d7-f71e8cb7873f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m base_path = Path( artifact.storage_info.location )\n\u001b[32m      6\u001b[39m filepath = base_path / artifact.storage_info.file_name\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m;\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpython\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m dataframes[ artifact.name ] = df\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/13_odis/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/13_odis/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/13_odis/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1916\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/13_odis/.venv/lib/python3.12/site-packages/pandas/io/parsers/python_parser.py:252\u001b[39m, in \u001b[36mPythonParser.read\u001b[39m\u001b[34m(self, rows)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread\u001b[39m(\n\u001b[32m    247\u001b[39m     \u001b[38;5;28mself\u001b[39m, rows: \u001b[38;5;28mint\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    248\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[\n\u001b[32m    249\u001b[39m     Index | \u001b[38;5;28;01mNone\u001b[39;00m, Sequence[Hashable] | MultiIndex, Mapping[Hashable, ArrayLike]\n\u001b[32m    250\u001b[39m ]:\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m252\u001b[39m         content = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_lines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    253\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    254\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._first_chunk:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/13_odis/.venv/lib/python3.12/site-packages/pandas/io/parsers/python_parser.py:1140\u001b[39m, in \u001b[36mPythonParser._get_lines\u001b[39m\u001b[34m(self, rows)\u001b[39m\n\u001b[32m   1137\u001b[39m rows = \u001b[32m0\u001b[39m\n\u001b[32m   1139\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1140\u001b[39m     next_row = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_iter_line\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow_num\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1141\u001b[39m     rows += \u001b[32m1\u001b[39m\n\u001b[32m   1143\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m next_row \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/13_odis/.venv/lib/python3.12/site-packages/pandas/io/parsers/python_parser.py:805\u001b[39m, in \u001b[36mPythonParser._next_iter_line\u001b[39m\u001b[34m(self, row_num)\u001b[39m\n\u001b[32m    802\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    803\u001b[39m     \u001b[38;5;66;03m# assert for mypy, data is Iterator[str] or None, would error in next\u001b[39;00m\n\u001b[32m    804\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m805\u001b[39m     line = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    806\u001b[39m     \u001b[38;5;66;03m# for mypy\u001b[39;00m\n\u001b[32m    807\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(line, \u001b[38;5;28mlist\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "for artifact in artifacts:\n",
    "\n",
    "    base_path = Path( artifact.storage_info.location )\n",
    "    filepath = base_path / artifact.storage_info.file_name\n",
    "\n",
    "    df = pd.read_csv(\n",
    "        filepath,\n",
    "        sep = ';',\n",
    "        engine = 'python'\n",
    "        )\n",
    "\n",
    "    dataframes[ artifact.name ] = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828b03e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataframes:\n",
      "population_nb_menages_2021_td_men1_2021\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NIVGEO</th>\n",
       "      <th>CODGEO</th>\n",
       "      <th>LIBGEO</th>\n",
       "      <th>NPERC</th>\n",
       "      <th>CS2_24</th>\n",
       "      <th>NB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COM</td>\n",
       "      <td>01001</td>\n",
       "      <td>L'Abergement-Clémenciat</td>\n",
       "      <td>3</td>\n",
       "      <td>46</td>\n",
       "      <td>9.687066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COM</td>\n",
       "      <td>01001</td>\n",
       "      <td>L'Abergement-Clémenciat</td>\n",
       "      <td>2</td>\n",
       "      <td>61</td>\n",
       "      <td>4.801620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COM</td>\n",
       "      <td>01001</td>\n",
       "      <td>L'Abergement-Clémenciat</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>4.928460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>COM</td>\n",
       "      <td>01001</td>\n",
       "      <td>L'Abergement-Clémenciat</td>\n",
       "      <td>2</td>\n",
       "      <td>56</td>\n",
       "      <td>4.801620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>COM</td>\n",
       "      <td>01001</td>\n",
       "      <td>L'Abergement-Clémenciat</td>\n",
       "      <td>4</td>\n",
       "      <td>56</td>\n",
       "      <td>4.885812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  NIVGEO CODGEO                   LIBGEO  NPERC  CS2_24        NB\n",
       "0    COM  01001  L'Abergement-Clémenciat      3      46  9.687066\n",
       "1    COM  01001  L'Abergement-Clémenciat      2      61  4.801620\n",
       "2    COM  01001  L'Abergement-Clémenciat      5      32  4.928460\n",
       "3    COM  01001  L'Abergement-Clémenciat      2      56  4.801620\n",
       "4    COM  01001  L'Abergement-Clémenciat      4      56  4.885812"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loaded dataframes:\")\n",
    "last_key = ''\n",
    "for key in dataframes.keys():\n",
    "    last_key = key\n",
    "    print(key)\n",
    "\n",
    "dataframes[last_key].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f122f2-89aa-4c2c-a124-83dead75c61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "import sqlalchemy\n",
    "from sqlalchemy import text\n",
    "\n",
    "# prepare db client\n",
    "vals = dotenv_values()\n",
    "\n",
    "conn_str = \"postgresql://{}:{}@{}:{}/{}\".format(\n",
    "    vals[\"PG_DB_USER\"],\n",
    "    vals[\"PG_DB_PWD\"],\n",
    "    vals[\"PG_DB_HOST\"],\n",
    "    vals[\"PG_DB_PORT\"],\n",
    "    vals[\"PG_DB_NAME\"]\n",
    ")\n",
    "\n",
    "dbengine = sqlalchemy.create_engine(conn_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8e6152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping if exists: emploi_ds_rp_navettes_princ_metadata\n",
      "Inserting DataFrame emploi_ds_rp_navettes_princ_metadata\n",
      "Dropping if exists: emploi_deplacement_domicile_travail_ds_rp_navettes_princ_data\n",
      "Inserting DataFrame emploi_deplacement_domicile_travail_ds_rp_navettes_princ_data\n"
     ]
    }
   ],
   "source": [
    "# insert all to bronze\n",
    "# make the final table name lowercase to avoid issues in Postgre\n",
    "\n",
    "for name, dataframe in dataframes.items():\n",
    "\n",
    "    query_str = f\"DROP TABLE IF EXISTS bronze.{name} CASCADE\"\n",
    "\n",
    "    # dropping existing table with cascade\n",
    "    with dbengine.connect() as con:\n",
    "        print(f\"Dropping if exists: {name}\")\n",
    "        result = con.execute(text(query_str))\n",
    "        con.commit()\n",
    "\n",
    "    print(f\"Inserting DataFrame {name}\")\n",
    "    dataframe.to_sql(\n",
    "        name = name,\n",
    "        con = dbengine,\n",
    "        schema = 'bronze',\n",
    "        index = True,\n",
    "        if_exists = 'replace'\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
