{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9568f991",
   "metadata": {},
   "source": [
    "# Preprocessor Notebook : Zipped CSV Files\n",
    "\n",
    "Ce notebook permet de traiter les sources de données récupérés en .zip et contenant des fichiers csv.\n",
    "\n",
    " ### Paramètres\n",
    " Ce Notebook prend des paramètres en entrée, définis sur la seconde cellule (ci-dessous).\n",
    " La cellule a le tag \"parameters\" ce qui permet de lui passer des valeurs via papermill.\n",
    " - filepath : le chemin vers le fichier Excel à traiter\n",
    " - model_name : le nom du modèle source\n",
    "\n",
    " ### Principe\n",
    " Ce notebook extrait les fichiers CSV contenus dans un fichier ZIP et les charge en schéma Bronze.\n",
    " Chaque fichier CSV constitue un dataset et donc une table en schéma Bronze. \n",
    " La règle de nommage des tables est la suivante :\n",
    " - Par défaut : \"nom de la table\" = \"nom_domaine\"_\"nom_modèle\"_\"nom du fichier csv extrait\"\n",
    " - Si le nom dépasse 63 caractères (limite PostgreSQL) : \"nom de la table\" = \"nom_domaine\"_\"nom du fichier csv extrait\"\n",
    " \n",
    " La suite d'actions réalisée est :\n",
    " - Import des fonctions utiles\n",
    " - Dézip de tous les fichiers contenus dans le .zip\n",
    " - Dump d'un artefact pour chaque fichier extrait (quel que soit le format)\n",
    " - Import de tous les artefacts CSV en Pandas \n",
    " - Initialisation d'une connexion PostGreSQL\n",
    " \n",
    " Pour chaque dataframe :\n",
    " - Drop de la table Bronze (if exists)\n",
    " - Chargement du dataframe en Bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aee0f389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/alex/dev/13_odis\n"
     ]
    }
   ],
   "source": [
    "# Manage all imports\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "# Dirty trick to be able to import common odis modules, if the notebook is not executed from 13_odis\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "while not current_dir.endswith(\"13_odis\"):\n",
    "    print(\"changing to parent dir\")\n",
    "    os.chdir(parent_dir)\n",
    "    current_dir = parent_dir\n",
    "    parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "print(os.getcwd())\n",
    "sys.path.append(current_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation\n",
    "\n",
    "Chargement des principaux imports et variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f19fac1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional imports\n",
    "from common.config import load_config\n",
    "from common.data_source_model import DataSourceModel\n",
    "from common.utils.file_handler import FileHandler\n",
    "from common.utils.interfaces.data_handler import OperationType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd4c15cb",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# define parameters for papermill\n",
    "\n",
    "# model_name = \"emploi.salaire_median\"\n",
    "# filepath = 'data/imports/emploi/emploi.salaire_median_1.zip'\n",
    "\n",
    "model_name = \"emploi.deplacement_domicile_travail\"\n",
    "filepath = 'data/imports/emploi/emploi.deplacement_domicile_travail_1.zip'\n",
    "\n",
    "# model_name = \"emploi.etablissements_employeurs_secteur_prive\"\n",
    "# filepath = 'data/imports/emploi/emploi.etablissements_employeurs_secteur_prive_1.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fadac0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize common variables\n",
    "dataframes = {}\n",
    "artifacts = []\n",
    "\n",
    "config = load_config(\"datasources.yaml\", response_model=DataSourceModel)\n",
    "model = config.get_model( model_name = model_name )\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# Instantiate File Handler for file loads and dumps\n",
    "handler = FileHandler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction des CSV\n",
    "\n",
    "Extraction des fichiers CSV du Zip, avec une fonction pour calculer les noms de fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subtable_name(dfname:str, model:DataSourceModel) -> str:\n",
    "\n",
    "    subtable_name = ''\n",
    "\n",
    "    long_name = f\"{model.table_name}_{dfname.lower()}\"\n",
    "\n",
    "    print(len(long_name))\n",
    "\n",
    "    if len(long_name) >= 63:\n",
    "        print(f\"Long table name exceeds the 63 characters limit: {long_name}\")\n",
    "        subtable_name = f\"{model.domain_name}_{dfname.lower()}\"\n",
    "        print(f\"Creating table with shorter name: {subtable_name}\")\n",
    "        print(f\"Shorter table name length: {len(subtable_name)}\")\n",
    "    else:\n",
    "        subtable_name = long_name\n",
    "\n",
    "    return subtable_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b5ebbd6-1ec4-4e45-8890-1155ea52684c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv\n",
      "65\n",
      "Long table name exceeds the 63 characters limit: emploi_deplacement_domicile_travail_ds_rp_navettes_princ_metadata\n",
      "Creating table with shorter name: emploi_ds_rp_navettes_princ_metadata\n",
      "Shorter table name length: 36\n",
      "2025-05-24 15:04:09,530 - DEBUG :: file_handler.py :: dump (130) :: dumping: data/imports/emploi/emploi.deplacement_domicile_travail_emploi_ds_rp_navettes_princ_metadata.csv\n",
      "2025-05-24 15:04:09,536 - DEBUG :: file_handler.py :: file_dump (273) :: emploi.deplacement_domicile_travail -> results saved to : 'data/imports/emploi/emploi.deplacement_domicile_travail_emploi_ds_rp_navettes_princ_metadata.csv'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'emploi_ds_rp_navettes_princ_metadata', 'storage_info': {'location': 'data/imports/emploi', 'format': 'csv', 'file_name': 'emploi.deplacement_domicile_travail_emploi_ds_rp_navettes_princ_metadata.csv', 'encoding': 'utf-8'}, 'load_to_bronze': True, 'success': True}\n",
      "csv\n",
      "61\n",
      "2025-05-24 15:04:09,659 - DEBUG :: file_handler.py :: dump (130) :: dumping: data/imports/emploi/emploi.deplacement_domicile_travail_emploi_deplacement_domicile_travail_ds_rp_navettes_princ_data.csv\n",
      "2025-05-24 15:04:09,775 - DEBUG :: file_handler.py :: file_dump (273) :: emploi.deplacement_domicile_travail -> results saved to : 'data/imports/emploi/emploi.deplacement_domicile_travail_emploi_deplacement_domicile_travail_ds_rp_navettes_princ_data.csv'\n",
      "{'name': 'emploi_deplacement_domicile_travail_ds_rp_navettes_princ_data', 'storage_info': {'location': 'data/imports/emploi', 'format': 'csv', 'file_name': 'emploi.deplacement_domicile_travail_emploi_deplacement_domicile_travail_ds_rp_navettes_princ_data.csv', 'encoding': 'utf-8'}, 'load_to_bronze': True, 'success': True}\n"
     ]
    }
   ],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "# unzip and dump files into the data/imports folder\n",
    "with open(filepath, 'rb') as f:\n",
    "    zip_archive = ZipFile(f)\n",
    "\n",
    "    zip_members = zip_archive.infolist()\n",
    "    for member in zip_members:\n",
    "\n",
    "        if not member.is_dir():\n",
    "\n",
    "            member_filename = member.filename\n",
    "            member_name = member_filename.split(\".\")[0]\n",
    "            member_format = member_filename.split(\".\")[-1]\n",
    "            print(member_format)\n",
    "            \n",
    "            f_member = zip_archive.open( member, 'r' ).read()\n",
    "    \n",
    "            artifact = handler.artifact_dump(\n",
    "                f_member,\n",
    "                get_subtable_name(member_name,model),\n",
    "                model,\n",
    "                format = member_format\n",
    "            )\n",
    "\n",
    "            print(artifact.model_dump(mode=\"yaml\"))\n",
    "\n",
    "            artifacts.append(artifact)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dump intermédiaire\n",
    "\n",
    "Sauvegarde locale de tous les artefacts extraits du zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0e95b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-24 15:04:09,788 - DEBUG :: file_handler.py :: dump (130) :: dumping: data/imports/emploi/emploi.deplacement_domicile_travail_metadata_preprocess.json\n",
      "2025-05-24 15:04:09,791 - DEBUG :: file_handler.py :: file_dump (273) :: emploi.deplacement_domicile_travail -> results saved to : 'data/imports/emploi/emploi.deplacement_domicile_travail_metadata_preprocess.json'\n",
      "2025-05-24 15:04:09,792 - DEBUG :: file_handler.py :: dump_metadata (455) :: Metadata written in: 'data/imports/emploi/emploi.deplacement_domicile_travail_metadata_preprocess.json'\n"
     ]
    }
   ],
   "source": [
    "preprocess_metadata = handler.dump_metadata(\n",
    "    model = model,\n",
    "    operation = OperationType.PREPROCESS,\n",
    "    start_time = start_time,\n",
    "    complete = True,\n",
    "    errors = 0,\n",
    "    artifacts = artifacts,\n",
    "    pages = []\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traitement Pandas\n",
    "\n",
    "Import des CSV en Dataframes et chargement en tables Bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e35462b-bcc3-4f98-84d7-f71e8cb7873f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "for artifact in artifacts:\n",
    "\n",
    "    base_path = Path( artifact.storage_info.location )\n",
    "    filepath = base_path / artifact.storage_info.file_name\n",
    "\n",
    "    df = pd.read_csv(\n",
    "        filepath,\n",
    "        sep = ';',\n",
    "        engine = 'python'\n",
    "        )\n",
    "\n",
    "    dataframes[ artifact.name ] = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataframes:\n",
      "emploi_ds_rp_navettes_princ_metadata\n",
      "emploi_deplacement_domicile_travail_ds_rp_navettes_princ_data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AGE</th>\n",
       "      <th>EMPSTA_ENQ</th>\n",
       "      <th>FREQ</th>\n",
       "      <th>GEO</th>\n",
       "      <th>GEO_OBJECT</th>\n",
       "      <th>RP_MEASURE</th>\n",
       "      <th>TRANS</th>\n",
       "      <th>WORK_AREA</th>\n",
       "      <th>TIME_PERIOD</th>\n",
       "      <th>OBS_VALUE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Y_GE15</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>73087</td>\n",
       "      <td>COM</td>\n",
       "      <td>POP</td>\n",
       "      <td>_T</td>\n",
       "      <td>21</td>\n",
       "      <td>2015</td>\n",
       "      <td>1803.446354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Y_GE15</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>72237</td>\n",
       "      <td>COM</td>\n",
       "      <td>POP</td>\n",
       "      <td>_T</td>\n",
       "      <td>21</td>\n",
       "      <td>2015</td>\n",
       "      <td>202.962271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Y_GE15</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>73012</td>\n",
       "      <td>COM</td>\n",
       "      <td>POP</td>\n",
       "      <td>_T</td>\n",
       "      <td>23</td>\n",
       "      <td>2015</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Y_GE15</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>73006</td>\n",
       "      <td>COM</td>\n",
       "      <td>POP</td>\n",
       "      <td>_T</td>\n",
       "      <td>22</td>\n",
       "      <td>2015</td>\n",
       "      <td>25.980651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Y_GE15</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>73220</td>\n",
       "      <td>COM</td>\n",
       "      <td>POP</td>\n",
       "      <td>_T</td>\n",
       "      <td>21</td>\n",
       "      <td>2015</td>\n",
       "      <td>108.939577</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      AGE  EMPSTA_ENQ FREQ    GEO GEO_OBJECT RP_MEASURE TRANS WORK_AREA  \\\n",
       "0  Y_GE15           1    A  73087        COM        POP    _T        21   \n",
       "1  Y_GE15           1    A  72237        COM        POP    _T        21   \n",
       "2  Y_GE15           1    A  73012        COM        POP    _T        23   \n",
       "3  Y_GE15           1    A  73006        COM        POP    _T        22   \n",
       "4  Y_GE15           1    A  73220        COM        POP    _T        21   \n",
       "\n",
       "   TIME_PERIOD    OBS_VALUE  \n",
       "0         2015  1803.446354  \n",
       "1         2015   202.962271  \n",
       "2         2015     2.000000  \n",
       "3         2015    25.980651  \n",
       "4         2015   108.939577  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loaded dataframes:\")\n",
    "last_key = ''\n",
    "for key in dataframes.keys():\n",
    "    last_key = key\n",
    "    print(key)\n",
    "\n",
    "dataframes[last_key].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26f122f2-89aa-4c2c-a124-83dead75c61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "import sqlalchemy\n",
    "from sqlalchemy import text\n",
    "\n",
    "# prepare db client\n",
    "vals = dotenv_values()\n",
    "\n",
    "conn_str = \"postgresql://{}:{}@{}:{}/{}\".format(\n",
    "    vals[\"PG_DB_USER\"],\n",
    "    vals[\"PG_DB_PWD\"],\n",
    "    vals[\"PG_DB_HOST\"],\n",
    "    vals[\"PG_DB_PORT\"],\n",
    "    vals[\"PG_DB_NAME\"]\n",
    ")\n",
    "\n",
    "dbengine = sqlalchemy.create_engine(conn_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca8e6152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping if exists: emploi_ds_rp_navettes_princ_metadata\n",
      "Inserting DataFrame emploi_ds_rp_navettes_princ_metadata\n",
      "Dropping if exists: emploi_deplacement_domicile_travail_ds_rp_navettes_princ_data\n",
      "Inserting DataFrame emploi_deplacement_domicile_travail_ds_rp_navettes_princ_data\n"
     ]
    }
   ],
   "source": [
    "# insert all to bronze\n",
    "# make the final table name lowercase to avoid issues in Postgre\n",
    "\n",
    "for name, dataframe in dataframes.items():\n",
    "\n",
    "    query_str = f\"DROP TABLE IF EXISTS bronze.{name} CASCADE\"\n",
    "\n",
    "    # dropping existing table with cascade\n",
    "    with dbengine.connect() as con:\n",
    "        print(f\"Dropping if exists: {name}\")\n",
    "        result = con.execute(text(query_str))\n",
    "        con.commit()\n",
    "\n",
    "    print(f\"Inserting DataFrame {name}\")\n",
    "    dataframe.to_sql(\n",
    "        name = name,\n",
    "        con = dbengine,\n",
    "        schema = 'bronze',\n",
    "        index = True,\n",
    "        if_exists = 'replace'\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
