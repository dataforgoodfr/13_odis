{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des biblioth√®ques n√©cessaires\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import psycopg2\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "\n",
    "\n",
    "from psycopg2.extras import Json\n",
    "from psycopg2.extras import execute_values\n",
    "import math\n",
    "from datetime import datetime\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_s3_client():\n",
    "    \"\"\"\n",
    "    Creates an S3 client using environment variables for credentials.\n",
    "    \n",
    "    :return: Boto3 S3 client\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # R√©cup√®re les identifiants depuis les variables d'environnement\n",
    "        access_key = os.getenv('SCW_ACCESS_KEY')\n",
    "        secret_key = os.getenv('SCW_SECRET_KEY')\n",
    "        endpoint_url = os.getenv('SCW_OBJECT_STORAGE_ENDPOINT')\n",
    "        region = os.getenv('SCW_REGION')\n",
    "\n",
    "        if not access_key or not secret_key or not endpoint_url or not region:\n",
    "            raise ValueError(\"Identifiants S3 manquants dans les variables d'environnement.\")\n",
    "\n",
    "        # Initialise le client S3\n",
    "        session = boto3.Session(\n",
    "            aws_access_key_id=access_key,\n",
    "            aws_secret_access_key=secret_key,\n",
    "            region_name=region\n",
    "        )\n",
    "        s3_client = session.client('s3', endpoint_url=endpoint_url)\n",
    "\n",
    "        print(\"‚úîÔ∏è Client S3 cr√©√© avec succ√®s.\")\n",
    "        return s3_client\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå √âchec de la cr√©ation du client S3 : {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def read_excel_from_s3(s3_client, bucket_name, file_path):\n",
    "    \"\"\"\n",
    "    Retrieves an Excel file from an S3 bucket and loads it as a stream\n",
    "    \n",
    "    :param s3_client: S3 client\n",
    "    :param bucket_name: Name of the S3 bucket\n",
    "    :param file_path: Key (path) of the file in the S3 bucket\n",
    "    :return: A file object as a stream\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Retrieve the file from S3 as a binary stream\n",
    "        response = s3_client.get_object(Bucket=bucket_name, Key=file_path)\n",
    "        file_content = response['Body'].read()\n",
    "        \n",
    "        # Use BytesIO to create a file-like object from the binary data\n",
    "        file_stream = BytesIO(file_content)\n",
    "        \n",
    "        return file_stream\n",
    "\n",
    "    except NoCredentialsError:\n",
    "        print(\"‚ùå Les informations d'identification sont manquantes ou incorrectes.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur lors de la r√©cup√©ration du fichier : {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_sheet_from_dataframe(dataframe):\n",
    "    \"\"\"\n",
    "    Converts a Pandas DataFrame to JSON format.\n",
    "    \n",
    "    :param dataframe: Pandas DataFrame\n",
    "    :return: JSON data (list of dictionaries)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        json_data = dataframe.to_dict(orient='records')\n",
    "        print(f\"‚úîÔ∏è {len(json_data)} lignes converties en JSON.\")\n",
    "        return json_data\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur lors de la conversion du DataFrame en JSON : {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def get_connexion():\n",
    "    \"\"\"\n",
    "     Establishes a PostgreSQL connection using environment variables.\n",
    "    \n",
    "    :return: PostgreSQL connection object\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=os.getenv('PG_DB_NAME'),\n",
    "            user=os.getenv('PG_DB_USER'),\n",
    "            password=os.getenv('PG_DB_PWD'),\n",
    "            host=\"localhost\",\n",
    "            port=\"5432\"\n",
    "        )\n",
    "        print(\"‚úîÔ∏è Connexion √† PostgreSQL r√©ussie.\")\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå √âchec de la connexion √† PostgreSQL : {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def create_bronze_table(conn, table_name):\n",
    "    \"\"\"\n",
    "    Drops the 'bronze' table if it exists and creates a new one in PostgreSQL.\n",
    "\n",
    "    :param conn: PostgreSQL connection object\n",
    "    :param table_name: Name of the table to create\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "         # Force drop the table with CASCADE to remove dependencies\n",
    "        drop_table_query = f\"DROP TABLE IF EXISTS {table_name} CASCADE;\"\n",
    "        cursor.execute(drop_table_query)\n",
    "        print(f\"‚úîÔ∏è Table '{table_name}' supprim√©e avec CASCADE.\")\n",
    "\n",
    "        # Recreate the table\n",
    "        create_table_query = f\"\"\"\n",
    "        CREATE TABLE {table_name} (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            data JSONB NOT NULL,\n",
    "            created_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP\n",
    "        );\n",
    "        \"\"\"\n",
    "        cursor.execute(drop_table_query)\n",
    "        cursor.execute(create_table_query)\n",
    "        conn.commit()\n",
    "        print(f\"‚úîÔ∏è La table '{table_name}' est pr√™te.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur lors de la cr√©ation de la table '{table_name}' : {e}\")\n",
    "        conn.rollback()\n",
    "        raise\n",
    "    finally:\n",
    "        cursor.close()\n",
    "\n",
    "\n",
    "def clean_json(obj):\n",
    "    \"\"\"\n",
    "    Cleans JSON data by removing invalid values (e.g., NaN, INF, empty strings).\n",
    "    \n",
    "    :param obj: JSON object\n",
    "    :return: Cleaned JSON object\n",
    "    \"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: clean_json(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [clean_json(v) for v in obj]\n",
    "    elif isinstance(obj, float):\n",
    "        return None if math.isinf(obj) or math.isnan(obj) else obj\n",
    "    elif isinstance(obj, str):\n",
    "        return None if obj.upper() in (\"INF\", \"NA\", \"NAN\", \"\") else obj\n",
    "    return obj\n",
    "\n",
    "\n",
    "def import_data_to_bronze_table(conn, table_name, json_data, created_at=None):\n",
    "    \"\"\"\n",
    "    Inserts JSON data into a PostgreSQL table.\n",
    "\n",
    "    :param conn: PostgreSQL connection object\n",
    "    :param table_name: Name of the table\n",
    "    :param json_data: List of dictionaries representing JSON data\n",
    "    :param created_at: Timestamp for data insertion (defaults to now)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        created_at = created_at or datetime.timezone.utc()\n",
    "\n",
    "        insert_query = f\"INSERT INTO {table_name} (created_at, data) VALUES %s\"\n",
    "        json_records = [(created_at, Json(clean_json(record))) for record in json_data]\n",
    "\n",
    "        execute_values(cursor, insert_query, json_records)\n",
    "        conn.commit()\n",
    "\n",
    "        print(f\"‚úîÔ∏è {len(json_records)} lignes ins√©r√©es dans '{table_name}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur lors de l'insertion des donn√©es dans '{table_name}' : {e}\")\n",
    "        conn.rollback()\n",
    "        raise\n",
    "    finally:\n",
    "        cursor.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìÖ D√©but du traitement √† 2025-04-02 07:21:04.987509\n",
      "\n",
      "üöÄChargement du fichier depuis S3 : resultats_rpls_2024_v3.xlsx (Bucket: odis-s3)\n",
      "‚úîÔ∏è Client S3 cr√©√© avec succ√®s.\n",
      "‚úîÔ∏è Fichier charg√©\n",
      "\n",
      "üöÄChargement des diff√©rentes feuilles du fichier Excel dans des DataFrames pandas\n",
      "‚úîÔ∏è Feuilles charg√©es\n",
      "\n",
      "üöÄConversion des DataFrames en JSON\n",
      "‚úîÔ∏è 23 lignes converties en JSON.\n",
      "‚úîÔ∏è 96 lignes converties en JSON.\n",
      "‚úîÔ∏è 16859 lignes converties en JSON.\n",
      "‚úîÔ∏è 1325 lignes converties en JSON.\n",
      "\n",
      "üöÄ Connexion √† PostgreSQL\n",
      "‚úîÔ∏è Connexion √† PostgreSQL r√©ussie.\n",
      "\n",
      "üöÄV√©rification et cr√©ation des tables Bronze si n√©cessaire\n",
      "‚ùå Erreur lors de la cr√©ation de la table 'bronze.bronze.logement_rpls_region' : cross-database references are not implemented: \"bronze.bronze.logement_rpls_region\"\n",
      "\n"
     ]
    },
    {
     "ename": "FeatureNotSupported",
     "evalue": "cross-database references are not implemented: \"bronze.bronze.logement_rpls_region\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFeatureNotSupported\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 42\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Create Bronze tables if they don‚Äôt exist\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müöÄV√©rification et cr√©ation des tables Bronze si n√©cessaire\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[43mcreate_bronze_table\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbronze_table_name_region\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m create_bronze_table (conn, bronze_table_name_departement)\n\u001b[32m     44\u001b[39m create_bronze_table (conn, bronze_table_name_commune)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 131\u001b[39m, in \u001b[36mcreate_bronze_table\u001b[39m\u001b[34m(conn, table_name, schema)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;66;03m# Drop the table with CASCADE\u001b[39;00m\n\u001b[32m    130\u001b[39m drop_table_query = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDROP TABLE IF EXISTS \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_table_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m CASCADE;\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m \u001b[43mcursor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdrop_table_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[38;5;66;03m# Create the table again\u001b[39;00m\n\u001b[32m    134\u001b[39m create_table_query = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m    135\u001b[39m \u001b[33mCREATE TABLE \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_table_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[33m    id SERIAL PRIMARY KEY,\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    139\u001b[39m \u001b[33m);\u001b[39m\n\u001b[32m    140\u001b[39m \u001b[33m\u001b[39m\u001b[33m\"\"\"\u001b[39m\n",
      "\u001b[31mFeatureNotSupported\u001b[39m: cross-database references are not implemented: \"bronze.bronze.logement_rpls_region\"\n"
     ]
    }
   ],
   "source": [
    "# Processing time \n",
    "created_at = datetime.now()\n",
    "print(f\"\\nüìÖ D√©but du traitement √† {created_at}\")\n",
    "\n",
    "# File name and S3 bucket to process\n",
    "file_path = \"resultats_rpls_2024_v3.xlsx\"\n",
    "bucket_name = os.getenv('SCW_BUCKET_NAME')\n",
    "\n",
    "# Retrieve the Excel file from S3 as a stream\n",
    "print(f\"\\nüöÄChargement du fichier depuis S3 : {file_path} (Bucket: {bucket_name})\")\n",
    "s3_client=create_s3_client()\n",
    "file_stream = read_excel_from_s3(s3_client, bucket_name, file_path)\n",
    "print(f\"‚úîÔ∏è Fichier charg√©\")\n",
    "\n",
    "# Load Excel sheets into pandas DataFrames\n",
    "print(f\"\\nüöÄChargement des diff√©rentes feuilles du fichier Excel dans des DataFrames pandas\")\n",
    "df_region = pd.read_excel(file_stream, sheet_name=\"REGION\", header=5)\n",
    "df_departement = pd.read_excel(file_stream, sheet_name=\"DEPARTEMENT\", header=5)\n",
    "df_commune = pd.read_excel(file_stream, sheet_name=\"COMMUNES\", header=5)\n",
    "df_epci = pd.read_excel(file_stream, sheet_name=\"EPCI\", header=5)\n",
    "print(f\"‚úîÔ∏è Feuilles charg√©es\")\n",
    "\n",
    "# Convert DataFrames to JSON for PostgreSQL insertion\n",
    "print(\"\\nüöÄConversion des DataFrames en JSON\")\n",
    "json_data_region = load_sheet_from_dataframe(df_region)\n",
    "json_data_departement = load_sheet_from_dataframe(df_departement)\n",
    "json_data_communes = load_sheet_from_dataframe(df_commune)\n",
    "json_data_epci = load_sheet_from_dataframe(df_epci)\n",
    "\n",
    "# Connect to PostgreSQL\n",
    "print(\"\\nüöÄ Connexion √† PostgreSQL\")\n",
    "conn = get_connexion()\n",
    "\n",
    "# Define PostgreSQL table names\n",
    "bronze_table_name_region = \"bronze.logement_rpls_region\"\n",
    "bronze_table_name_departement = \"bronze.logement_rpls_departement\"\n",
    "bronze_table_name_commune = \"bronze.logement_rpls_commune\"\n",
    "bronze_table_name_epci = \"bronze.logement_rpls_epci\"\n",
    "\n",
    "# Create Bronze tables if they don‚Äôt exist\n",
    "print(\"\\nüöÄV√©rification et cr√©ation des tables Bronze si n√©cessaire\")\n",
    "create_bronze_table (conn, bronze_table_name_region)\n",
    "create_bronze_table (conn, bronze_table_name_departement)\n",
    "create_bronze_table (conn, bronze_table_name_commune)\n",
    "create_bronze_table (conn, bronze_table_name_epci)\n",
    "print(\"‚úîÔ∏è Tables Bronze pr√™tes\")\n",
    "\n",
    "# Insert JSON data into Bronze tables\n",
    "print(\"\\nüöÄ Insertion des donn√©es dans les tables Bronze\")\n",
    "import_data_to_bronze_table (conn, bronze_table_name_region, json_data_region, created_at)\n",
    "import_data_to_bronze_table (conn, bronze_table_name_departement, json_data_departement, created_at)\n",
    "import_data_to_bronze_table (conn, bronze_table_name_commune, json_data_communes, created_at)\n",
    "import_data_to_bronze_table (conn, bronze_table_name_epci, json_data_epci, created_at)\n",
    "print(\"‚úîÔ∏è Donn√©es ins√©r√©es avec succ√®s\")\n",
    "\n",
    "# Close PostgreSQL connection\n",
    "conn.close()\n",
    "print(\"\\n‚úÖ Fin du traitement.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "odis-05r1PXDx-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
