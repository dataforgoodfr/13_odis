{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessor Notebook : Logements Sociaux, fichier RPLS annuel\n",
    "\n",
    "Ce notebook traite le fichier Excel du RPLS annuel : données sur les logements sociaux.\n",
    "Le but est de récupérer les datasets suivants, à partir du fichier XSLX téléchargé depuis le site du ministère du Développement Durable :\n",
    " - Données par régions\n",
    " - Données par départements\n",
    " - Données par EPCI\n",
    " - Données par communes\n",
    "\n",
    " ### Paramètres\n",
    " Ce Notebook prend des paramètres en entrée, définis sur la toute première cellule (ci-dessus).\n",
    " La cellule a le tag \"parameters\" ce qui permet de lui passer des valeurs via papermill.\n",
    " - filepath : le chemin vers le fichier Excel à traiter\n",
    " - model_name : le nom du modèle source\n",
    "\n",
    " ### Principe\n",
    " Ce notebook extrait 4 feuilles du fichier Excel d'entrée : region, departement, epci, communes. \n",
    " Chaque feuille est chargée dans un dataFrame puis sauvegardée en .xlsx, et chargée en base de données Bronze.\n",
    " Peu de retraitement sur ces dataFrames, seul le tableau \"departement\" a besoin de renommer une colonne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation\n",
    "\n",
    "Les cellules suivantes servent à importer les modules nécessaires et à préparer les variables communes utilisées dans les traitements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline imports\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "\n",
    "# Dirty trick to be able to import common odis modules, if the notebook is not executed from 13_odis\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(os.getcwd())\n",
    "while not current_dir.endswith(\"13_odis\"):\n",
    "    print(\"changing to parent dir\")\n",
    "    os.chdir(parent_dir)\n",
    "    current_dir = parent_dir\n",
    "    parent_dir = os.path.dirname(current_dir)\n",
    "\n",
    "print(os.getcwd())\n",
    "sys.path.append(current_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional imports\n",
    "from common.config import load_config\n",
    "from common.data_source_model import DataSourceModel\n",
    "from common.utils.file_handler import FileHandler\n",
    "from common.utils.interfaces.data_handler import OperationType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paramètres du Notebook\n",
    "Paramètres pouvant être passés en input par papermill.\n",
    "\n",
    "Seuls des types built-in semblent marcher (str, int etc), les classes spécifiques ou les objets mutables (datetime...) semblent faire planter papermill.\n",
    "\n",
    "Doc officielle de papermill : parametrize [https://papermill.readthedocs.io/en/latest/usage-parameterize.html]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Define parameters for papermill. \n",
    "filepath = 'data/imports/logement_social/logement_social.logements_sociaux_1.xlsx'\n",
    "model_name = \"logement_social.logements_sociaux\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize common variables\n",
    "dataframes = {}\n",
    "artifacts = []\n",
    "\n",
    "start_time = datetime.datetime.now(tz=datetime.timezone.utc)\n",
    "config = load_config(\"datasources.yaml\", response_model=DataSourceModel)\n",
    "model = config.get_model( model_name = model_name )\n",
    "# Instantiate File Handler for file loads and dumps\n",
    "handler = FileHandler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traitement des données\n",
    "A partir de là, on charge le fichier Excel dans Pandas et on traite les feuilles à récupérer, une par une"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load workbook to pandas\n",
    "wb = pd.ExcelFile(\n",
    "    filepath,\n",
    "    engine = 'openpyxl'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load excel sheet for Regions\n",
    "sheet_name = \"REGION\"\n",
    "keep_columns_region = [\n",
    "    'LIBREG',\n",
    "    'densite',\n",
    "    'nb_ls',\n",
    "    'tx_vac',\n",
    "    'tx_mob'\n",
    "]\n",
    "\n",
    "\n",
    "df_region = pd.read_excel(wb, \n",
    "                    sheet_name = \"REGION\",\n",
    "                    index_col = \"REG\",\n",
    "                    header = 5\n",
    "                    )\n",
    "\n",
    "# df_region = df_region[keep_columns_region]\n",
    "dataframes[\"REGION\"] = df_region\n",
    "\n",
    "region_artifact = handler.artifact_dump( df_region, \"REGION\", model)\n",
    "artifacts.append(region_artifact)\n",
    "\n",
    "df_region.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load excel sheet for Departments\n",
    "keep_columns_departments = [\n",
    "    'Unnamed: 1',\n",
    "    'densite',\n",
    "    'nb_ls',\n",
    "    'tx_vac',\n",
    "    'tx_mob'\n",
    "]\n",
    "\n",
    "df_department = pd.read_excel(wb, \n",
    "                    sheet_name = \"DEPARTEMENT\",\n",
    "                    index_col = \"DEP\",\n",
    "                    header = 5\n",
    "                    )\n",
    "\n",
    "# df_department = df_department[keep_columns_departments]\n",
    "\n",
    "# TODO : rename column for Unnamed: 1\n",
    "\n",
    "dataframes[\"DEPARTEMENT\"] = df_department\n",
    "\n",
    "department_artifact = handler.artifact_dump( df_department, \"DEPARTEMENT\", model)\n",
    "artifacts.append(department_artifact)\n",
    "\n",
    "df_department.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load excel sheet for EPCI\n",
    "keep_columns_epci = [\n",
    "    'LIBEPCI',\n",
    "    'densite',\n",
    "    'nb_ls',\n",
    "    'tx_vac',\n",
    "    'tx_mob'\n",
    "]\n",
    "\n",
    "df_epci = pd.read_excel(wb, \n",
    "                    sheet_name = \"EPCI\",\n",
    "                    index_col = \"EPCI_DEP\",\n",
    "                    header = 5\n",
    "                    )\n",
    "\n",
    "# df_epci = df_epci[keep_columns_epci]\n",
    "\n",
    "dataframes[\"EPCI\"] = df_epci\n",
    "\n",
    "epci_artifact = handler.artifact_dump( df_epci, \"EPCI\", model)\n",
    "artifacts.append(epci_artifact)\n",
    "\n",
    "df_epci.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load excel sheet for COMMUNES\n",
    "keep_columns_communes = [\n",
    "    'LIBCOM_DEP',\n",
    "    'densite',\n",
    "    'nb_ls',\n",
    "    'tx_vac',\n",
    "    'tx_mob'\n",
    "]\n",
    "\n",
    "df_communes = pd.read_excel(wb, \n",
    "                    sheet_name = \"COMMUNES\",\n",
    "                    index_col = \"DEPCOM_ARM\",\n",
    "                    header = 5\n",
    "                    )\n",
    "\n",
    "# df_communes = df_communes[keep_columns_communes]\n",
    "\n",
    "dataframes[\"COMMUNES\"] = df_communes\n",
    "\n",
    "communes_artifact = handler.artifact_dump( df_communes, \"COMMUNES\", model )\n",
    "artifacts.append(communes_artifact)\n",
    "\n",
    "df_communes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sauvegarde des métadonnées\n",
    "On sauvegarde les métadonnées du processus localement, pour garder l'historique et pouvoir reprendre après erreur si besoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for artifact in artifacts:\n",
    "    print(artifact.model_dump( mode = \"json\" ))\n",
    "\n",
    "preprocess_metadata = handler.dump_metadata(\n",
    "    model = model,\n",
    "    operation = OperationType.PREPROCESS,\n",
    "    start_time = start_time,\n",
    "    complete = True,\n",
    "    errors = 0,\n",
    "    artifacts = artifacts,\n",
    "    pages = []\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement en couche Bronze\n",
    "On charge un engine SQLAchemy pour charger tous les datasets en base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "import sqlalchemy\n",
    "from sqlalchemy import text\n",
    "\n",
    "# prepare db client\n",
    "vals = dotenv_values()\n",
    "\n",
    "conn_str = \"postgresql://{}:{}@{}:{}/{}\".format(\n",
    "    vals[\"PG_DB_USER\"],\n",
    "    vals[\"PG_DB_PWD\"],\n",
    "    vals[\"PG_DB_HOST\"],\n",
    "    vals[\"PG_DB_PORT\"],\n",
    "    vals[\"PG_DB_NAME\"]\n",
    ")\n",
    "\n",
    "dbengine = sqlalchemy.create_engine(conn_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert all to bronze\n",
    "# make the final table name lowercase to avoid issues in Postgre\n",
    "\n",
    "for name, dataframe in dataframes.items():\n",
    "\n",
    "    subtable_name = f\"{model.table_name}_{name.lower()}\"\n",
    "    query_str = f\"DROP TABLE IF EXISTS bronze.{subtable_name} CASCADE\"\n",
    "\n",
    "    # dropping existing table with cascade\n",
    "    with dbengine.connect() as con:\n",
    "        print(f\"Dropping if exists: {subtable_name}\")\n",
    "        result = con.execute(text(query_str))\n",
    "        con.commit()\n",
    "\n",
    "    print(f\"Inserting DataFrame {subtable_name}\")\n",
    "    dataframe.to_sql(\n",
    "        name = subtable_name,\n",
    "        con = dbengine,\n",
    "        schema = 'bronze',\n",
    "        index = True,\n",
    "        if_exists = 'replace'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
