import asyncio
import urllib
from typing import AsyncGenerator, Generator, Optional

import jmespath
import nbformat
from nbconvert.preprocessors import ExecutePreprocessor
from pydantic import BaseModel, Field

from common.data_source_model import FILE_FORMAT
from common.utils.interfaces.extractor import AbstractSourceExtractor, ExtractionResult
from common.utils.logging_odis import logger


class FileExtractor(AbstractSourceExtractor):
    """Generic extractor for a file dump from an API"""

    is_json: bool = False

    async def download(self) -> AsyncGenerator[ExtractionResult, None]:
        """Downloads data corresponding to the given source model.
        The parameters of the request (URL, headers etc) are set using the inherited set_query_parameters method.
        """

        # Send request to API
        response = await self.http_client.get(
            self.url,
            headers=self.model.headers.model_dump(mode="json"),
            params=self.model.extract_params,
            as_json=self.is_json,
        )

        # yield the request result
        yield ExtractionResult(payload=response, success=True, is_last=True)


class JsonExtractor(FileExtractor):
    """Extractor for getting JSON data from an API"""

    is_json = True


class MelodiExtractor(FileExtractor):
    """Extractor for getting JSON data from an API"""

    async def download(self) -> AsyncGenerator[ExtractionResult, None]:

        is_last = False
        url = self.url

        while not is_last:

            # yield the request result
            result = await self.download_page(url)

            is_last = result.is_last

            if not is_last and result.next_url:

                await asyncio.sleep(60 / self.api_config.throttle)

                url = result.next_url

                logger.debug(f"Next page: {result.next_url}")

            yield result

    async def download_page(self, url: str) -> ExtractionResult:
        """Downloads data corresponding to the given source model.
        The parameters of the request (URL, headers etc) are set using the inherited set_query_parameters method.
        """

        # if url has a query string, ignore the dict-defined parameters
        url_querystr = urllib.parse.urlparse(url).query
        passed_params = self.model.extract_params if url_querystr == "" else None
        # logger.info(f"querying '{url}'")

        success = False
        payload = None
        is_last = False
        next_url = None

        try:
            # Send request to API
            payload = await self.http_client.get(
                url,
                headers=self.model.headers.model_dump(mode="json"),
                params=passed_params,
                as_json=True,
            )

            # Get next page URL
            next_key = self.model.response_map.get("next")
            next_url = jmespath.search(next_key, payload) if next_key else None

            # Determine whether this is the last page :
            # if explicitly given by the API response, get the explicit value
            # if not given in API response, BUT if no next page could be derived, then true
            # else: false
            is_last_key = self.model.response_map.get("is_last")
            if is_last_key:
                is_last_from_response = jmespath.search(is_last_key, payload)
                is_last = is_last_from_response if is_last_from_response is not None else (next_url is None)
            else:
                is_last = (next_url is None)

            # If all went well, success = true
            success = True

        except Exception as e:
            error = str(e)
            logger.exception(f"Error while extracting page {url}: {error}")

        return ExtractionResult(
            success=success, payload=payload, is_last=is_last, next_url=next_url
        )


class NotebookResult(BaseModel):
    """TODO:

    Fixme: this is a quickwin to get the cell output

    """

    data_files: list[str] = Field(
        ...,
        description="List of data files generated by the notebook",
    )

    format: Optional[FILE_FORMAT] = Field(
        default="json",
        description="Format of the data files generated by the notebook",
    )

    dictionnary_file: Optional[str] = Field(
        default="",
        description="Path to the dictionnary file generated by the notebook",
    )


class NotebookExtractor(FileExtractor):
    """Extractor for getting JSON data from an API"""

    def download(self) -> Generator[ExtractionResult, None, None]:
        """
        execute the notebook and yield the result

        Yields:
            ExtractionResult: the result of the notebook execution
                each cell output yielded as a separate ExtractionResult
                the last cell output is marked as is_last=True
                the next_url is None

        Example:
            >>> extractor = NotebookExtractor(config, model)
            >>> for result in extractor.download():
            ...     print(result.payload)
        """

        # Prepare Notebook parameters
        notebook_name = self.model.preprocessor.name

        notebook_name = self.model.preprocessor.name
        notebook_path = self.model.preprocessor.base_path / f"{notebook_name}.ipynb"

        logger.info(f"Executing notebook '{notebook_path}'")

        with open(notebook_path) as f:
            nb_in = nbformat.read(f, nbformat.NO_CONVERT)

            ep = ExecutePreprocessor(timeout=600)
            ep.preprocess(
                nb_in,
            )

            for cell in nb_in.cells:

                if cell.cell_type == "code" and "outputs" in cell:

                    for out in cell.outputs:
                        if out.output_type == "execute_result":

                            try:

                                logger.info(
                                    f"Extracting cell output: {out.data.get("text/plain")}"
                                )

                                # fixme: this is a workaround to get the cell output
                                # which arrives as a json delimited by single quotes (which is not valid json)
                                stdout = out.data.get("text/plain").replace("'", '"')

                                # regenerate the model from the cell output
                                # which comes from the notebook as a string
                                result = NotebookResult.model_validate_json(stdout)

                                logger.info(
                                    f"Extracted cell output: {result.model_dump()}"
                                )

                                # and yield them as ExtractionResult
                                if result:
                                    logger.info(
                                        f"Skipping dictionary file: '{result.dictionnary_file}'"
                                    )

                                    for i, file in enumerate(result.data_files):
                                        logger.info(f"Extracting file: {file}")

                                        # if the file is a dictionnary, load it as a model
                                        with open(file, "rb") as f:
                                            payload = f.read()

                                        yield ExtractionResult(
                                            payload=payload,
                                            is_last=(i == len(result.data_files) - 1),
                                            success=True,
                                        )

                            except Exception as e:
                                logger.error(f"Error while parsing cell output: {e}")
                                continue


class OpenDataSoftExtractor(FileExtractor):

    async def download(self) -> AsyncGenerator[ExtractionResult, None]:

        is_last = False
        url = self.url

        pageno = 1
        offset = 0
        total_count = 0
        aggregated_count = 0

        while not is_last:

            # yield the request result
            result = await self.download_page(url, offset)

            is_last = result.is_last
            total_count = result.total_count
            page_records_count = result.count
            # Accumulate number of records
            aggregated_count += page_records_count

            logger.debug(f"Extracted {page_records_count} from page {pageno}")
            logger.debug(f"Extracted {aggregated_count} out of {total_count} so far.")

            await asyncio.sleep(60 / self.api_config.throttle)

            offset = aggregated_count + 1

            logger.debug(f"Next offset: {offset}")

            yield result

    async def download_page(self, url, offset) -> ExtractionResult:

        passed_params = self.model.extract_params
        passed_params["offset"] = offset

        logger.info(f"querying '{url}'")

        success = False
        payload = None
        count = 0
        total_count = 0

        try:
            # Send request to API

            raw_data = await self.http_client.get(
                url,
                headers=self.model.headers.model_dump(mode="json"),
                params=passed_params,
                as_json=True,
            )

            # get payload
            datapath = self.model.response_map.get("data")
            payload = jmespath.search(datapath, raw_data)

            # get records count
            count = len(payload)

            # get total count
            total_count_key = "total_count"
            total_count = jmespath.search(total_count_key, raw_data)

            # If offset + count goes is greater thatn total_count,
            # means this is the last call to be made
            is_last = offset + count > total_count

            # If all went well, success = true
            success = True

        except Exception as e:
            error = str(e)
            logger.exception(f"Error while extracting page {url}: {error}")

        return ExtractionResult(
            success=success,
            payload=payload,
            is_last=is_last,
            count=count,
            total_count=total_count,
        )
