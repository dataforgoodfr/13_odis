{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "604669f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T15:05:20.983601Z",
     "iopub.status.busy": "2025-04-12T15:05:20.982994Z",
     "iopub.status.idle": "2025-04-12T15:05:22.249926Z",
     "shell.execute_reply": "2025-04-12T15:05:22.248838Z"
    },
    "papermill": {
     "duration": 1.290418,
     "end_time": "2025-04-12T15:05:22.260269",
     "exception": false,
     "start_time": "2025-04-12T15:05:20.969851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importation des bibliothèques nécessaires\n",
    "import pandas as pd\n",
    "import os\n",
    "import psycopg2\n",
    "\n",
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "\n",
    "\n",
    "from psycopg2.extras import Json\n",
    "from psycopg2.extras import execute_values\n",
    "import math\n",
    "from datetime import datetime\n",
    "from io import BytesIO\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # defaults to .env in cwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad734f28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T15:05:22.275076Z",
     "iopub.status.busy": "2025-04-12T15:05:22.274533Z",
     "iopub.status.idle": "2025-04-12T15:05:22.311346Z",
     "shell.execute_reply": "2025-04-12T15:05:22.309001Z"
    },
    "papermill": {
     "duration": 0.044805,
     "end_time": "2025-04-12T15:05:22.313240",
     "exception": false,
     "start_time": "2025-04-12T15:05:22.268435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_s3_client():\n",
    "    \"\"\"\n",
    "    Creates an S3 client using environment variables for credentials.\n",
    "    \n",
    "    :return: Boto3 S3 client\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Récupère les identifiants depuis les variables d'environnement\n",
    "        access_key = os.getenv('SCW_ACCESS_KEY')\n",
    "        secret_key = os.getenv('SCW_SECRET_KEY')\n",
    "        endpoint_url = os.getenv('SCW_OBJECT_STORAGE_ENDPOINT')\n",
    "        region = os.getenv('SCW_REGION')\n",
    "\n",
    "        if not access_key or not secret_key or not endpoint_url or not region:\n",
    "            raise ValueError(\"Identifiants S3 manquants dans les variables d'environnement.\")\n",
    "\n",
    "        # Initialise le client S3\n",
    "        session = boto3.Session(\n",
    "            aws_access_key_id=access_key,\n",
    "            aws_secret_access_key=secret_key,\n",
    "            region_name=region\n",
    "        )\n",
    "        s3_client = session.client('s3', endpoint_url=endpoint_url)\n",
    "\n",
    "        print(\"✔️ Client S3 créé avec succès.\")\n",
    "        return s3_client\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Échec de la création du client S3 : {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def read_excel_from_s3(s3_client, bucket_name, file_path):\n",
    "    \"\"\"\n",
    "    Retrieves an Excel file from an S3 bucket and loads it as a stream\n",
    "    \n",
    "    :param s3_client: S3 client\n",
    "    :param bucket_name: Name of the S3 bucket\n",
    "    :param file_path: Key (path) of the file in the S3 bucket\n",
    "    :return: A file object as a stream\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Retrieve the file from S3 as a binary stream\n",
    "        response = s3_client.get_object(Bucket=bucket_name, Key=file_path)\n",
    "        file_content = response['Body'].read()\n",
    "        \n",
    "        # Use BytesIO to create a file-like object from the binary data\n",
    "        file_stream = BytesIO(file_content)\n",
    "        \n",
    "        return file_stream\n",
    "\n",
    "    except NoCredentialsError:\n",
    "        print(\"❌ Les informations d'identification sont manquantes ou incorrectes.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur lors de la récupération du fichier : {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def load_sheet_from_dataframe(dataframe):\n",
    "    \"\"\"\n",
    "    Converts a Pandas DataFrame to JSON format.\n",
    "    \n",
    "    :param dataframe: Pandas DataFrame\n",
    "    :return: JSON data (list of dictionaries)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        json_data = dataframe.to_dict(orient='records')\n",
    "        print(f\"✔️ {len(json_data)} lignes converties en JSON.\")\n",
    "        return json_data\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur lors de la conversion du DataFrame en JSON : {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def get_connexion():\n",
    "    \"\"\"\n",
    "     Establishes a PostgreSQL connection using environment variables.\n",
    "    \n",
    "    :return: PostgreSQL connection object\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=os.getenv('PG_DB_NAME'),\n",
    "            user=os.getenv('PG_DB_USER'),\n",
    "            password=os.getenv('PG_DB_PWD'),\n",
    "            host=\"localhost\",\n",
    "            port=\"5432\"\n",
    "        )\n",
    "        print(\"✔️ Connexion à PostgreSQL réussie.\")\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Échec de la connexion à PostgreSQL : {e}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "def create_bronze_table(conn, table_name):\n",
    "    \"\"\"\n",
    "    Drops the 'bronze' table if it exists and creates a new one in PostgreSQL.\n",
    "\n",
    "    :param conn: PostgreSQL connection object\n",
    "    :param table_name: Name of the table to create\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "         # Force drop the table with CASCADE to remove dependencies\n",
    "        drop_table_query = f\"DROP TABLE IF EXISTS {table_name} CASCADE;\"\n",
    "        cursor.execute(drop_table_query)\n",
    "        print(f\"✔️ Table '{table_name}' supprimée avec CASCADE.\")\n",
    "\n",
    "        # Recreate the table\n",
    "        create_table_query = f\"\"\"\n",
    "        CREATE TABLE {table_name} (\n",
    "            id SERIAL PRIMARY KEY,\n",
    "            data JSONB NOT NULL,\n",
    "            created_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP\n",
    "        );\n",
    "        \"\"\"\n",
    "        cursor.execute(drop_table_query)\n",
    "        cursor.execute(create_table_query)\n",
    "        conn.commit()\n",
    "        print(f\"✔️ La table '{table_name}' est prête.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur lors de la création de la table '{table_name}' : {e}\")\n",
    "        conn.rollback()\n",
    "        raise\n",
    "    finally:\n",
    "        cursor.close()\n",
    "\n",
    "\n",
    "def clean_json(obj):\n",
    "    \"\"\"\n",
    "    Cleans JSON data by removing invalid values (e.g., NaN, INF, empty strings).\n",
    "    \n",
    "    :param obj: JSON object\n",
    "    :return: Cleaned JSON object\n",
    "    \"\"\"\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: clean_json(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [clean_json(v) for v in obj]\n",
    "    elif isinstance(obj, float):\n",
    "        return None if math.isinf(obj) or math.isnan(obj) else obj\n",
    "    elif isinstance(obj, str):\n",
    "        return None if obj.upper() in (\"INF\", \"NA\", \"NAN\", \"\") else obj\n",
    "    return obj\n",
    "\n",
    "\n",
    "def import_data_to_bronze_table(conn, table_name, json_data, created_at=None):\n",
    "    \"\"\"\n",
    "    Inserts JSON data into a PostgreSQL table.\n",
    "\n",
    "    :param conn: PostgreSQL connection object\n",
    "    :param table_name: Name of the table\n",
    "    :param json_data: List of dictionaries representing JSON data\n",
    "    :param created_at: Timestamp for data insertion (defaults to now)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cursor = conn.cursor()\n",
    "        created_at = created_at or datetime.timezone.utc()\n",
    "\n",
    "        insert_query = f\"INSERT INTO {table_name} (created_at, data) VALUES %s\"\n",
    "        json_records = [(created_at, Json(clean_json(record))) for record in json_data]\n",
    "\n",
    "        execute_values(cursor, insert_query, json_records)\n",
    "        conn.commit()\n",
    "\n",
    "        print(f\"✔️ {len(json_records)} lignes insérées dans '{table_name}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Erreur lors de l'insertion des données dans '{table_name}' : {e}\")\n",
    "        conn.rollback()\n",
    "        raise\n",
    "    finally:\n",
    "        cursor.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b4f6ebc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T15:05:22.323435Z",
     "iopub.status.busy": "2025-04-12T15:05:22.322130Z",
     "iopub.status.idle": "2025-04-12T15:07:56.939743Z",
     "shell.execute_reply": "2025-04-12T15:07:56.936523Z"
    },
    "papermill": {
     "duration": 154.62741,
     "end_time": "2025-04-12T15:07:56.943905",
     "exception": false,
     "start_time": "2025-04-12T15:05:22.316495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📅 Début du traitement à 2025-04-12 15:05:22.333812\n",
      "\n",
      "🚀Chargement du fichier depuis S3 : resultats_rpls_2024_v3.xlsx (Bucket: odis-s3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ Client S3 créé avec succès.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ Fichier chargé\n",
      "\n",
      "🚀Chargement des différentes feuilles du fichier Excel dans des DataFrames pandas\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ Feuilles chargées\n",
      "\n",
      "🚀Conversion des DataFrames en JSON\n",
      "✔️ 23 lignes converties en JSON.\n",
      "✔️ 96 lignes converties en JSON.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ 16859 lignes converties en JSON.\n",
      "✔️ 1325 lignes converties en JSON.\n",
      "\n",
      "🚀 Connexion à PostgreSQL\n",
      "✔️ Connexion à PostgreSQL réussie.\n",
      "\n",
      "🚀Supression et recréation des tables Bronze\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ Table 'bronze.logement_rpls_region' supprimée avec CASCADE.\n",
      "✔️ La table 'bronze.logement_rpls_region' est prête.\n",
      "✔️ Table 'bronze.logement_rpls_departement' supprimée avec CASCADE.\n",
      "✔️ La table 'bronze.logement_rpls_departement' est prête.\n",
      "✔️ Table 'bronze.logement_rpls_commune' supprimée avec CASCADE.\n",
      "✔️ La table 'bronze.logement_rpls_commune' est prête.\n",
      "✔️ Table 'bronze.logement_rpls_epci' supprimée avec CASCADE.\n",
      "✔️ La table 'bronze.logement_rpls_epci' est prête.\n",
      "✔️ Tables Bronze prêtes\n",
      "\n",
      "🚀 Insertion des données dans les tables Bronze\n",
      "✔️ 23 lignes insérées dans 'bronze.logement_rpls_region'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ 96 lignes insérées dans 'bronze.logement_rpls_departement'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ 16859 lignes insérées dans 'bronze.logement_rpls_commune'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ 1325 lignes insérées dans 'bronze.logement_rpls_epci'.\n",
      "✔️ Données insérées avec succès\n",
      "\n",
      "✅ Fin du traitement.\n"
     ]
    }
   ],
   "source": [
    "# Processing time \n",
    "created_at = datetime.now()\n",
    "print(f\"\\n📅 Début du traitement à {created_at}\")\n",
    "\n",
    "# File name and S3 bucket to process\n",
    "file_path = \"resultats_rpls_2024_v3.xlsx\"\n",
    "bucket_name = os.getenv('SCW_BUCKET_NAME')\n",
    "\n",
    "# Retrieve the Excel file from S3 as a stream\n",
    "print(f\"\\n🚀Chargement du fichier depuis S3 : {file_path} (Bucket: {bucket_name})\")\n",
    "s3_client=create_s3_client()\n",
    "file_stream = read_excel_from_s3(s3_client, bucket_name, file_path)\n",
    "print(f\"✔️ Fichier chargé\")\n",
    "\n",
    "# Load Excel sheets into pandas DataFrames\n",
    "print(f\"\\n🚀Chargement des différentes feuilles du fichier Excel dans des DataFrames pandas\")\n",
    "df_region = pd.read_excel(file_stream, sheet_name=\"REGION\", header=5)\n",
    "df_departement = pd.read_excel(file_stream, sheet_name=\"DEPARTEMENT\", header=5)\n",
    "df_commune = pd.read_excel(file_stream, sheet_name=\"COMMUNES\", header=5)\n",
    "df_epci = pd.read_excel(file_stream, sheet_name=\"EPCI\", header=5)\n",
    "print(f\"✔️ Feuilles chargées\")\n",
    "\n",
    "# Convert DataFrames to JSON for PostgreSQL insertion\n",
    "print(\"\\n🚀Conversion des DataFrames en JSON\")\n",
    "json_data_region = load_sheet_from_dataframe(df_region)\n",
    "json_data_departement = load_sheet_from_dataframe(df_departement)\n",
    "json_data_communes = load_sheet_from_dataframe(df_commune)\n",
    "json_data_epci = load_sheet_from_dataframe(df_epci)\n",
    "\n",
    "# Connect to PostgreSQL\n",
    "print(\"\\n🚀 Connexion à PostgreSQL\")\n",
    "conn = get_connexion()\n",
    "\n",
    "# Define PostgreSQL table names\n",
    "bronze_table_name_region = \"bronze.logement_rpls_region\"\n",
    "bronze_table_name_departement = \"bronze.logement_rpls_departement\"\n",
    "bronze_table_name_commune = \"bronze.logement_rpls_commune\"\n",
    "bronze_table_name_epci = \"bronze.logement_rpls_epci\"\n",
    "\n",
    "# Create Bronze tables if they don’t exist\n",
    "print(\"\\n🚀Supression et recréation des tables Bronze\")\n",
    "create_bronze_table (conn, bronze_table_name_region)\n",
    "create_bronze_table (conn, bronze_table_name_departement)\n",
    "create_bronze_table (conn, bronze_table_name_commune)\n",
    "create_bronze_table (conn, bronze_table_name_epci)\n",
    "print(\"✔️ Tables Bronze prêtes\")\n",
    "\n",
    "# Insert JSON data into Bronze tables\n",
    "print(\"\\n🚀 Insertion des données dans les tables Bronze\")\n",
    "import_data_to_bronze_table (conn, bronze_table_name_region, json_data_region, created_at)\n",
    "import_data_to_bronze_table (conn, bronze_table_name_departement, json_data_departement, created_at)\n",
    "import_data_to_bronze_table (conn, bronze_table_name_commune, json_data_communes, created_at)\n",
    "import_data_to_bronze_table (conn, bronze_table_name_epci, json_data_epci, created_at)\n",
    "print(\"✔️ Données insérées avec succès\")\n",
    "\n",
    "# Close PostgreSQL connection\n",
    "conn.close()\n",
    "print(\"\\n✅ Fin du traitement.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (d4g)",
   "language": "python",
   "name": "d4g"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 160.016146,
   "end_time": "2025-04-12T15:07:58.985027",
   "environment_variables": {},
   "exception": null,
   "input_path": "notebook/rpls_Load_bronze.ipynb",
   "output_path": "notebook/rpls_Load_bronze.ipynb",
   "parameters": {},
   "start_time": "2025-04-12T15:05:18.968881",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}